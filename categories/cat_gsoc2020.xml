<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Universe OpenAstronomy (Posts about gsoc2020)</title><link>http://openastronomy.org/Universe_OA/</link><description></description><atom:link href="http://openastronomy.org/Universe_OA/categories/cat_gsoc2020.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Mon, 10 Aug 2020 01:52:28 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>GSoC 2020: glue-solar project 3.1</title><link>http://openastronomy.org/Universe_OA/posts/2020/08/20200809_2322_kakirastern/</link><dc:creator>Kris Stern</dc:creator><description>&lt;div&gt;&lt;p&gt;The 3rd coding period of GSoC 2020 will officially conclude in 2 weeks time. I would like to take this opportunity to review the progress made thus far, and to outline what other major feature can be added to glue-solar, perhaps over the remaining 2 weeks and beyond.&lt;/p&gt;
&lt;p&gt;We have finally resumed code reviews for the remaining PRs in the glue repo, and I am happy to report that the PR dealing with adding a preferred_cmap attribute to the visual module of glue/core has been merged four days ago from today. This is a very memorable milestone as this is my first contribution to the glue codebase. The remaining PRs which are being worked on include the 1D Profile PR (&lt;a href="https://github.com/glue-viz/glue/pull/2156"&gt;PR #2156&lt;/a&gt;) as well as the wcs auto-linking PR (&lt;a href="https://github.com/glue-viz/glue/pull/2161"&gt;PR #2161&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Regarding our work at the glue-solar repo, all of the PRs reviewed except the two experimental ones has been merged. A User’s Guide and a Developer’s Guide have been added to the &lt;a href="https://glue-solar.readthedocs.io/en/latest/"&gt;docs&lt;/a&gt;, while there is one open WIP PR which I am working on to add both a contributing document and the code references (or API) for the repo. Also some docs introducing users on how to start their own extensions in glue-solar for conducting solar physics has been planned.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;All of the above will form the bulk of work to be submitted for the GSoC project.&lt;/p&gt;
&lt;p&gt;More can be done for glue-solar, including but not limited to the following:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Add NDData support to glue via glue-solar&lt;/li&gt;&lt;li&gt;Add instrument loader code from sunkit-instruments to glue-solar&lt;/li&gt;&lt;li&gt;Enable image / Movie exports, both with axes and without axes via matplotlib&lt;/li&gt;&lt;li&gt;Add support for pre-computed statistics in datasets / viewers.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;Hopefully with the support of the mentors much of what has been planned can be brought to fruition, so that this project will be a successful one.&lt;/p&gt;
&lt;img src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=4aebd6964154" width="1" height="1"&gt;&lt;/div&gt;</description><category>SunPy</category><guid>http://openastronomy.org/Universe_OA/posts/2020/08/20200809_2322_kakirastern/</guid><pubDate>Sun, 09 Aug 2020 22:22:40 GMT</pubDate></item><item><title>GSoC 2020: Blog 4 - Update on Null Geodesics in Kerr Spacetime</title><link>http://openastronomy.org/Universe_OA/posts/2020/08/20200806_1009_jes24/</link><dc:creator>Jyotirmaya Shivottam</dc:creator><description>&lt;div&gt;&lt;h3&gt;
&lt;a href="http://openastronomy.org/Universe_OA/posts/2020/08/20200806_1009_jes24/#progress-so-far" class="anchor"&gt;
&lt;/a&gt;
&lt;!-- TEASER_END --&gt;
Progress so far...
&lt;/h3&gt;

&lt;p&gt;Support for calculation and graphing of Null Geodesics in Kerr (and by extension, Schwarzschild) spacetime is nearing completion (PR &lt;a href="https://github.com/einsteinpy/einsteinpy/pull/527"&gt;#527&lt;/a&gt;). Last week, I hit a serious obstacle, related to maximum floating point precision and accumulation of numerical errors (which is also the reason for the delayed blog). Since the Geodesic Equations are stiff ODEs, small instabilities can wreak havoc on step-size control and completely destabilize the solution. I observed this happening with my code for Null Geodesics. As the light ray approaches the black hole, the integrator can no longer choose a proper step-size and the solution becomes inaccurate. In this blog, I will be discussing this issue and how we are approaching it with the new Null Geodesics module. I also present some of the null geodesic plots, created using this module.&lt;/p&gt;

&lt;h3&gt;
&lt;a href="http://openastronomy.org/Universe_OA/posts/2020/08/20200806_1009_jes24/#stiff-odes-are-evil" class="anchor"&gt;
&lt;/a&gt;
Stiff ODEs are evil!
&lt;/h3&gt;

&lt;p&gt;Stiff ODEs and Numerical Methods have always been at loggerheads. There is no precise definition for stiff ODEs, but an important feature is that, they are prone to become unstable. The usual solution is to choose a solver, that can accommodate very small step-sizes, while keeping overall error low. SciPy provides performant wrappers for LSODA/BDF methods, that are usually suitable for stiff systems, but in our case, these methods are unhelpful, as can be seen in the image below. For comparison, I have used Mathematica to obtain geodesics for the same conditions. The only major difference, here, is the solver. The plot on the left is Mathematica-generated, while the plot on the right was generated by Python. Note that, all the plots in this post have their axes normalized to the gravitational radius, or units of

&lt;span class="katex-element"&gt;
&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;GMc2\frac{GM}{c^2}&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mopen nulldelimiter"&gt;&lt;/span&gt;&lt;span class="mfrac"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathdefault mtight"&gt;c&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size3 size1 mtight"&gt;&lt;span class="mord mtight"&gt;2&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="frac-line"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathdefault mtight"&gt;G&lt;/span&gt;&lt;span class="mord mathdefault mtight"&gt;M&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mclose nulldelimiter"&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/span&gt;
.&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--3K6LPGIe--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/4rrh7mkn6fgd46l61r5s.jpg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--3K6LPGIe--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/4rrh7mkn6fgd46l61r5s.jpg" alt="Plot 1"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The initial conditions for the plots above are as follows. The timelike component of the initial velocity was calculated by setting
&lt;span class="katex-element"&gt;
&lt;span class="katex"&gt;&lt;span class="katex-mathml"&gt;gabuaub=0g_{ab}u^au^b = 0&lt;/span&gt;&lt;span class="katex-html"&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathdefault"&gt;g&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t vlist-t2"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mtight"&gt;&lt;span class="mord mathdefault mtight"&gt;a&lt;/span&gt;&lt;span class="mord mathdefault mtight"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-s"&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathdefault"&gt;u&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathdefault mtight"&gt;a&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mord"&gt;&lt;span class="mord mathdefault"&gt;u&lt;/span&gt;&lt;span class="msupsub"&gt;&lt;span class="vlist-t"&gt;&lt;span class="vlist-r"&gt;&lt;span class="vlist"&gt;&lt;span&gt;&lt;span class="pstrut"&gt;&lt;/span&gt;&lt;span class="sizing reset-size6 size3 mtight"&gt;&lt;span class="mord mathdefault mtight"&gt;b&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;span class="mrel"&gt;=&lt;/span&gt;&lt;span class="mspace"&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class="base"&gt;&lt;span class="strut"&gt;&lt;/span&gt;&lt;span class="mord"&gt;0&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;
&lt;/span&gt;
.&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre class="highlight plaintext"&gt;&lt;code&gt;a = 0.9
end_lambda = 200
max_steps = 200
position = [0, 20., pi / 2, pi / 2]
velocity = [-0.2, 0., 0.002]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;



&lt;p&gt;Other solvers (that are suited to non-stiff problems) become unstable long before the desired number of integration steps is reached. Given the lack of a proper solver, I wrote my own solver, using a step-size control scheme from the venerable Numerical Recipes (Press et al, 2007), fine-tuned to the problem. Sadly, this did not produce better results and it even failed for certain pathological higher-order orbits. Here, "high-order" implies "loopy" orbits, very close to the black hole, while "pathological" can encompass higher-order orbits to orbits, that are scattered at large angles (i.e., orbits, with sharp turning points, à la the plots above).&lt;/p&gt;

&lt;p&gt;Then, I set out to find the reason behind the instability. Based on my tests, the stiffness comes from the singular nature of the black hole horizon (in Boyer-Lindquist coordinates), which can force the solver to choose incredibly small step-sizes, which in turn leads to more and more floating point error and over large intervals, the obtained solution becomes completely unphysical. This is what, "unstable" means here. Apart from the graphical representation of the instability through the plots, we can also see the instability numerically, through the norm of 4-Velocity of the light ray, as it evolves:&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--x773iglx--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/unqkuukvuez64rw9sytu.jpg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--x773iglx--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/unqkuukvuez64rw9sytu.jpg" alt="U1"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;All of these values should be ~0 and on comparing with the plot on the right, it is easy to see, that the norm becomes too high as the light ray gets closer to the black hole. This tells us, that a correlation exists between the initial conditions and the instability, which is expected.&lt;/p&gt;

&lt;p&gt;In discussions with my mentors, we explored a few solutions, such as, using another system of units or coordinate system. However, we are already using the most suitable unit and coordinate systems for numerical computation of geodesics - &lt;em&gt;M&lt;/em&gt;-Units and Boyer-Lindquist Coordinates. I should note here, that at slightly larger initial radial distances and speeds, the code provides a good approximation to the actual solution, as can be observed in the plot and table below.&lt;br&gt;
&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--MpWHMED6--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/8jguox8rqj06yjwrrleo.jpg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--MpWHMED6--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/8jguox8rqj06yjwrrleo.jpg" alt="Plot 2"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://res.cloudinary.com/practicaldev/image/fetch/s--d97ENcX4--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/b7m73dy8glfgu6wyg78c.jpg" class="article-body-image-wrapper"&gt;&lt;img src="https://res.cloudinary.com/practicaldev/image/fetch/s--d97ENcX4--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/b7m73dy8glfgu6wyg78c.jpg" alt="U2"&gt;&lt;/a&gt;&lt;br&gt;
Clearly, the accumulated error over lambda is smaller in this plot.&lt;/p&gt;

&lt;p&gt;The initial conditions for the second set of plots are as follows:&lt;br&gt;
&lt;/p&gt;

&lt;div class="highlight"&gt;&lt;pre class="highlight plaintext"&gt;&lt;code&gt;a = 0.9
end_lambda = 200
max_steps = 200
position = [0, 30., pi / 2, pi / 2] # Only difference
velocity = [-0.2, 0., 0.002]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;



&lt;h3&gt;
&lt;a href="http://openastronomy.org/Universe_OA/posts/2020/08/20200806_1009_jes24/#until-next-time" class="anchor"&gt;
&lt;/a&gt;
Until next time...
&lt;/h3&gt;

&lt;p&gt;Initially, I had planned to develop the Null Geodesics module, such that simulating a photon sheet would be possible through this module itself. The purpose would be applications in radiative transfer calculations, which require simulation of pathological orbits for better approximations in the strong gravity regime. But the issue of error accumulation has made it difficult to continue with this strategy. We have decided to make the current code merge-ready, while keeping the PR open, mainly because, the code performs well at larger initial distances. I have already made relevant changes to ensure the code is merge-ready. The status of the PR can be viewed at &lt;a href="https://github.com/einsteinpy/einsteinpy/pull/527"&gt;PR #527&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I am currently mulling the option of implementing the code in some low level language and then, building a wrapper around it, goal being to achieve better error-control, which I have not been able to obtain with Python/SciPy. Another approach that I am considering, is to restrict integration near the event horizon, based on step-size changes. Since multiple options are being explored and this is the last coding period, I have decided to make these blogs weekly. So my next blog should be up, next Friday. Hopefully, I will have solved this by then.&lt;/p&gt;&lt;/div&gt;</description><category>EinsteinPy</category><guid>http://openastronomy.org/Universe_OA/posts/2020/08/20200806_1009_jes24/</guid><pubDate>Thu, 06 Aug 2020 09:09:20 GMT</pubDate></item><item><title>gsoc_journey.update({“Chapter 2”: [“First Evaluations”, “Google Foobar?”]})</title><link>http://openastronomy.org/Universe_OA/posts/2020/07/20200729_1643_theand9/</link><dc:creator>Amogh Desai</dc:creator><description>&lt;div&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wmLYBAA422_jyF9QSWqS6w.jpeg"&gt;&lt;figcaption&gt;The James Webb Space Telescope is the $10 Billion successor to the Hubble Space Telescope. It can peer back into primordial times to show images of how the universe looked like around a quarter of a billion years after the Big Bang when the first stars and galaxies started to form.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Evaluation, a dreaded 10-letter word(just counted the number of letters😅) that brings stress and anxiety in everyone from a small kid to a golden-ager(sounds so much better than ‘old-person’ doesn’t it!!). The Cambridge dictionary defines evaluation &lt;em&gt;(noun)&lt;/em&gt;, &lt;em&gt;ɪˌvæl.juˈeɪ.ʃən, &lt;/em&gt;as &lt;strong&gt;“ the process of judging or calculating the quality, importance, amount, or value of something”. &lt;br&gt;
&lt;/strong&gt;Evaluations bring the best or worst out of people. I work better under pressure, I am one of those guys who cannot work until the room is on fire😂(I have a better phrase but it doesn’t quite qualify as PG-13😜).&lt;/p&gt;
&lt;p&gt;Evaluations although simple(the procedure) in GSoC are a big deal as they determine if you get (cue the Heavenly choir sound effect) &lt;strong&gt;the monthly stipend&lt;/strong&gt;🤑, just kidding, they determine if the student will be allowed to continue their projects. There are three &lt;strong&gt;&lt;em&gt;main &lt;/em&gt;&lt;/strong&gt;evaluations, one after each month to quantify if the project achieves the goals that it intended to and if the time was utilized most efficiently for the greatest possible impact.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;Now I understand procedural information is pretty boring so let’s directly jump to the fun stuff.&lt;/p&gt;
&lt;h4&gt;The Google FooBar Challenege&lt;/h4&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FvygSOchRZWfDLrq4QB9GQ.png"&gt;&lt;figcaption&gt;Looks like an invitation from a secret society!!&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;It happened at around 9:00 am on 9th of June, I had pulled an all-nighter and was half-asleep, finding ways to optimize stingray, when on typing _________ my browser window pivoted, the above image unfolded, my eyes widened, heartbeat skyrocketed, I looked in awe at my screen and then blinked 100 times, washed my face to truly confirm what I was seeing was true. &lt;br&gt;
It was the GOOGLE FOOBAR CHALLENGE!&lt;/p&gt;
&lt;p&gt;The FooBar challenge is a coveted, hidden challenge, a special invitation that is only sent to a selected few(makes me feel like James Bond😜). FooBar is Google’s secret hiring challenge. Google uses this to hire some of the best developers around the globe which they think can be a good match for their organization. &lt;br&gt;
Google sends an invitation based on one’s search history and problem-solving related keyword searches. If hidden magic in the Google search algorithm chooses you, you may receive an invitation for Google Foobar(makes me feel like Harry Potter😂).&lt;/p&gt;
&lt;blockquote&gt;Curious developers are known to seek interesting problems. Solve one from Google?&lt;/blockquote&gt;&lt;p&gt;Without thinking twice I instantly clicked &lt;em&gt;‘I want to play’. &lt;/em&gt;A black screen with a command-line interface greeted me.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*sxh-YA_0437rxf5lu4r8mA.png"&gt;&lt;figcaption&gt;&lt;strong&gt;&lt;em&gt;Objects we ardently pursue bring little happiness when gained; most of our pleasures come from unexpected sources.&lt;/em&gt;&lt;/strong&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The FooBar challenge is essentially an online, time-bound Competitive Programming(CP) challenge with 5 rounds of increasing difficulty, each round having a different number of questions. &lt;br&gt;
To spice things up (both in terms of interest and difficulty) Google has a storyline, progressing through the challenges you are asked for a friend who you’d like to refer for the challenge and eventually some personal details.&lt;/p&gt;
&lt;p&gt;FooBar proved to be a great learning experience for me as I’m not proficient in CP. I felt that the main aim was not to test the skills and knowledge of an individual but also how quickly they could learn and implement solutions.&lt;br&gt;
To learn more about FooBar you could read this &lt;a href="https://medium.com/plutonic-services/things-you-should-know-about-google-foobar-invitation-703a535bf30f"&gt;article&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;First GSoC Evaluation&lt;/h4&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Vw7I5eOtqMVRWRRLQcJKiw.png"&gt;&lt;figcaption&gt;The Google Summer of Code Dashboard a friend who I only see once every month😂😂&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The first evaluations were scheduled from the June 29, 2020 — July 3, 2020. My mentor Matteo and I had finalized on two main deliverables crucial for the foundations of the project. &lt;br&gt;
1. A &lt;a href="https://github.com/StingraySoftware/stingray/blob/9d7fa8a76d70df1eeb4501cb7ce701fb8e5bddf0/Benchmarks/Benchmark%20Analysis%20Report%202020%20-%20stingray.pdf"&gt;benchmark analysis report&lt;/a&gt; containing the time and memory benchmarks, profiling results for &lt;strong&gt;all the 31 functions&lt;/strong&gt;(3000+ lines of code)in the &lt;a href="https://github.com/StingraySoftware/stingray/blob/master/stingray/lightcurve.py"&gt;Lightcurve&lt;/a&gt;, &lt;a href="https://github.com/StingraySoftware/stingray/blob/master/stingray/powerspectrum.py"&gt;Powerspectrum&lt;/a&gt;, &lt;a href="https://github.com/StingraySoftware/stingray/blob/master/stingray/crossspectrum.py"&gt;Crossspectrum&lt;/a&gt;, AveragedPowerspectrum and AveragedCrossspectrum classes. &lt;br&gt;
2. Integrating &lt;a href="https://asv.readthedocs.io/en/stable/"&gt;airspeed velocity&lt;/a&gt;, a tool for benchmarking Python packages over their lifetime into stingray.&lt;/p&gt;
&lt;p&gt;The benchmark analysis report was a very time consuming yet possibly the most important step of the project. It singled out the functions that were causing the most delay and provided an overall estimate of the performance of stingray.&lt;/p&gt;
&lt;p&gt;Benchmarking was performed in jupyter-notebooks. Data points of sizes ranging from 1,000(1K) to 100,000,000(100M) were used for benchmarking. For basic benchmarking, the %timeit and %memit magic commands were used. Two profilers &lt;a href="http://joerick.me/posts/2017/12/15/pyinstrument-20/"&gt;Pyinstrument&lt;/a&gt; and &lt;a href="https://docs.python.org/3/library/profile.html#module-cProfile"&gt;CProfile&lt;/a&gt; were used to profile time and &lt;a href="https://pypi.org/project/filprofiler/"&gt;filprofiler&lt;/a&gt; was used to profile memory use. &lt;br&gt;
The results of the CProfile were saved as a &lt;em&gt;.pstats&lt;/em&gt; file which was then visualized using &lt;a href="https://jiffyclub.github.io/snakeviz/"&gt;Snakeviz&lt;/a&gt; and &lt;a href="https://github.com/jrfonseca/gprof2dot"&gt;gprof2dot&lt;/a&gt; in the form of a graphical viewer and call-graph respectively. &lt;br&gt;
The results of the Pyinstrument and filprofiler were saved as HTML files.&lt;br&gt;
The results of the &lt;em&gt;%timeit&lt;/em&gt; and &lt;em&gt;%memit&lt;/em&gt; magic commands was visualized in an interactive &lt;a href="https://plotly.com/"&gt;Plotly&lt;/a&gt; graph in the jupyter-notebook itself and also saved as an HTML file for easier access.&lt;/p&gt;
&lt;p&gt;The result of all of this was &lt;strong&gt;535 &lt;/strong&gt;files&lt;strong&gt; &lt;/strong&gt;ready for analysis, compiled into a &lt;strong&gt;74-page&lt;/strong&gt; &lt;a href="https://github.com/StingraySoftware/stingray/blob/9d7fa8a76d70df1eeb4501cb7ce701fb8e5bddf0/Benchmarks/Benchmark%20Analysis%20Report%202020%20-%20stingray.pdf"&gt;document&lt;/a&gt;, &lt;strong&gt;9 &lt;/strong&gt;new GitHub issues and &lt;strong&gt;3&lt;/strong&gt; new Pull Requests(for now, more to come soon).&lt;br&gt;
The functions causing the main slowdown i.e. &lt;em&gt;check_lightcurve, counts_err, cross_two_gtis, sort_counts, join, fourier_cross, rebin_data, rms_error, gti_border_bins, check_gtis, operation_with_other_lc and p_multitrial_from_single_trial &lt;/em&gt;were identified. These were further analyzed line-wise for deeper insights(also included in the document).&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*q3I5H5d6R8BEj73q7-YuAA.png"&gt;&lt;figcaption&gt;These look beautiful in the picture but analyzing 535 of these is super tedious!&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;airspeed velocity is a powerful tool that helps track the performance of a project over time, commit after commit. It is essentially a set of tests that can be run after every commit/change to check if the performance has been altered. It is used by numpy, scipy and astropy. The documentation is very well written(&lt;em&gt;sighs with relief&lt;/em&gt;). &lt;br&gt;
Thus, integrating airspeed velocity was &lt;em&gt;comparatively &lt;/em&gt;easier.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;A brief personal submission&lt;/em&gt;&lt;/strong&gt;&lt;br&gt;
The thing I enjoy the most about GSoC is that just like most of you reading this article, I didn’t know, rather hadn’t even heard the names of all the tools I used and am probably adept at right now. &lt;br&gt;
I start working every day not knowing how to solve the task(often daunting) in front of me or what the day holds. But at the end of the day, I’m confident that I’ve learnt something new. &lt;br&gt;
This excites me and I look forward to every new day working on my GSoC project&lt;/p&gt;
&lt;p&gt;You can check out my repository for airspeed velocity below. There is a link on the right side that leads to a webpage which looks relatively empty at the moment, someday hopefully, it will look like &lt;a href="https://pv.github.io/numpy-bench/"&gt;numpy’s asv&lt;/a&gt; page.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/theand9/stingray-benchmarks"&gt;theand9/stingray-benchmarks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can also check the benchmarks I have performed below.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/StingraySoftware/stingray/pull/477"&gt;Benchmarking Stingray -&amp;gt; GSoC 2020 by theand9 · Pull Request #477 · StingraySoftware/stingray&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thank you soo much for giving it a read. Please comment and leave a clap if you liked the article. Feel free to reach out to me on &lt;a href="https://www.linkedin.com/in/theand9/"&gt;Linkedin&lt;/a&gt;.&lt;br&gt;
Have an amazing day!! &lt;strong&gt;You are awesome!!!&lt;/strong&gt;&lt;/p&gt;
&lt;img src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=d4a51e01685d" width="1" height="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2020/07/20200729_1643_theand9/</guid><pubDate>Wed, 29 Jul 2020 15:43:01 GMT</pubDate></item><item><title>Week 7 &amp; 8: The end of second month!</title><link>http://openastronomy.org/Universe_OA/posts/2020/07/20200729_1113_siddharthlal25/</link><dc:creator>siddharthlal25</dc:creator><description>&lt;div&gt;&lt;h3 id="hey-sid-how-was-the-second-month"&gt;&lt;em&gt;Hey Sid, how was the second month?&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Awesome, lot’s of new learning experiences!&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;h3 id="so-tell-us-about-your-progress-in-the-last-two-weeks"&gt;&lt;em&gt;So, tell us about your progress in the last two weeks?&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;These two weeks went on designing the new data structure &lt;code class="language-plaintext highlighter-rouge"&gt;CCDData&lt;/code&gt; for storing &lt;code class="language-plaintext highlighter-rouge"&gt;ImageHDU&lt;/code&gt;, this was primarily done to make accessing data and header files easier, and secondly, to tackle the file closing issue that was encountered. Let me explain the file issue in brief:&lt;/p&gt;

&lt;p&gt;Suppose a user uses &lt;code class="language-plaintext highlighter-rouge"&gt;fitscollection&lt;/code&gt; at a location and gets a list of all &lt;code class="language-plaintext highlighter-rouge"&gt;FITS&lt;/code&gt; files, now by using the generator methods one can collect all &lt;code class="language-plaintext highlighter-rouge"&gt;ImageHDU&lt;/code&gt; listed in the data frame obtained from &lt;code class="language-plaintext highlighter-rouge"&gt;fitscollection&lt;/code&gt;, but once the generator is executed and used for collecting the &lt;code class="language-plaintext highlighter-rouge"&gt;ImageHDU&lt;/code&gt;s, the generator closes the open file handles from which &lt;code class="language-plaintext highlighter-rouge"&gt;ImageHDU&lt;/code&gt;s were accessed, this subsequently leads to error while accessing the collected &lt;code class="language-plaintext highlighter-rouge"&gt;ImageHDU&lt;/code&gt;s since its source &lt;code class="language-plaintext highlighter-rouge"&gt;FITS&lt;/code&gt; file were closed after the execution of generator. So, to tackle this we introduced &lt;code class="language-plaintext highlighter-rouge"&gt;CCDData&lt;/code&gt; which couples the data and header together in memory and can be accessed even if the filehandles get closed.&lt;/p&gt;

&lt;p&gt;The &lt;code class="language-plaintext highlighter-rouge"&gt;CCDData&lt;/code&gt; is based on &lt;code class="language-plaintext highlighter-rouge"&gt;AbstractArray&lt;/code&gt; interface which leads to a lot of code being reused with a bit of modification in the function signature.&lt;/p&gt;

&lt;h3 id="hmmm-so-what-next"&gt;&lt;em&gt;Hmmm, so what next?&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;I will be copying the saving versions of &lt;code class="language-plaintext highlighter-rouge"&gt;images&lt;/code&gt;, &lt;code class="language-plaintext highlighter-rouge"&gt;arrays&lt;/code&gt; and &lt;code class="language-plaintext highlighter-rouge"&gt;filenames&lt;/code&gt; from the previously closed PR, which would probably take a day or two. With some minor modifications, it would be good to go to the main code. After this, one can easily see the code in action! Next, I will also be implementing some macros for getting values from header of &lt;code class="language-plaintext highlighter-rouge"&gt;ImageHDU&lt;/code&gt; using keys directly in a method. After all this, we can bump up the version!&lt;/p&gt;

&lt;p&gt;Stay tuned to know more!&lt;/p&gt;

&lt;p&gt;-sl&lt;/p&gt;&lt;/div&gt;</description><category>JuliaAstro</category><guid>http://openastronomy.org/Universe_OA/posts/2020/07/20200729_1113_siddharthlal25/</guid><pubDate>Wed, 29 Jul 2020 10:13:56 GMT</pubDate></item><item><title>Chapter 3 : The Search Events Object</title><link>http://openastronomy.org/Universe_OA/posts/2020/07/20200728_1932_raahul-singh/</link><dc:creator>Raahul Singh</dc:creator><description>&lt;div&gt;&lt;h4&gt;Chapter 3 : The Search Events Object&lt;/h4&gt;&lt;img src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=ec306bbf6e51" width="1" height="1"&gt;
&lt;!-- TEASER_END --&gt;&lt;/div&gt;</description><category>SunPy</category><guid>http://openastronomy.org/Universe_OA/posts/2020/07/20200728_1932_raahul-singh/</guid><pubDate>Tue, 28 Jul 2020 18:32:44 GMT</pubDate></item><item><title>Google Summer of Code - Blog #4!</title><link>http://openastronomy.org/Universe_OA/posts/2020/07/20200728_1823_pkj-m/</link><dc:creator>pkj-m</dc:creator><description>&lt;div&gt;&lt;p&gt;Hello! Welcome to the latest blog post in my Google Summer of Code series. With this blog, we’ll marking the end of the second phase of evaluations! Additionally, this blog is going to be a little different from all the previous ones as we finally start to work and discuss about Python and the actual RADIS code base.&lt;/p&gt;

&lt;p&gt;Honestly, I am actually really glad that we’re finally at this stage. The monotonity of Cython and working on the same, huge Cython+CuPy file with more than a thousand lines of code was getting very frustrating :P&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;p&gt;In this blog, I shall be discussing more about the way we will integrate the GPU code with RADIS. But before we do that, let us first understand how RADIS handles this part right now, performing computations purely on the CPU. In order to compute the spectra of a molecule, we make use of module defined in RADIS known as &lt;code class="language-plaintext highlighter-rouge"&gt;Line-by-Line&lt;/code&gt; Module or LBL for short. This module contains numerous methods which are used for calculations of spectras. One of the most standard methods for computing the spectras is known as &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt;, which takes in inputs such as the molecule, isotopes, temperature, pressure, waverange over which the spectra needs to be calculated (and many other depending on the user’s requirements) and returns the result in the form of &lt;code class="language-plaintext highlighter-rouge"&gt;Spectrum&lt;/code&gt; object which neatly packs the information in single object which can then be processed as needed. This is what the user sees when they use RADIS. However, for us to develop and contribute, we also need to understand what goes on under the hood when a method such as &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt; is called.&lt;/p&gt;

&lt;p&gt;The first thing that occurs when &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt; is called is the validation and conversion of physical quantities (such as temprature and pressure) into default units that are assumed in the rest of the code. This is followed by the instantiation of an object from the &lt;code class="language-plaintext highlighter-rouge"&gt;SpectrumFactory&lt;/code&gt; class, which will contain all the information about the spectrum to be computed. This is all standard, something that will happen each time &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt; gets called. However, from here on, things start to get interesting as we look into the databank. Let us first understand what the purpose of a databank is: in order to compute a spectra for a specfic molecule, we need information about it. This could include data like line positions, line intensities, pressure-broadening parameters, etc. We don’t have to understand what each of these quantities mean or represent (to be honest, I don’t either) but the main idea we need to internalize is that we can’t compute a spectra without any information. So while it might be fair to say that a spectra can literally be generated from thin air, metaphorically that does not hold true.
This data, used to compute the spectra, can often be huge and therefore we try to avoid moving it around as much as possible. RADIS has implemented cache features in order to minimize the computations to be done on the raw databank. The important idea we need to focus on, is that once a databank has been loaded, we don’t have to load it once more if we ever call &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt; again for the same data (with maybe different waverange, etc.) This is done using extra memory: the original data that is loaded in the memory is saved as a dataframe, and every call made to &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt; first creates a copy of this data and works on it instead. This preserves the original dataset and allows us to use it again in case it is required, as the cost of the extra memory needed to save the copy. This was also the first problem I had to tackle: the GPU code we were going to use made use of &lt;code class="language-plaintext highlighter-rouge"&gt;npy&lt;/code&gt; formatted arrays to load the data, whereas RADIS was designed to load &lt;code class="language-plaintext highlighter-rouge"&gt;h5&lt;/code&gt; or &lt;code class="language-plaintext highlighter-rouge"&gt;par&lt;/code&gt; files. This meant I had to modify the databank loader RADIS used to support npy files. This was not a very significant problem however, as we could simply load the data separately instead of using the databank. The only issue with this approach would have been the repeated loading of data everytime the function call was made, but since we were using a small dataset, it wasn’t a significant issue and we decided to push it back and first focus on getting the GPU code to work with independent data being loaded.&lt;/p&gt;

&lt;p&gt;Once we had the data, the next step was to perform the computations. Now I won’t go into the details in this part since the mathematics behind these computations was something which: 1. I didn’t understand completely, and 2. I didn’t need to understand completely. Given my project already had a proof-of-concept code with the mathematics inplace, and my job was to make that code compatible with RADIS, I didn’t need to understand the nuances of it. I did go through the paper draft to understand the general idea behind the parallel approach we used, but I didn’t even try with the serial code since I had absolutely nothing to do with it. This mathematics was actually abstracted and encapsulated in another method &lt;code class="language-plaintext highlighter-rouge"&gt;eq_spectrum&lt;/code&gt;. Once this method finished execution, we would have the spectrum ready with all the information ready for the user. Within this method, there was one specific method that was of particular interest to us: &lt;code class="language-plaintext highlighter-rouge"&gt;_calc_broadening&lt;/code&gt;, responsible for the broadening step in the spectra calculation pipeline. This method was the bottleneck in the entire process, and the GPU code was primarily helping us speed up this particular portion of the process. My job here was to create another analogous method &lt;code class="language-plaintext highlighter-rouge"&gt;eq_spectrum_gpu&lt;/code&gt; which would do the exact same job as &lt;code class="language-plaintext highlighter-rouge"&gt;eq_spectrum&lt;/code&gt;, except instead of calling the sequence of serial methods to compute the spectra, I’d simply call the GPU code which was ready for us, and obtain the result. Some further processing of this result (which is computationally very inexpensive compared the rest of process), we have our spectra ready. It is a lot simpler and significantly easier than the CPU equivalent since we off-load the entire process to the Cython-compiled binaries and don’t have to worry about anything else. One problem I had with this (which is still not resolved as of writing this) is how do we fill in the missing information gaps in the Spectrum object, which in case of the CPU code were filled sequentially as the pipeline proceeded. Since our GPU code would only return the final result, I am not sure if we would be able to recover the preceding information bits which we didn’t obtain. Hopefully I’ll have the answer to it tomororrow when Erwan replies. That would pretty much finish my job here.&lt;/p&gt;

&lt;p&gt;However, another important bit of information that we’re skimming over is how we call the compiled binary file. This part is in fact what I will be working on over the coming week. The idea is that although right now I am using a simple binary file located in the same file to run the code, it is possible (and highly likely) that the same binary file will not work properly (or at all) in another system it is not meant for. As a consequence, the entire method might crash. In order to resolve this, we will be looking into placing the source code instead of the binary file in the RADIS codebase, and everytime a user needs to use the GPU version of &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt;, the code will first, automatically, compile the Cython file on their system, obtain the binary file meant for their system, and use that instead. The details regarding the implementation of this bit are still not clear as it is something I will be working on next week!
The next blog will cover the implementation details about this runtime-compilation setup along with a discussion on what is next! Thank you!&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2020/07/20200728_1823_pkj-m/</guid><pubDate>Tue, 28 Jul 2020 17:23:12 GMT</pubDate></item><item><title>GSOC 2020: Metadata searches using Fido</title><link>http://openastronomy.org/Universe_OA/posts/2020/07/20200728_1816_abhijeetmanhas/</link><dc:creator>Abhijeet Manhas</dc:creator><description>&lt;div&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/816/0*hF5ggLHngI8p-yaa"&gt;&lt;figcaption&gt;Missed Comet NEOWISE due to annoying cloud cover in Vadodara straight for 2 weeks :(&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;And that’s the central theme of the project :)&lt;/p&gt;
&lt;p&gt;It would now be possible to query clients that return metadata tables using Fido. So SunPy’s &lt;strong&gt;Fido&lt;/strong&gt; is a unified interface that allows searching and downloading solar physics data. In other words, it is a consistent and easy way to query most forms of solar physics data. It searches various archives and web services based on search attributes specified in the query.&lt;/p&gt;
&lt;p&gt;SunPy currently supports metadata facilities viz., JSOC Client, HEK Client, and Helio Client.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;SunPy’s hek module is used to access the information in the &lt;strong&gt;Heliophysics Event Knowledgebase&lt;/strong&gt; (HEK). HEK helps solar and heliospheric researchers locate features and events of interest.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Joint Science Operations Center&lt;/strong&gt; (JSOC) supports data products from various observatories and solar physics instruments.&lt;/p&gt;
&lt;p&gt;But they were not Fido compatible for metadata searches. PR #4358 addresses it.&lt;/p&gt;
&lt;h4&gt;Generic Class for metadata clients&lt;/h4&gt;&lt;p&gt;Since a lot of methods were similar in these clients, so I made a new superclass for them. JSOC, Helio, and HEK responses now inherit BaseQueryResponseTable to ease inspecting data retrieved through their clients. The idea was to retain the old look of response tables and also support a method to show all columns if required.&lt;/p&gt;
&lt;iframe src="http://openastronomy.org/Universe_OA/posts/2020/07/20200728_1816_abhijeetmanhas/" width="0" height="0" frameborder="0" scrolling="no"&gt;&lt;a href="https://medium.com/media/abb67aed8bc120d39e874af9ef9ab599/href"&gt;https://medium.com/media/abb67aed8bc120d39e874af9ef9ab599/href&lt;/a&gt;&lt;/iframe&gt;&lt;h4&gt;Deprecations&lt;/h4&gt;&lt;p&gt;~sunpy.net.hek.attrs.Time is deprecated since we can now use ~sunpy.net.attrs.Time for HEK queries, making it redundant.&lt;/p&gt;
&lt;p&gt;I also deprecated ~sunpy.net.jsoc.attrs.Keys because now the response table contains all keys by default. Users can specify the column names as *args in :meth:~sunpy.net.hek.HEKResponse.show for getting an ~astropy.table.Table instance containing only those columns.&lt;/p&gt;
&lt;iframe src="http://openastronomy.org/Universe_OA/posts/2020/07/20200728_1816_abhijeetmanhas/" width="0" height="0" frameborder="0" scrolling="no"&gt;&lt;a href="https://medium.com/media/d663f6878e23ecc59cd3f0b7cce932ef/href"&gt;https://medium.com/media/d663f6878e23ecc59cd3f0b7cce932ef/href&lt;/a&gt;&lt;/iframe&gt;&lt;p&gt;Finally, HEKTable was renamed to HEKResponse for consistency in naming.&lt;/p&gt;
&lt;h4&gt;Devguide for writing Fido Clients&lt;/h4&gt;&lt;p&gt;I have explained how to write a simple Fido client in &lt;a href="https://github.com/sunpy/sunpy/pull/4387"&gt;PR 4387&lt;/a&gt;. This was the first documentation pull request that I made in SunPy. Work is in progress for adding details of writing an “AttrWalker” and registring an “Attr” for Fido.&lt;/p&gt;
&lt;iframe src="http://openastronomy.org/Universe_OA/posts/2020/07/20200728_1816_abhijeetmanhas/" width="0" height="0" frameborder="0" scrolling="no"&gt;&lt;a href="https://medium.com/media/449c51d6f6528dbb0a236344fe4a9524/href"&gt;https://medium.com/media/449c51d6f6528dbb0a236344fe4a9524/href&lt;/a&gt;&lt;/iframe&gt;&lt;h4&gt;Other Stuff&lt;/h4&gt;&lt;p&gt;I reviewed&lt;a href="https://github.com/sunpy/sunpy/pull/4394"&gt; PR #4394&lt;/a&gt;, which allows XRSClient to download reprocessed data for GOES Satellites. I also need to add support for this new pattern in my &lt;a href="https://github.com/sunpy/sunpy/pull/4321"&gt;dataretriever refactoring pull request, #4321&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I also go through the discussions present in the net issues so I can fit their fixes in my project. I also suggest updates to the description of outdated issues and check if they still persist, like in &lt;a href="https://github.com/sunpy/sunpy/issues/2401"&gt;Issue #2401&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/sunpy/sunpy/issues/2032"&gt;Issue #2032&lt;/a&gt; was fixed, so now helio wsdl_retriever returns the first valid taverna link.&lt;/p&gt;
&lt;p&gt;We are going to now enter the last phase of the work period!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CARPE NOCTEM!&lt;/strong&gt;&lt;/p&gt;
&lt;img src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=909bda98b771" width="1" height="1"&gt;&lt;/div&gt;</description><category>SunPy</category><guid>http://openastronomy.org/Universe_OA/posts/2020/07/20200728_1816_abhijeetmanhas/</guid><pubDate>Tue, 28 Jul 2020 17:16:19 GMT</pubDate></item><item><title>Week 7 &amp; 8: Full Throttle Ahead!</title><link>http://openastronomy.org/Universe_OA/posts/2020/07/20200727_1715_sahilyadav27/</link><dc:creator>Sahil Yadav</dc:creator><description>&lt;div&gt;&lt;p&gt;In the last blog post, I was discussing about creating a MAGICEventSource and calling it inside DL1DataWriter to make HDF5 files with the CTA ML data format from the ROOT files.&lt;/p&gt;
&lt;p&gt;I have integrated the MAGICEventSource to writer.py and also updated it to ctapipe v0.8.0. This code is housed at &lt;a href="https://github.com/cta-observatory/dl1-data-handler/pull/90"&gt;PR #90&lt;/a&gt;. There were a few small issues with the metadata and transformations. The current ctapipe MCHeader container has variables needed to be filled by the input file, but the ROOT file has separate RunHeaders, so we were in a dilemma as to which metadata to store.&lt;/p&gt;
&lt;p&gt;Finally, we decided to create our own MCHeader container inside the MAGICEventSource and use this to store metadata instead of using the ctapipe container.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;Also, for the simtel files that have been analysed till now, the telescope pointing Alt and Az are constant and can be hardcoded when we train the deep learning models, to subtract them from the arrival direction Alt and Az. But for the ROOT files, this value is not constant so we will have to create another column into the HDF5 file and pass this value to CTLearn when training on ROOT files.&lt;/p&gt;
&lt;p&gt;There was also some confusion regarding the transformations applied to these Alt and Az values before dumping them to the HDF5 file. We were finally able to figure out what was going wrong with our approach, and plotted the difference between arrival direction and pointing position.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/386/1*FG648RrN8nWpKPCJ04IF8g.png"&gt;&lt;figcaption&gt;Pointing position vs Arrival direction&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;So now all that is left is create the metadata container, and the MAGIC dataset to start training the CTLearn models before we start working on VERITAS data.&lt;/p&gt;
&lt;img src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=87dd0823ba1b" width="1" height="1"&gt;&lt;/div&gt;</description><category>CTLearn</category><guid>http://openastronomy.org/Universe_OA/posts/2020/07/20200727_1715_sahilyadav27/</guid><pubDate>Mon, 27 Jul 2020 16:15:13 GMT</pubDate></item><item><title>Connecting Spark Logs with Jupyter UI</title><link>http://openastronomy.org/Universe_OA/posts/2020/07/20200727_1341_techguybiswa/</link><dc:creator>Biswarup Banerjee</dc:creator><description>&lt;div&gt;&lt;p&gt;I have successfully been able to show the kernel logs in realtime into the Jupyter front end.&lt;br&gt;
So basically I configured the Jupyter kernel to dump the logs in real-time in a log file.&lt;br&gt;
Then I created a new function that runs in a separate thread and pulls the content of the log file every 2 seconds.&lt;br&gt;
&lt;!-- TEASER_END --&gt;
Then the fetched data is sent to the front end via sockets every two seconds ONLY when the user opens the log.&lt;br&gt;
So when the user is not in the logs page the socket communication and the reading of the log file do not happen.&lt;/p&gt;
&lt;blockquote&gt;def fetch_logs(comm):&lt;/blockquote&gt;&lt;blockquote&gt;f = open(“log.file”, “r”)&lt;/blockquote&gt;&lt;blockquote&gt;logs = f.read()&lt;/blockquote&gt;&lt;blockquote&gt;comm.send({‘status’ : ‘log_fetched_success’ , ‘log’ : logs})&lt;/blockquote&gt;&lt;blockquote&gt;if (ipython.ev(‘shouldFetchLog’)):&lt;/blockquote&gt;&lt;blockquote&gt;threading.Timer(2.0,fetch_logs, [comm]).start()&lt;/blockquote&gt;&lt;img src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=3a522a2b89e4" width="1" height="1"&gt;&lt;/div&gt;</description><category>astronomy-commons</category><guid>http://openastronomy.org/Universe_OA/posts/2020/07/20200727_1341_techguybiswa/</guid><pubDate>Mon, 27 Jul 2020 12:41:49 GMT</pubDate></item><item><title>GSoC 2020: glue-solar project 2.2</title><link>http://openastronomy.org/Universe_OA/posts/2020/07/20200726_1213_kakirastern/</link><dc:creator>Kris Stern</dc:creator><description>&lt;div&gt;&lt;p&gt;The end of the 2nd Coding Period of this year’s Google Summer of Code has finally arrived. I cannot help but noticed the many things I have learned/built over the past two months for both the &lt;a href="http://glueviz.org/"&gt;glue&lt;/a&gt; Graphical User Interface (GUI), as well for its solar physics plugin glue-solar. One of the tricks I have learned is writing up tests for the GUI programming I have done in the process, which is using Qt for Python to port the entire &lt;a href="https://wiki.qt.io/PySide"&gt;PySide&lt;/a&gt; module to Qt5. My main approach is quite simple: Imitate, modify, test. Since glue has a fairly well developed codebase, it is not hard to find sample code snippets within it for inspiration of new code to add. The GUI unit tests are no exception to this rule. And, I would like to use the opportunity to share the experience with more novice contributors to the software, so perhaps somehow someone else somewhere down the line will be able to benefit from this.&lt;/p&gt;
&lt;p&gt;The first and foremost concern we should have regarding any type of unit testing is what we should check for functionality-wise. Let me take some code I have just written over the past few days which adds NDData (a data structure native to Astropy) support to glue and enables the loading of various types of astronomical data more readily, such as the standard FITS files. As I have discussed with my mentors via the glue-solar IRC channel, we have observed that NDData is much like laser was back in the 1960’s (e.g. as &lt;a href="https://www.nytimes.com/1964/05/06/archives/developer-of-the-laser-calls-it-a-solution-seeking-a-problem.html"&gt;reported by NYTimes&lt;/a&gt; then) was a solution in search of a problem before its wider adoption by the astronomy community for LSST, DKIST and CCDProc data. Now we are in the process of integrating it into glue. The original conception of this, at least in principle, is to use the simple and fluid structure of NDData to help process for example FITS data. This is because there are no generic NDData files in existence at all. This is to facilitate the manipulation of not only the data component, but also its units, mask, uncertainty, and meta attributes, which are quite common in the handling of astronomical data (pun intended). With such a motivation in mind, we have added a nddata.py module to the glue/core/data_factories directory in a &lt;a href="https://github.com/glue-viz/glue/pull/2164"&gt;PR&lt;/a&gt; at glue. To complete the PR, it is standard practice to add tests where applicable, so we have added a testing module called test_nddata.py in the glue/core/data_factories/tests directory to not only serve as a routine, but also to test whether the code has been properly debugged, which caught all of the major known bugs I have inadvertently introduced to the codebase before testing.&lt;/p&gt;
&lt;p&gt;The GUI unit test I have written is as follows:&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;iframe src="http://openastronomy.org/Universe_OA/posts/2020/07/20200726_1213_kakirastern/" width="0" height="0" frameborder="0" scrolling="no"&gt;&lt;a href="https://medium.com/media/ba8b2229df9cf1b5b160c70096612af9/href"&gt;https://medium.com/media/ba8b2229df9cf1b5b160c70096612af9/href&lt;/a&gt;&lt;/iframe&gt;&lt;p&gt;I have studied the other tests in the same glue/core/data_factories/tests carefully before I proceed to write the code, as I have observed that the most basic test to write is to test the data loader. Of course, official docs helped me greatly as well. As for the formatted NDData factory, that would be hard to test. I will need to consult with my mentors, before deciding on whether a separate unit test for that later.&lt;/p&gt;
&lt;p&gt;One takeaway I have come away with in the process is that sufficient time is needed before I could brainstorm and come up with a tangible plan to write unit tests for any established codebase, and that I should not rush through the process. This is a lesson that I will definitely keep in mind. There is no use going through the motion and not enjoying the process as I go along, not to mention the omissions that I would make otherwise without a well thought-out plan.&lt;/p&gt;
&lt;img src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=db0e0ef935b2" width="1" height="1"&gt;&lt;/div&gt;</description><category>SunPy</category><guid>http://openastronomy.org/Universe_OA/posts/2020/07/20200726_1213_kakirastern/</guid><pubDate>Sun, 26 Jul 2020 11:13:28 GMT</pubDate></item></channel></rss>