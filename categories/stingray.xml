<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Universe OpenAstronomy (Posts about stingray)</title><link>http://openastronomy.org/Universe_OA/</link><description></description><atom:link href="http://openastronomy.org/Universe_OA/categories/stingray.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Sat, 09 Oct 2021 05:45:34 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Google Summer of Code- Final Evaluation</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210823_1027_rashmiraj137/</link><dc:creator>Raj Rashmi</dc:creator><description>&lt;div&gt;&lt;h4&gt;&lt;strong&gt;Topic: Implement JAX based automatic differentiation to Stingray&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;The project involved the study of modern statistical modelling to augment the accuracy, speed, and robustness of the likelihood function, into a software package called Stingray. This report demonstrates the experiment done for a combination of different optimizers to fit the scipy.optimize function. Another emphasis is to investigate the gradient calculation using JAX and compare it with scipy.optimize.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Introduction:&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;The proposed milestone was to investigate the room for improvement to enhance the overall performance of modelling to Stingray, using JAX. However, the current stage of the model is still a sandbox model. Stingray is astrophysical spectral timing software, a library in python built to perform time series analysis and related tasks on astronomical light curves. JAX is a python library designed for high-performance numerical computing. Its API for numerical functions is based on NumPy, a collection of functions used in scientific computing. Both Python and NumPy are widely used and familiar, making JAX simple, flexible, and easy to adopt. It can differentiate through a large subset of python’s features, including loops, ifs, recursion, and closures, and it can even take derivatives of derivatives. Such modern differentiation packages deploy a broad range of computational techniques to improve applicability, run time, and memory management.&lt;/p&gt;
&lt;p&gt;JAX utilizes the grad function transformation to convert a function into a function that returns the original function’s gradient, just like Autograd. Beyond that, JAX offers a function transformation jit for just-in-time compilation of existing functions and vmap and pmap for vectorization and parallelization, respectively.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;h4&gt;Experiment:&lt;/h4&gt;&lt;p&gt;The powerlaw and lorentzian function are the most used to describe periodograms in astronomy. In practice, we use the sum of these components to design a realistic model. For the analysis here we consider a quasi-periodic oscillation and a constant and try to fail the algorithm by, (i) reduce the amplitude, (ii) start the optimization process with parameters very far away from the true parameters, (iii) try different optimizers to experiment on different sensitive aspect of the current likelihood calculation. The current ongoing milestone is to try alternatives of scipy.optimize but this requires series of tests for the same.&lt;/p&gt;
&lt;p&gt;The above tests can be visualized in the notebook added on Github: &lt;a href="https://github.com/rashmiraj137/GSoC-Project"&gt;https://github.com/rashmiraj137/GSoC-Project&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Results:&lt;/h4&gt;&lt;p&gt;During the experiment, it was observed that the algorithm is sensitive to input parameters i.e., it fails for couple of combinations like when the amplitude is set far away from true value, precisely blow absolute value of 1. In general, if we set the parameters very far away from the true value, it fails to approximate the likelihood function. In the notebook (&lt;a href="https://github.com/rashmiraj137/GSoC-Project/blob/main/GSoC_Evaluation%20Notebook.ipynb"&gt;link&lt;/a&gt;), we demonstrate the room for improvement in the current algorithm by choosing a different set of parameters. The current data fit for the evaluation of likelihood happens using scipy.optimize.minimize function. However, there exists numerous ways to do this. SciPy optimize provides functions for minimizing (or maximizing) objective functions, possibly subject to constraints. It includes solvers for nonlinear problems (with support for both local and global optimization algorithms), linear programming, constrained and nonlinear least-squares, root finding, and curve fitting. The problem with the current minimization algorithm is that it converges at local minimum instead of global, i.e. it is not very robust. Recently, Machine Learning has evident development in such optimization tools. The strategy was to find alternatives that potentially accelerate the code, makes it robust.&lt;/p&gt;
&lt;p&gt;For this experiment case, we choose a couple of optimizers and compare the robustness to Powell (the current optimizer used). So we visualize the fit for couple of optimizers like :&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-neldermead.html"&gt;minimize(method=’Nelder-Mead’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-powell.html"&gt;minimize(method=’Powell’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-cg.html"&gt;minimize(method=’CG’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-bfgs.html"&gt;minimize(method=’BFGS’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-newtoncg.html"&gt;minimize(method=’Newton-CG’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html"&gt;minimize(method=’L-BFGS-B’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-tnc.html"&gt;minimize(method=’TNC’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-cobyla.html"&gt;minimize(method=’COBYLA’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html"&gt;minimize(method=’SLSQP’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;minimize(method = “trust-constr”)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This notebook (&lt;a href="https://github.com/rashmiraj137/GSoC-Project/blob/main/GSoC_Evaluation%20Notebook.ipynb"&gt;link&lt;/a&gt;) has results using each method and it was observed that Nelder-Mead is more robust as compared to other optimizers. Another optimizer like dogleg, trust-ncg, might be good as well, but the jacobian and hess need to be calculated for them.&lt;/p&gt;
&lt;h4&gt;Future Work:&lt;/h4&gt;&lt;p&gt;JAX has now its own version of scipy.optimize.minimize but it has couple of bugs and is not as robust as scipy.optimize.minimize. Finding an alternative for scipy.optimize.minimize such that it doesn’t fails even if the start parameters are far away from the true parameters was a goal for this project but unfortunately JAX did not assist that well enough. But there might be a superior algorithm to scipy.optimize.minimize that can be useful.&lt;/p&gt;
&lt;h4&gt;Repositories:&lt;/h4&gt;&lt;p&gt;&lt;a href="https://github.com/StingraySoftware/stingray"&gt;https://github.com/StingraySoftware/stingray&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/StingraySoftware/notebooks"&gt;GitHub - StingraySoftware/notebooks: Tutorial notebooks for Stingray&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Blog Post:&lt;/h4&gt;&lt;p&gt;1. &lt;a href="https://raj-rashmi741.medium.com/jax-based-automatic-differentiation-introduction-of-modern-statistical-modeling-to-stingray-1bc26da7571f"&gt;JAX-based automatic differentiation: Introduction of modern statistical modeling to Stingray.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2. Insight of Implementation of JAX to stingray- GSoC coding period!&lt;/p&gt;
&lt;p&gt;3. &lt;a href="https://raj-rashmi741.medium.com/gsoc-update-2d16a70cc267?source=your_stories_page-------------------------------------"&gt;GSoC update!&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://raj-rashmi741.medium.com/time-to-review-my-gsoc-project-c34297f2dc81"&gt;4. Time to review my GSoC Project.&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Profiles:&lt;/h4&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/rashmiraj137"&gt;https://github.com/rashmiraj137&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;LinkedIn: &lt;a href="https://www.linkedin.com/in/rashmi-raj-4b8a2b106/"&gt;https://www.linkedin.com/in/rashmi-raj-4b8a2b106/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Medium: &lt;a href="https://raj-rashmi741.medium.com/"&gt;https://raj-rashmi741.medium.com/&lt;/a&gt;&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=578c0088bcbd" width="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210823_1027_rashmiraj137/</guid><pubDate>Mon, 23 Aug 2021 09:27:57 GMT</pubDate></item><item><title>Time to review my GSoC Project</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210816_1603_rashmiraj137/</link><dc:creator>Raj Rashmi</dc:creator><description>&lt;div&gt;&lt;p&gt;With the end of the GSoC project, I will give this blog to summarise the JAX based optimization to analyze its applicability to enhance the loglikelihood calculation. The goal is to analyze, (i) the performance of different optimizers to evaluate the loglikelihood function, (ii) demonstrated the robustness of JAX to calculate gradients. And talk about the current code and corresponding improvement due to JAX.&lt;/p&gt;
&lt;p&gt;The application of loglikelihood fitting to periodograms is discussed in [1]. Let us start with analyzing best-fit power spectrum (i) with different sets of optimizers namely: &lt;em&gt;minimize(method=’Nelder-Mead’, ’Powell’, ’CG’, ’BFGS’, ’Newton-CG’, ’L-BFGS-B’, ’TNC’, ’COBYLA’, ’SLSQP’, ’trust-constr’, ’dogleg’, ’trust-ncg’, ’trust-krylov’, ’trust-exact’). &lt;/em&gt;The problem setting shifts the start and test parameters to study the graph of best fit optimizer using different “methods” listed above. First, we will stick with the Powell optimizer and try to check what is the current sensitivity of the implementation.&lt;/p&gt;
&lt;p&gt;Currently, we seek to find a solution to the problem when the optimization algorithm often gets stuck in local minima, terminate without meeting its formal success criteria, or fails due to any contributing factor. Possible ways are: (1) add more Lorentzian components, (2) reduce the amplitude, (3) start the optimization process with parameters very far away from the true parameters, (4) experiment with the different optimizers/ “methods” to investigate if there is more superior algorithm compared to Powell.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*oEfdtoL2Fa0XAbjmugMvnA.png"&gt;&lt;figcaption&gt;Reference: blog.gitguardian.com&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;So far the &lt;em&gt;Powell&lt;/em&gt; and &lt;em&gt;Nelder-Mead &lt;/em&gt;gives almost the same best-fit curve compared to other optimizers, surprisingly even better than &lt;em&gt;BFGS(which is a well-known &lt;/em&gt;numerical optimizer for an iterative method for solving unconstrained nonlinear optimization problems. This directs to more investigation with (1) and (2) and (3). Both (2) and (3) makes the algorithm fail with the current &lt;em&gt;scipy.optimize.minimize, &lt;/em&gt;and we can see the&lt;em&gt; &lt;/em&gt;graph as given below.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1SA0BwGc48I8qRozxuK6CQ.png"&gt;&lt;/figure&gt;&lt;p&gt;I am still holding on to try &lt;a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.optimize.minimize.html"&gt;jax.scipy.optimize.minimize&lt;/a&gt; instead of &lt;a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.optimize.minimize.html"&gt;scipy.optimize.minimize&lt;/a&gt; and analyze the increment in robustness. Another way to enhance the current algorithm alongside experimenting with different optimisers is:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Use a different gradient finding method.&lt;/li&gt;&lt;li&gt;Speed up objective function.&lt;/li&gt;&lt;li&gt;Reduce the number of design variables.&lt;/li&gt;&lt;li&gt;Choose a better initial guess.&lt;/li&gt;&lt;li&gt;Use parallel processing.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;In my next blog, I will provide a more detailed explanation of current events. In this blog, I highlighted the emphasis of analysis.&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;[1] Maximum likelihood fitting of X-ray power density spectra: Application to high-frequency quasi-periodic oscillations from the neutron star X-ray binary 4U1608-522. Didier Barret, Simon Vaughan. &lt;a href="https://arxiv.org/abs/1112.0535"&gt;https://arxiv.org/abs/1112.0535&lt;/a&gt;&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=c34297f2dc81" width="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210816_1603_rashmiraj137/</guid><pubDate>Mon, 16 Aug 2021 15:03:25 GMT</pubDate></item><item><title>A Glimpse into my GSoC project</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210812_2028_dhruv9vats/</link><dc:creator>Dhruv Vats</dc:creator><description>&lt;div&gt;&lt;p&gt;While all my blog posts till now were kind of abstract, here I will try to show some of the technical details of the project without making it too bloated. So as a one-line description, I had to study, implement and integrate a spectral estimation technique, namely the &lt;em&gt;Multitaper Periodogram³&lt;/em&gt; (and its derivatives¹), which are used to analyze astronomical time series.&lt;/p&gt;
&lt;h5&gt;Why spectral representations?&lt;/h5&gt;&lt;p&gt;Before getting into the how of spectral analysis and its estimation, a brief sidenote on the why. Why do we even bother to study the spectral properties of a time series? It turns out, some of the determining characteristics or defining parameters associated with a certain time series are better &lt;em&gt;brought out&lt;/em&gt; in their spectral representations (frequency domain representations).&lt;/p&gt;
&lt;p&gt;As an example, the power spectral density is a common tool to try and unearth the periodic element(s) in a time series. Such spectral analysis techniques, at their core, are enabled by the Fourier Transform, and if you’d like to gain a better intuitive understanding of it, do check out &lt;a href="https://www.youtube.com/watch?v=spUNpyF58BY"&gt;this awesome video&lt;/a&gt; by &lt;a href="https://www.youtube.com/c/GrantSanderson"&gt;Grant Sanderson&lt;/a&gt; on &lt;a href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw"&gt;3Blue1Brown&lt;/a&gt;.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;h5&gt;Multi what?&lt;/h5&gt;&lt;p&gt;Spectral analysis isn’t without its fair share of practical complications, in fact, far from it. But there have been quite a few very effective techniques to mitigate them.&lt;/p&gt;
&lt;p&gt;One of them is &lt;em&gt;tapering (&lt;/em&gt;multiplying the time series with a bell-shaped function), effective in reducing spectral leakage. Tapering a time series as a way of obtaining a spectral estimator with acceptable bias properties is an important concept. The loss of information (contained at the extremes of the time series) inherent in tapering can often be avoided either by prewhitening or by using Welch’s overlapped segment averaging.&lt;/p&gt;
&lt;p&gt;The multitaper periodogram is another approach to recover information lost due to tapering. This approach was introduced by Thomson (1982)³ and involves the use of multiple orthogonal tapers, having approximately uncorrelated spectral densities.&lt;/p&gt;
&lt;p&gt;In the multitaper method, the data is windowed or tapered, but this method differs from the traditional methods in the tapers used, which are the most band-limited functions amongst those defined on a finite time domain, and also, these tapers are orthogonal, enabling us to average the &lt;em&gt;eigenspectrum&lt;/em&gt; (spectrum estimates from individual tapers) from more than one tapers to obtain a superior estimate in terms of noise. The resulting spectrum has low leakage, low variance, and retains information contained in the beginning and end of the time series.&lt;/p&gt;
&lt;p&gt;The tapers used are the discrete prolate spheroidal sequences (DPSS), or, the Slepians (Slepian 1978)⁴.&lt;/p&gt;
&lt;h5&gt;A look at the DPSS tapers&lt;/h5&gt;&lt;p&gt;Let’s consider a time series sampled from an autoregressive process of order 4, AR(4), which has been frequently exemplified in literature¹ in similar contexts.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/657/1*h-k9Xm1CgerMQVIeKbfyOw.png"&gt;&lt;figcaption&gt;A time series sampled from an autoregressive process of order 4.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;A good way to gain an intuitive understanding of the properties of the DPSS tapers, and how they affect the time series, is to visualize the effect. Given here are the time and frequency domain representations of the tapers and the tapered time series.&lt;/p&gt;
&lt;p&gt;The first 8 tapers and the corresponding tapered time series⁵.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/645/1*SEG0OEstBuAgP5OqgnS4zA.png"&gt;&lt;/figure&gt;&lt;p&gt;This showcases the product of a windowing function and a time series quite well. Next let’s have a look at their spectral representations⁵, more specifically, their power spectrum densities (PSD).&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/680/1*nvqbaD08gDf1zdh9P2ljqQ.png"&gt;&lt;figcaption&gt;For this example, we took the normalized half-bandwidth product to be equal to 4 (NW = 4), resulting in 8 tapers being used. The spectral concentration in the band [-W, W] can then be seen from the plots. (Here N, the number of data points, is 1024, hence, W = 4/N = 0.003906)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;There is a significant increase in the bias of the PSD estimates as the spectral concentration of the tapers worsens. To prevent these estimates with greater biases from affecting the final averaged estimate but still use the variance reductions they bring, we weigh the different estimates according to their spectral concentration (percentage of energy concentrated in the desired frequency band).&lt;/p&gt;
&lt;p&gt;This can be kicked up a notch by using what is called adaptive weighing, which adaptively (duh?!) combines the different estimates, calculating the weights using an iterative process.&lt;/p&gt;
&lt;h5&gt;A brief summary of the Multitaper spectral estimation&lt;/h5&gt;&lt;a href="https://medium.com/media/3f23eb5c2c41460b8793fbc2e6fbc04d/href"&gt;https://medium.com/media/3f23eb5c2c41460b8793fbc2e6fbc04d/href&lt;/a&gt;&lt;p&gt;This summary, by no means, is an exhaustive explanation of the multitapering concept. Further exploration of the topic is highly encouraged. Use the references as the starting point.&lt;/p&gt;
&lt;h5&gt;The Final Result&lt;/h5&gt;&lt;p&gt;Using all the techniques outlined here, let's see how well can this multitaper periodogram estimate the true spectrum of this auto-regressive process. Also added is the classical periodogram (also sometimes referred to as a naïve spectrum estimator because of its basic estimation process) for comparison.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/707/1*7eYiabftab1cYuMpBC4uqA.png"&gt;&lt;figcaption&gt;Here the multitaper estimate uses the adaptive weighting technique and the first 7 DPSS tapers&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;All this functionality is now implemented in &lt;a href="https://github.com/StingraySoftware/stingray"&gt;Stingray&lt;/a&gt;²&lt;/p&gt;
&lt;h5&gt;References&lt;/h5&gt;&lt;p&gt;[1]: Springford, Aaron, Gwendolyn M. Eadie, and David J. Thomson. 2020. “Improving the Lomb–Scargle Periodogram with the Thomson Multitaper.” The Astronomical Journal (American Astronomical Society) 159: 205. doi:10.3847/1538–3881/ab7fa1.&lt;/p&gt;
&lt;p&gt;[2]: Huppenkothen, Daniela, Matteo Bachetti, Abigail L. Stevens, Simone Migliari, Paul Balm, Omar Hammad, Usman Mahmood Khan, et al. 2019. “Stingray: A Modern Python Library for Spectral Timing.” The Astrophysical Journal (American Astronomical Society) 881: 39. doi:10.3847/1538–4357/ab258d.&lt;/p&gt;
&lt;p&gt;[3]: Thomson, D. J. 1982. “Spectrum Estimation and Harmonic Analysis.” IEEE Proceedings 70: 1055–1096. &lt;a href="https://ui.adsabs.harvard.edu/abs/1982IEEEP..70.1055T."&gt;https://ui.adsabs.harvard.edu/abs/1982IEEEP..70.1055T.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4]: Slepian, D. 1978. “Prolate Spheroidal Wave Functions, Fourier Analysis, and Uncertainty-V: The Discrete Case.” Bell System Technical Journal (Institute of Electrical and Electronics Engineers (IEEE)) 57: 1371–1430. doi:10.1002/j.1538–7305.1978.tb02104.x&lt;/p&gt;
&lt;p&gt;[5]: D.B. Percival and A.T. Walden, Spectral Analysis for Physical Applications: Multitaper and Conventional Univariate Techniques. Cambridge, U.K.: Cambridge Univ. Press, 1993.&lt;/p&gt;
&lt;p&gt;[6]: Thomson, D. J. 1990. “Time series analysis of Holocene climate data.” Philosophical Transactions of the Royal Society of London. Series A, Mathematical and Physical Sciences (The Royal Society) 330: 601–616. doi:10.1098/rsta.1990.0041&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=25c0fe3296dd" width="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210812_2028_dhruv9vats/</guid><pubDate>Thu, 12 Aug 2021 19:28:01 GMT</pubDate></item><item><title>GSoC update!</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210803_0200_rashmiraj137/</link><dc:creator>Raj Rashmi</dc:creator><description>&lt;div&gt;&lt;p&gt;GSoC started four months ago and it is not just about knowing more about the open-source that made the experience great! My mentors made it way cooler than I thought it would be. I was writing my Master thesis, for the last three months and surely, it has been a super productive summer for me! The best part is I get to do things at my own pace. My project particularly hasn’t been very easy to implement. I need to bridge a Machine Learning algorithm in the existing codebase. The fun part is venturing with different notebooks and figuring out with intuition, what could be efficient in terms of computational time, efficiency, cost etc. But as of now, the struggle has been to define the problem as exactly to achieve the result. But I will keep working on finding a solution with my mentor Daniela, and trust that struggle will bring some positive construction in Stingray.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*2GcUC2cgvKapk8Lj"&gt;&lt;/figure&gt;&lt;p&gt;The current data fit for the evaluation of likelihood happens using scipy.optimize.minimize function. However, there exists numerous ways to do this. SciPy optimize provides functions for minimizing (or maximizing) objective functions, possibly subject to constraints. It includes solvers for nonlinear problems (with support for both local and global optimization algorithms), linear programming, constrained and nonlinear least-squares, root finding, and curve fitting. The problem with the current minimization algorithm is that it converges at local minimum instead of global, i.e. it is not very robust. Recently, Machine Learning has evident development in such optimization tools. The strategy for ahead is that I will work on finding alternatives that potentially accelerate the code, makes it robust.&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=2d16a70cc267" width="1"&gt;
&lt;!-- TEASER_END --&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210803_0200_rashmiraj137/</guid><pubDate>Tue, 03 Aug 2021 01:00:58 GMT</pubDate></item><item><title>Halfway into GSoC</title><link>http://openastronomy.org/Universe_OA/posts/2021/07/20210719_1916_dhruv9vats/</link><dc:creator>Dhruv Vats</dc:creator><description>&lt;div&gt;&lt;p&gt;This introductory adventure to Open Source is already at its midpoint, and while the learnings have been great and the experiences meaningful, I’m sure many of my fellow participants feel that a program like this should have an extended duration, and I am no exception. Such an extended timeline could provide many benefits, such as the ability to work on more complex and sophisticated projects, more time to collaborate and improve, to name a few.&lt;/p&gt;
&lt;h5&gt;First Evaluation&lt;/h5&gt;&lt;p&gt;Another noteworthy thing concerning GSoC that happened in the last week was that the results of the first evaluation were declared, and while most cleared it, some didn’t. Although there is little to no need to question their abilities, sometimes life just doesn’t go as planned; it seems easy to say that that’s what the real test is, nevertheless it can quickly become something tricky to cope with.&lt;/p&gt;
&lt;h5&gt;What next?&lt;/h5&gt;&lt;p&gt;While most of the “proposed” work has been done, I will now be preparing some tutorials for the newly added functionality and tools, in an attempt to reduce the barrier to experimentation, use, and possible adoption of these new techniques into the workflow of its users.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;So while this could take a good amount of time if quality is needed, there will surely some be time to play around with other things, but what exactly will end up happening will be answered by time.&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=b6f9ec014333" width="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/07/20210719_1916_dhruv9vats/</guid><pubDate>Mon, 19 Jul 2021 18:16:00 GMT</pubDate></item><item><title>A Month into GSoC</title><link>http://openastronomy.org/Universe_OA/posts/2021/07/20210705_2037_dhruv9vats/</link><dc:creator>Dhruv Vats</dc:creator><description>&lt;div&gt;&lt;p&gt;It’s almost been a month since the start of GSoC’s coding period and the work, I’m glad to write, is progressing at a steady and satisfactory rate.&lt;/p&gt;
&lt;h5&gt;The Developments&lt;/h5&gt;&lt;p&gt;The last time around, my first ever not-so-meaningless contribution to open-source had just got merged, and I was really happy about it. But what that also did was, get me over the initial anxiety and intimidation I might have been feeling towards open-source. This, I think, has also helped speed things along.&lt;/p&gt;
&lt;p&gt;While I started working on the optional features of my project around 2 weeks ago, I had to scrape the initial implementation because it turned out to be very, very slow and therefore had to be completely replaced with a better and more efficient approach, which was a bit less straightforward. But now, two weeks into experimenting and iterating, a new pull-request has been opened with the newly implemented efficient version of the feature, and while it's still a few minor commits away from its final form, the core functionality works as expected and, if everything goes as expected, which is never a guarantee, a hefty part of my proposal’s objectives will be complete.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;And while this does not guarantee anything, I’d be lying if I said that I am not hoping for something exciting to do as I might have time to try out other things. What exactly, I honestly don’t know, but if I find myself in that minority who actually like what there doing, it’ll be an absolute privilege, which I’m looking forward to and wishing for.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*DvmM4lhclsVa5tIwl5um4A.png"&gt;&lt;figcaption&gt;Just some pastel colors for you to look at&lt;/figcaption&gt;&lt;/figure&gt;&lt;h5&gt;The Goal&lt;/h5&gt;&lt;p&gt;What follows might be a very steep change in topic, but is one, that I think lies at the root of many seemingly normal activities. This is just something that has been on my mind lately, and what writing is, if not a tool to better understand yourself?&lt;/p&gt;
&lt;p&gt;I feel like an invisible aura is building around me saying that you are at a stage in life where you need to man up, where you should have everything together and figured out, but whenever I try and assess myself in this context, I always, without fail, fell short of it and by a good margin. While the contrasting opposite of this would be saying that I am everything I ever wanted to be and have nothing to work towards, would be outright arrogant and even dangerous, there must be a balance somewhere, right?&lt;/p&gt;
&lt;p&gt;But why should I have it all figured out, what’s even the need? And while statements like these can be argued against using something like, because everyone is doing it, and this is the way, they give the vibes of being in a pipeline you’ve been pushed into and now have no option but to pass through. And this, I think, many will agree, is not a very desirable situation.&lt;/p&gt;
&lt;p&gt;This need to progress towards something also spurs off many questions, one of which is “the why ?”. The why, is an oh-so-difficult question to answer that honestly makes me feel frustrated at times, not knowing to what end all the efforts are being put.&lt;/p&gt;
&lt;p&gt;While it can be argued that this is a ridiculous thing to think about, and one should not set overly optimistic expectations, this, I feel, contradicts the notion of elegance that I somehow have associated with the fundamental workings of the world. If someone asked me to comment on the secrets of the Universe, I’d be very comfortable with using the words elegant and sophisticated, even though I basically know nothing about it? Why? Is this just a desire to find meaning in everything, or is there something else at play?&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=805d42b1b5ce" width="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/07/20210705_2037_dhruv9vats/</guid><pubDate>Mon, 05 Jul 2021 19:37:36 GMT</pubDate></item><item><title>Insight of Implementation of JAX to stingray- GSoC coding period!</title><link>http://openastronomy.org/Universe_OA/posts/2021/07/20210705_1420_rashmiraj137/</link><dc:creator>Raj Rashmi</dc:creator><description>&lt;div&gt;&lt;p&gt;In the last blog, I wrote about Introduction to JAX and Automatic Differentiation. In this one, my plan for the next stage of implementation. Currently, I am working on the modeling notebook (&lt;a href="https://github.com/StingraySoftware/notebooks/blob/main/Modeling/ModelingExamples.ipynb"&gt;https://github.com/StingraySoftware/notebooks/blob/main/Modeling/ModelingExamples.ipynb&lt;/a&gt;) to re-design it using JAX, especially to make optimization more robust by having JAX compute gradients on the likelihood function.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/380/1*u_c4S0h60T1IECOBQVTS1A.jpeg"&gt;&lt;/figure&gt;&lt;p&gt;My mentor Daniela highlighted the issue that the current implementation is not robust using NumPy. The plan is to keep working on the current modeling notebook replacing NumPy by jax.numpy and also use grad, jit, vmap, random functionality of JAX.&lt;br&gt;When it comes to re-design, understanding the current design and the possible drawback and issues with corresponding packages comes on you first and I am trying them out. One such challenge is importing emcee into jupyter notebook for sampling. Despite making sure, I download the dependency in the current virtual environment and then making sure I import emcee into the notebook, it is still acting weird and showing an error: emcee not installed! Can’t sample! It looks like a clash of dependencies.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/240/1*JtGB50sLscB1BBPt9k3pfw.jpeg"&gt;&lt;figcaption&gt;Trying to have fun while it lasts!&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;For now, the plan is to solve every bug I face in the journey and then proceed with understanding how everything connects and the next step is to come up with the report of optimization using JAX. Stay tuned for more on how JAX can accelerate and augment the current modeling framework.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;I would recommend one video for anyone who wants to understand the functionality of JAX better and relate more to my study (click &lt;a href="https://www.youtube.com/watch?v=0mVmRHMaOJ4&amp;amp;ab_channel=GoogleCloudTech"&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=1756040fa5ae" width="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/07/20210705_1420_rashmiraj137/</guid><pubDate>Mon, 05 Jul 2021 13:20:55 GMT</pubDate></item><item><title>GSoC Progress Report? Almost Done!</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_1523_dhruv9vats/</link><dc:creator>Dhruv Vats</dc:creator><description>&lt;div&gt;&lt;p&gt;With only 2 weeks into the coding period, it feels good to say that the bulk of the work associated with the 2 milestones is &lt;em&gt;almost &lt;/em&gt;done. Almost because the newly added functionality is yet to be battle-tested and while there is always room for change and improvements, my focus will be shifting from the main objective to the optional objectives.&lt;/p&gt;
&lt;h5&gt;The Project&lt;/h5&gt;&lt;p&gt;Scientific jargon ahead!&lt;/p&gt;
&lt;p&gt;The study and interpretation of time-series data have become an integral part of modern-day astronomical studies and a common approach for characterizing the properties of time-series data is to estimate the power spectrum of the data using the periodogram. But the periodogram as an estimate suffers from being:&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;ol&gt;&lt;li&gt;Statistically inconsistent (that is, its variance does not go to zero as the number of data samples reach infinity),&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;2. Biased for finite samples, and&lt;/p&gt;
&lt;p&gt;3. Suffers from spectral leakage.&lt;/p&gt;
&lt;figure&gt;&lt;a href="https://github.com/StingraySoftware/stingray"&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/700/1*5GHLU2O-S9d06amIH5ESBA.png"&gt;&lt;/a&gt;&lt;figcaption&gt;Stingray is a spectral-timing software package for astrophysical X-ray (and other) data.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;My project aimed at implementing and integrating a superior spectral estimation technique, known as the Multitaper periodogram, into a Software Package called Stingray, a sub-organization of OpenAstronomy.&lt;/p&gt;
&lt;p&gt;This Multitaper algorithm uses windows or tapers (bell-shaped functions), which are multiplied with the time-series data before finding its frequency domain estimate. These windows, called the discrete prolate spheroidal sequences (DPSS), help mitigate the problems mentioned above.&lt;/p&gt;
&lt;p&gt;Any more technical stuff and this blog will start taking the shape of my proposal, so I’ll leave it here, but for anyone more interested, &lt;a href="https://www.dropbox.com/s/g2m2p10en8ygpmz/Proposal.pdf?dl=0"&gt;here&lt;/a&gt; is the proposal.&lt;/p&gt;
&lt;h5&gt;The Process&lt;/h5&gt;&lt;p&gt;I started working on the project in early March, before submitting the proposal, and it initially included writing a wrapper around an external package to make the use of this tool coherent with the rest of the project. While a proof-of-concept implementation was put together, it was later decided to use SciPy to do the grunt work, as it already was a dependency, somewhat changing the initial milestones.&lt;/p&gt;
&lt;p&gt;So in a way, I was already working on the project before the results were announced and ended up opening a pull request with the newly added method 1 week into the coding period and it got merged a week later, which happened to be fairly early in the GSoC program coding timeline. Perks of open-source?!&lt;/p&gt;
&lt;p&gt;So while there are sure to be improvements and additions in terms of coherency and features, this does give a bit more breathing room and time for experimentation and exploration, and I’ll try and make good use of it, primarily by working on the optional but quite good-to-have features.&lt;/p&gt;
&lt;p&gt;For anyone interested, &lt;a href="https://github.com/StingraySoftware/stingray/pull/578"&gt;here&lt;/a&gt; is the PR.&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=6239f301b23" width="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_1523_dhruv9vats/</guid><pubDate>Mon, 21 Jun 2021 14:23:27 GMT</pubDate></item><item><title>JAX-based automatic differentiation: Introduction of modern statistical modeling to Stingray</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_1404_rashmiraj137/</link><dc:creator>Raj Rashmi</dc:creator><description>&lt;div&gt;&lt;p&gt;I assume everyone reading this is already aware of two classical forms of differentiation, namely symbolic and finite differentiation. Symbolic differentiation operates on expanded mathematical expressions which lead to inefficient code and introduction of truncation error while finite differentiation deals with round-off errors. Optimized calculation of derivatives is crucial when it comes to training neural networks or mathematical modeling using bayesian inference. Both classical methods are slow at computing partial derivatives of a function with respect to many inputs, as is needed for gradient-based optimization algorithms. Here, automatic differentiation comes to the rescue. Automatic differentiation is a powerful tool to automate the calculation of derivatives and is preferable to more traditional methods, especially when differentiating complex algorithms and mathematical functions.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*NzyxsrkiLjjyjiIuCf123w.png"&gt;&lt;figcaption&gt;Photo Source: Wikipedia&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Stingray is astrophysical spectral timing software, a library in python built to perform time series analysis and related tasks on astronomical light curves. JAX is a python library designed for high-performance numerical computing. Its API for numerical functions is based on NumPy, a collection of functions used in scientific computing. Both Python and NumPy are widely used and familiar, making JAX simple, flexible, and easy to adopt. It can differentiate through a large subset of python’s features, including loops, ifs, recursion, and closures, and it can even take derivatives of derivatives. Such modern differentiation packages deploy a broad range of computational techniques to improve the applicability, run time, and memory management.&lt;/p&gt;
&lt;p&gt;JAX utilizes the grad function transformation to convert a function into a function that returns the original function’s gradient, just like Autograd. Beyond that, JAX offers a function transformation jit for just-in-time compilation of existing functions and vmap and pmap for vectorization and parallelization, respectively.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5hEIw7ou4eeAX-HnP_23_A.png"&gt;&lt;figcaption&gt;&lt;em&gt;Mini-MLP(Multiple layer Perceptron) execution time for 10,000 updates with a batch size of 1024. Source: AI Zone&lt;/em&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;As seen in the above figure, PyTorch has much more effective in terms of execution speed than TensorFlow when it came to implementing fully connected neural layers. For low-level implementations, on the other hand, JAX offers impressive speed-ups of an order of magnitude or more over the comparable Autograd library. JAX is faster than any other library when MLP implementation was limited to matrix multiplication operations.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;How do we decide the ideal library to go with?&lt;/strong&gt;&lt;/h4&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/626/1*8-_tFBCgszXxQwkMEwEcjA.jpeg"&gt;&lt;/figure&gt;&lt;p&gt;Our choice will first depend on the history of the project we start working on, if the code already uses PyTorch then most probably we will end up using PyTorch for writing our code. For general differentiable programming with low-level implementations of abstract mathematical concepts, JAX offers substantial advantages in speed and scale over Autograd while retaining much of Autograd’s simplicity and flexibility, while also offering surprisingly competitive performance against PyTorch and TensorFlow.&lt;/p&gt;
&lt;p&gt;I am implementing modern stingray modeling to Stingray software as a part of my GSoC project. Reference to Stingray source code: &lt;a href="https://github.com/StingraySoftware"&gt;https://github.com/StingraySoftware&lt;/a&gt;. Reference to JAX-based automatic differentiation: &lt;a href="https://jax.readthedocs.io/en/latest/jax-101/index.html"&gt;https://jax.readthedocs.io/en/latest/jax-101/index.html&lt;/a&gt;.&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=1bc26da7571f" width="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_1404_rashmiraj137/</guid><pubDate>Mon, 21 Jun 2021 13:04:09 GMT</pubDate></item><item><title>The first weeks of X-Rays and Electron.Js</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210620_1829_rachittshah/</link><dc:creator>Rachitt Shah</dc:creator><description>&lt;div&gt;&lt;p&gt;Python, X-rays, and JSON files that don’t comply and make you cry.&lt;/p&gt;
&lt;p&gt;GSoC has officially started!&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/498/0*AXMzJYLpLP0sFPgE"&gt;&lt;figcaption&gt;We’re going to code!&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Since is a Desktop App made with Electron and Python, my first priority was getting DAVE to compile again. The process was tedious and took a good amount of time to fix the flask backends which power DAVE.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;Another error that came while getting DAVE up was that DAVE uses Stingray’s APIs which are lower than 2.0, which gave a lot of broken packages. Fixing this was something that would be covered in the POC enhancements, however, it was important to tackle this to build the MVP.&lt;/p&gt;
&lt;p&gt;Looked at some major changes in the coming 0.3 for Stingray and how to accommodate them, so we don’t have issues later.&lt;/p&gt;
&lt;p&gt;Had exams alongside, so the first week was less effort extensive for GSoC.&lt;/p&gt;
&lt;p&gt;Waiting for the next week to write more about my progress and journey!&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=84f7557d97a7" width="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210620_1829_rachittshah/</guid><pubDate>Sun, 20 Jun 2021 17:29:09 GMT</pubDate></item></channel></rss>