<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Universe OpenAstronomy (Posts about radis)</title><link>http://openastronomy.org/Universe_OA/</link><description></description><atom:link href="http://openastronomy.org/Universe_OA/categories/radis.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Thu, 01 Jul 2021 04:39:04 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Chapter 1: First Flight</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_2240_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;div&gt;&lt;p&gt;Hey! Missed me? I’m back with another blog, the first related to the Coding Period. Got some progress and interesting observation to share!&lt;/p&gt;
&lt;h3&gt;Ready -&amp;gt; Set -&amp;gt; Code -&amp;gt; Analyze&lt;/h3&gt;
&lt;p&gt;The first thing I did in the coding period, was analyse the problem and get a feasible approach to resolve it.&lt;br&gt;&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; Find the complexity of the Legacy and LDM method.&lt;br&gt;
&lt;strong&gt;Solution:&lt;/strong&gt; Run some benchmarks and find the bottleneck step.&lt;br&gt;&lt;/p&gt;
&lt;p&gt;First I chose the &lt;strong&gt;Legacy&lt;/strong&gt; method because if its simpler architecture. I ran some benchmarks varying the &lt;code class="language-text"&gt;spectral range&lt;/code&gt; of &lt;code class="language-text"&gt;OH&lt;/code&gt; and &lt;code class="language-text"&gt;CO2&lt;/code&gt; molecule to get similar number of lines. I kept parameters like &lt;code class="language-text"&gt;pressure&lt;/code&gt;, &lt;code class="language-text"&gt;temperature&lt;/code&gt;, &lt;code class="language-text"&gt;broadening_max_width&lt;/code&gt;, &lt;code class="language-text"&gt;wstep&lt;/code&gt;, etc constant to see the dependence of Legacy method on &lt;strong&gt;Spectral range&lt;/strong&gt;. &lt;br&gt;&lt;/p&gt;
&lt;p&gt;In order to get similar number of lines, I created a function which will take the &lt;strong&gt;Spectrum Factory&lt;/strong&gt; &lt;code class="language-text"&gt;dataframe&lt;/code&gt; and select the target number of lines. But the issue with Pandas dataframe is that when modify the dataframe there are chances that the metadata will get lost and we will no longer be able to do Spectrum calculation. To avoid this we have to drop the right number of lines with &lt;code class="language-text"&gt;inplace=True&lt;/code&gt;. So we will need to fix the number of lines and then we can proceed ahead with the benchmarking. Every parameter is the same except the Spectral Range.  Full code &lt;a href="https://gist.github.com/anandxkumar/cbe12f47170e1d71a82f4b246bd01dcc"&gt;here&lt;/a&gt;.&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Earlier we assumed that the complexity of Legacy method is: &lt;br&gt;
&lt;strong&gt;&lt;code class="language-text"&gt;Voigt Broadening = Broadening_max_width * spectral_range/math.pow(wstep,2) * N&lt;/code&gt;&lt;/strong&gt; &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Thus I was expecting to have different calculation time for both benchmarks. But to my surprise the computational times were almost equivalent! I re-ran each benchmarks &lt;strong&gt;100 times&lt;/strong&gt; just to be sure and more precise about it. Following were the observations:&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of lines - &lt;b&gt;{‘OH’: 28143, ‘CO’: 26778}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;Total Calculation time(Avg) -  &lt;b&gt;{‘OH’: 4.4087, ‘CO’: 3.8404000000000003}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;Total Voigt_Broadening TIME(Avg) - &lt;b&gt;{‘OH’: 3.1428814244270327, ‘CO’: 3.081623389720917}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;spectral_range - &lt;b&gt;{‘OH’: 38010, ‘CO’: 8010}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;Legacy_Scale - &lt;b&gt;{‘OH’: 4x10^14, ‘CO’: 8x10^13}&lt;/b&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are some inference we can make from the above observation:&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A)&lt;/strong&gt; The bottleneck step(Voigt Broadening) loosely depends on &lt;code class="language-text"&gt;Spectral Range&lt;/code&gt;.&lt;br&gt;
&lt;strong&gt;B)&lt;/strong&gt; The complexity of Voigt Broadening needs to be modified because there is a difference of order of &lt;strong&gt;~10&lt;/strong&gt; in the Legacy Scaled value of OH and CO2.&lt;br&gt;&lt;/p&gt;
&lt;p align="center"&gt;
&lt;span class="gatsby-resp-image-wrapper" style="display: block; margin-left: auto; margin-right: auto;"&gt;
&lt;a class="gatsby-resp-image-link" href="https://anandkumar-blog.netlify.app/static/d9b32b4e96e6cd9a91016a49ad940239/0b533/Blog2.png" rel="noopener" style="display: block;" target="_blank"&gt;
&lt;span class="gatsby-resp-image-background-image" style="padding-bottom: 100%; display: block;"&gt;&lt;/span&gt;
&lt;img alt="Blog2" class="gatsby-resp-image-image" src="https://anandkumar-blog.netlify.app/static/d9b32b4e96e6cd9a91016a49ad940239/0b533/Blog2.png" style="width: 100%; height: 100%; margin: 0; vertical-align: middle;" title="Blog2"&gt;
&lt;/a&gt;
&lt;/span&gt;&lt;br&gt;
&lt;b&gt;Credits - Me :p&lt;/b&gt;&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;So in order to do some analysis, we first need data of different steps in the broadening phase and conditions of various Spectrum which brings me to the &lt;strong&gt;Code&lt;/strong&gt; part in &lt;strong&gt;Coding Period.&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Profiler Class&lt;/h3&gt;
&lt;p&gt;The aim of this class is to replace all the print statements by a common &lt;code class="language-text"&gt;start&lt;/code&gt;, &lt;code class="language-text"&gt;stop&lt;/code&gt;, &lt;code class="language-text"&gt;_print&lt;/code&gt; method. Earlier each step computational time was done using &lt;code class="language-text"&gt;time()&lt;/code&gt; library. Now the whole codebase is being refactored with the Profiler class that will do all the work based on the &lt;code class="language-text"&gt;verbose&lt;/code&gt; level. In addition to this the biggest benefit is that each step will be stored in a dictionary with its computational time that will help me gather data to find which step is in actual bottleneck and further which part of the function is the most expensive time wise. A simple example is below:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Before:&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;if __debug__:
t0 = time()
..........
..........
if __debug__:
t1 = time()
.........
.........
if __debug__:
if self.verbose &amp;gt;= 3:
printg("... Initialized vectors in {0:.1f}s".format(t1 - t0))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;After:&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;self.profiler.start(
key="init_vectors", verbose=3, details="Initialized vectors"
)
.........
.........
self.profiler.stop("init_vectors")&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So using a common key we can make it happen. This will be stored in the conditons of &lt;code class="language-text"&gt;Spectrum&lt;/code&gt; object in the &lt;code class="language-text"&gt;'profiler'&lt;/code&gt; key. All these Spectrums and their conditions can be exported using a &lt;a href="https://radis.readthedocs.io/en/latest/spectrum/spectrum.html#spectrum-database"&gt;SpecDatabase&lt;/a&gt;. This will create a csv file comprising of all the parameters of all Spectrums which will be useful in getting some insights.
-&amp;gt; &lt;a href="https://github.com/radis/radis/pull/286"&gt;PR LINK&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Digging in whiting_jit&lt;/h3&gt;
&lt;p&gt;Based on several benchmarks, it is estimated that around &lt;strong&gt;70-80%&lt;/strong&gt; time is spent on calculating the broadening. The broadening part has the following hierarchy:&lt;br&gt;
&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;_calc_broadening()
-&amp;gt; _calc_lineshape()
-&amp;gt; _voigt_broadening()
-&amp;gt; _voigt_lineshape()
-&amp;gt; whiting_jit()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;On close inspection we observed that &lt;strong&gt;80-90%&lt;/strong&gt; time is spent on &lt;code class="language-text"&gt;whiting_jit&lt;/code&gt; process. Going further down in &lt;code class="language-text"&gt;whiting_jit&lt;/code&gt;, &lt;strong&gt;60-80%&lt;/strong&gt; time is spent on &lt;strong&gt;lineshape calculation.&lt;/strong&gt; Below is the formula:&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;lineshape = (
(1 - wl_wv) * exp(-2.772 * w_wv_2)
+ wl_wv * 1 / (1 + 4 * w_wv_2)
# ... 2nd order correction
+ 0.016 * (1 - wl_wv) * wl_wv * (exp(-0.4 * w_wv_225) - 10 / (10 + w_wv_225))
)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The whole process can be divided into 4 parts:&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;    part_1 =   (1 - wl_wv) * exp(-2.772 * w_wv_2)

part_2 =    wl_wv * 1 / (1 + 4 * w_wv_2)

# ... 2nd order correction
part_3 =  0.016 * (1 - wl_wv) * wl_wv * exp(-0.4 * w_wv_225)

part_4 =  - 10 / (10 + w_wv_225)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The complexity of each part comes out: &lt;br&gt;
&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;    o1 = broadening__max_width * n_lines / wstep

O(part_1) = n_lines * o1
O(part_2) = n_lines * 4 * o1
O(part_3) = (n_lines)**2 * o1
O(part_4) = o1 &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Running several benchmark showed us that &lt;strong&gt;part_3&lt;/strong&gt; takes the most time out of all steps. So clearly we can see that the complexity of Legacy method is not dependent on
Spectral Range but rather &lt;code class="language-text"&gt;Number of Calculated Lines&lt;/code&gt;,&lt;code class="language-text"&gt;broadening__max_width&lt;/code&gt; and &lt;code class="language-text"&gt;wstep&lt;/code&gt;. It may seem that the complexity of Legacy method is:&lt;br&gt;&lt;/p&gt;
&lt;p align="center"&gt;&lt;b&gt; n_lines^2 * broadening__max_width * n_lines / wstep&lt;/b&gt;&lt;/p&gt; &lt;br&gt;
&lt;p&gt;But inorder to prove this we need more benchmarks and evidence to verify this and it may involve normalization of all steps in lineshape calculation!&lt;br&gt; &lt;/p&gt;
&lt;p&gt;So the goal for the next 2 weeks is clear:&lt;br&gt;
&lt;b&gt;i)&lt;/b&gt; Refactor the entire codebase with Profiler.&lt;br&gt;
&lt;b&gt;ii)&lt;/b&gt; Find the complexity of &lt;strong&gt;Legacy Method&lt;/strong&gt; with the help of more benchmark and analysis.&lt;br&gt;
&lt;b&gt;iii)&lt;/b&gt; Do the same for &lt;strong&gt;LDM Method&lt;/strong&gt;!&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Ok I guess time’s up! See you after 2 weeks :)&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_2240_anandxkumar/</guid><pubDate>Mon, 21 Jun 2021 21:40:32 GMT</pubDate></item><item><title>GSoC - 1</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210620_0300_gagan-aryan/</link><dc:creator>Gagan Aryan</dc:creator><description>&lt;div&gt;&lt;p&gt;This is the first blog that documents the coding period of my GSoC21 journey. I learnt a few interesting things in these two weeks, as I expected I would. So, let’s dive in and see if you knew few of these stuff I learnt.&lt;/p&gt;
&lt;h3 id="starting-off-"&gt;Starting off !!!&lt;/h3&gt;
&lt;p&gt;I started off by getting a brief idea of the scope of the changes that could be done to the dataframe. This was the task I had decided on for the first week. Whenever we are involved in a project that runs for a period of anywhere between 2-4 months it is important to have a timeline or a roadmap of sorts to be able to look back to. This doesn’t really have to be something rigid. We can chose to deviate from it and infact deviations are bound to happen due to multiple reasons. It can happen because of an unexpected bug in between, or because you came across some alternative that you did not consider at the start or simply because it is one of those projects that gives better insights as you dwell into it.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;Every good GSoC proposal consists of a tentative timeline that depicts the work we plan on doing as the weeks progress. Here is the timeline I had submitted in my proposal.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Timeline1" src="https://gagan-aryan.netlify.app/images/gsoc-1/timeline1.png"&gt;&lt;br&gt;
&lt;img alt="Timeline2" src="https://gagan-aryan.netlify.app/images/gsoc-1/timeline2.png"&gt;&lt;/p&gt;
&lt;p&gt;So as per this I was supposed to finish off the refactors to the dataframe and also finish setting up the benchamarks. But I was unable to complete these. I had underestimated the work it would take to complete them. Nonetheless, I also did have some time to look up at the things I am supposed to do in the second half of the coding period.&lt;/p&gt;
&lt;h3 id="memory-and-time-performance-benchmarks---tic-tok"&gt;Memory and Time performance benchmarks - Tic-Tok&lt;/h3&gt;
&lt;p&gt;Before making any changes to the codebase Erwan suggested me to have the benchmarks setup. So what do I mean by this? To make sure that the changes I am making to the code are indeed reducing the memory consumption of the computations we use a few tools that help us track the memory consumption for various calculations as a function of git commits. There are multiple tools that help us do this. Radis already used a tool developed by &lt;a href="https://github.com/airspeed-velocity/asv"&gt;airspeed velocity&lt;/a&gt; to track the memory computions. I ran into a lot of troubles in setting these up and a lost a lot of valuable time in the process ultimately Erwan fixed it and I was able to run the benchmarks on my machine.&lt;/p&gt;
&lt;p&gt;The benchmarks still seem to take a lot of time to run though and for them to be feasible to be used a tool through which I can check the performance regularly there are a few things I need to learn. I hope to pick these up in the next few days.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Performance" src="https://gagan-aryan.netlify.app/images/gsoc-1/brace-yourselves.png"&gt;&lt;/p&gt;
&lt;p&gt;We are also trying to look at a few other alternatives that can be used instead of asv. I will update the you guys regarding this in the next blog post.&lt;/p&gt;
&lt;h3 id="oh-pandas-here-i-deal-with-you-"&gt;Oh Pandas here I deal with you !&lt;/h3&gt;
&lt;h4 id="lets-ditch-a-few-columns"&gt;Let’s ditch a few columns&lt;/h4&gt;
&lt;p&gt;We can reduce the memory usage of pandas by using one really simple trick - avoid giving loading the columns that are not required for computation. Below I demostrate how just dropping a few columns can provide significant improvement in the memory consumption. I am using &lt;code&gt;HITEMP-CH4&lt;/code&gt; database for demonstration.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt; 1
&lt;/span&gt;&lt;span class="lnt"&gt; 2
&lt;/span&gt;&lt;span class="lnt"&gt; 3
&lt;/span&gt;&lt;span class="lnt"&gt; 4
&lt;/span&gt;&lt;span class="lnt"&gt; 5
&lt;/span&gt;&lt;span class="lnt"&gt; 6
&lt;/span&gt;&lt;span class="lnt"&gt; 7
&lt;/span&gt;&lt;span class="lnt"&gt; 8
&lt;/span&gt;&lt;span class="lnt"&gt; 9
&lt;/span&gt;&lt;span class="lnt"&gt;10
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre class="chroma"&gt;&lt;code class="language-python"&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;radis.io.hitran&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;hit2df&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hit2df&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"06_HITEMP2020_2000.0-2500.0.par"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;memory_usage&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"deep"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="n"&gt;memory&lt;/span&gt; &lt;span class="n"&gt;usage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;30.5&lt;/span&gt; &lt;span class="n"&gt;MB&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s2"&gt;"id"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"iso"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;memory_usage&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"deep"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="n"&gt;memory&lt;/span&gt; &lt;span class="n"&gt;usage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;25.4&lt;/span&gt; &lt;span class="n"&gt;MB&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;The peak memory usage before dropping the columns was 30.5 MB and once I remove a few columns the peak memory usage becomes 25.4 MB. I have already implemented the dropping of id column and handled the case of single isotope as well by dropping the column and istead just storing the information of the isotope as a meta attribute. We have also finalised on the discarding of the other columns by considering the physics of these quantities. Let’s check out a few of them. Since I haven’t already implemented the optimisations that follow I will save the implementation details for the next blog.&lt;/p&gt;
&lt;h4 id="einsteins-coeffecients-and-linestrengths"&gt;Einstein’s Coeffecients and Linestrengths&lt;/h4&gt;
&lt;p&gt;There are four parameters of interest to describe the intensity of a line : Linestrength $(int)$, Einstein emission coefficient $(A)$ and Einstein absorption coefificent $(B_{lu})$, Einstein induced emission coefficient $(B_{ul})$. All of them are somehow linked to the Squared Transition Dipole Moment $(R)$. &lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;$$ B_{lu}=10^{-36}\cdot\frac{8{\pi}^3}{3h^2} R_s^2 \cdot 10^{-7} $$&lt;br&gt;
$$ B_{ul}=10^{-36}\cdot\frac{8{\pi}^3}{3h^2} \frac{gl}{gu} R_s^2 \cdot 10^{-7} $$&lt;br&gt;
$$ A_{ul}=10^{-36}\cdot\frac{\frac{64{\pi}^4}{3h} {\nu}^3 gl}{gu} R_s^2 $$&lt;/p&gt;
&lt;p&gt;So now the idea would be to drop the $int$ column and use $A_{ul}$ to calculate the value of $int$ from it. The reason to drop $int$ and not $A_{ul}$ some databases like &lt;code&gt;ExoMol&lt;/code&gt; databases only provide the value of $A_{ul}$.&lt;/p&gt;
&lt;h4 id="concat-better"&gt;Concat better&lt;/h4&gt;
&lt;p&gt;For anyone who wants concate multiple datafiles pandas tends to become useless as the memory scales up. I started out experimenting concat operations inorder to cluster the isotopes of each type, run computations on them and later concat them. But I later learnt that since this data is already in the form of a single dataframe, indexing is a better parameter to track the memory consumption. Nonetheless there are a few other places in Radis where we process multiple files and concat them, hence this experiment would help us decide how we can chose to replace the current approach with a better one. I tried out three methods. I was using some random dummy datafiles of around 780 MBs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Normal &lt;code&gt;pandas.concat&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Concat with a doubly ended queue&lt;/li&gt;
&lt;li&gt;Concat with parquet&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here are the results of each of these methods -&lt;/p&gt;
&lt;div class="tab" id="cbbfde2b70afc7e2"&gt;
&lt;div class="tab__links"&gt;
&lt;button class="tab__link"&gt;pandas.concat&lt;/button&gt;
&lt;button class="tab__link"&gt;deque&lt;/button&gt;
&lt;button class="tab__link"&gt;parquet&lt;/button&gt;
&lt;/div&gt;
&lt;div class="tab__content" id="c6b1e010ed52c9f8"&gt;
&lt;h4 id="pandasconcat"&gt;pandas.concat&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;CPU Time - 0:02:43.797588
Peak Memory Usage - 4.1050 GB
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="tab__content" id="a412292f0cf68165"&gt;
&lt;h4 id="pandasconcat-with-a-doubly-queue"&gt;pandas.concat with a doubly queue&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;CPU Time - 0:02:34.484612
Peak Memory Usage - 3.7725 GB
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="tab__content" id="c655ff276a8106d7"&gt;
&lt;h4 id="concat-with-parquet"&gt;Concat with parquet&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;CPU Time - 0:01:37.984875
Peak Memory Usage - 1.6829 GB
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Looking at the results, parquet seems like a really good option to me. But we will run for a few more examples and later check which one suits the best.&lt;/p&gt;
&lt;h3 id="the-next-two-weeks"&gt;The next two weeks&lt;/h3&gt;
&lt;p&gt;The project is making progress in all fronts. I feel I need to reorganize my thoughts a bit. My main work for now would be to complete the task list of &lt;a href="https://github.com/radis/radis/pull/287"&gt;this pr&lt;/a&gt;. And then look at other stuff.&lt;/p&gt;
&lt;div class="footnotes"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="https://www.sciencedirect.com/science/article/pii/S0022407398000788?via%3Dihub"&gt;Rothmann Paper (Eqs.(A7), (A8), (A9)&lt;/a&gt; &lt;a class="footnote-backref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fnref:1"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210620_0300_gagan-aryan/</guid><pubDate>Sun, 20 Jun 2021 02:00:06 GMT</pubDate></item><item><title>Chapter 0: Prologue</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210606_2240_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;div&gt;&lt;p&gt;Hi There and Namaste! This is going to be the second blog and first blog related to GSoC where I will be sharing my experience Community Bonding Period Experience with &lt;b&gt;Radis&lt;/b&gt;. Before moving ahead lets learn about GSoC and my perspective about it.&lt;/p&gt;
&lt;h3&gt;Google Summer of Code&lt;/h3&gt;
&lt;p&gt;GSoC or the way I like to say it &lt;strong&gt;(Great Summer Opportunity to Code ;)&lt;/strong&gt; is a program conducted and funded by Google to promote college students around the world to engage with Open Source Community and contribute to the organization for a tenure of 3 months. In the process, code is created and released for the world to see and use. But the main aim of GSoC is to promote students to stick to the organizations and help to grow the Open Source Community. This is a great initiative by Google that brings thousands of students every year and help them get an opportunity to peek into the world of open source development, learn new skills and also get compensated for the work, quite generously.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;I remember during second year of my college, it was around end of March and my roommate was applying for GSoC and I was like what is this program? There I got to know about it but since the deadline was near I was afraid of doing all the stuffs in a week of time, so I didn’t apply for it. Fast forwarding to next year, I was prepared enough this time and I feel priviledged to be a part of GSoC as part of OpenAstronomy. &lt;/p&gt;
&lt;h3&gt;My GSoC Project&lt;/h3&gt;
&lt;p&gt;I’m part of &lt;b&gt;&lt;a href="https://github.com/radis/radis"&gt;Radis&lt;/a&gt;&lt;/b&gt; organization which is a sub-org of &lt;a href="https://github.com/OpenAstronomy"&gt;OpenAstronomy&lt;/a&gt;. Radis is a fast line-by-line code used to synthesize high-resolution infrared molecular
spectra and a post-processing library to analyze spectral lines. It can synthesize absorption
and emission spectrum for multiple molecular species at both equilibrium and
non-equilibrium conditions.&lt;br&gt;
Radis computes every spectral line (absorption/emission) from the molecule considering
the effect of parameters like Temperature, Pressure. Due to these parameters, we don’t get
a discrete line but rather a shape with a width. This is called line broadening and for any spectral synthesis code, this is the bottleneck step. Ok let us C what my GSoC project is all about! &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Radis has 2 methods to calculate the lineshape of lines.&lt;br&gt;
● Legacy Method&lt;br&gt;
● DLM Method&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The goal of this project is to derive an equation comprising all parameters that affect the
performance for calculating Voigt broadening by running several benchmarks for different
parameters involved in the calculation of lineshapes to check their significance in
computation time. Then we need to find the critical value for the derived equation (&lt;code class="language-text"&gt;Rc&lt;/code&gt;)
which will tell us which optimization technique to select based on the computed &lt;code class="language-text"&gt;R&lt;/code&gt; value in
&lt;b&gt;calc_spectrum()&lt;/b&gt;. An &lt;code class="language-text"&gt;optimization = "auto"&lt;/code&gt; will be added that will choose the best method based on the parameters provided.&lt;/p&gt;
&lt;h3&gt;Community Bonding Period&lt;/h3&gt;
&lt;p&gt;The first phase of GSoC is the &lt;b&gt;Community Bonding Period&lt;/b&gt; which is a 3 weeks long period. Its main aim is allow the student to get familiar with the community and the codebase. It serves as a warm-up period before the coding period. The first thing I did was that I went though the original Radis &lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S0022407318305867?via%3Dihub"&gt;paper&lt;/a&gt; and also the DLM implementation &lt;a href="https://ui.adsabs.harvard.edu/abs/2021JQSRT.26107476V/abstract"&gt;paper&lt;/a&gt; because our project objective is based on these 2 implementations. It helped me understand the main purpose of RADIS, its architecture and the science behind different steps of both equilibrium and non-equilibrium spectrum, though I have to accept these papers are way too technical for me :p (Complex Spectroscopy related formulas).&lt;br&gt; I believed inorder to get myself ready for the coding period, I shall focus on solving some related issues to make me more familiar with the codebase.&lt;br&gt;&lt;/p&gt;
&lt;p&gt;In order to compute any spectrum we need to determine several parameters like: minimum-maximum wavenumber, molecule, Temperature of gas, mole fraction, wstep, etc.&lt;br&gt;
&lt;code class="language-text"&gt;wstep&lt;/code&gt; determines the wavenumber grid’s resolution. Smaller the value, higher the resolution and vice-versa. By default radis uses &lt;code class="language-text"&gt;wstep=0.01&lt;/code&gt;. You can manually set the wstep value in &lt;b&gt;calc_spectrum()&lt;/b&gt; and &lt;strong&gt;SpectrumFactory&lt;/strong&gt;. To get more accurate result you can further reduce the value, and to increase the performance you can increase the value.&lt;/p&gt;
&lt;p&gt;Based on wstep, it will determine the number of gridpoints per linewidth. To make sure that there are enough gridpoints, Radis will raise an &lt;strong&gt;Accuracy Warning&lt;/strong&gt;. If number of gridpoints are less than &lt;code class="language-text"&gt;GRIDPOINTS_PER_LINEWIDTH_WARN_THRESHOLD&lt;/code&gt; and raises an &lt;strong&gt;Accuracy Error&lt;/strong&gt; if number of gridpoints are less than &lt;code class="language-text"&gt;GRIDPOINTS_PER_LINEWIDTH_ERROR_THRESHOLD&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So inorder to select the optimum value of &lt;code class="language-text"&gt;wstep&lt;/code&gt; I had to refactor the codebase such that we could compute the minimum FWHM (&lt;code class="language-text"&gt;min_width&lt;/code&gt;) value after calculating the HWHM of all lines and and set &lt;code class="language-text"&gt;wstep = min_width / GRIDPOINTS_PER_LINEWIDTH_WARN_THRESHOLD&lt;/code&gt;. All &lt;code class="language-text"&gt;wstep&lt;/code&gt; dependent parameters had to be refactored to make sure they are not being called before the calculating &lt;code class="language-text"&gt;min_width&lt;/code&gt;. At the end this feature was successfully merged in the develop branch of Radis and now users can use &lt;code class="language-text"&gt;wstep = "auto"&lt;/code&gt; to automatically get the optimal value of &lt;code class="language-text"&gt;wstep&lt;/code&gt;. This feature will be available from version &lt;b&gt;0.9.30&lt;/b&gt;. Here is the &lt;a href="https://github.com/radis/radis/pull/271"&gt;link&lt;/a&gt; of the merged PR.&lt;/p&gt;
&lt;p&gt;In short, the Community Bonding Period has been great and I have learned alot about Radis during this time. In the next 2 weeks I will be focussing on building a benchmarking framework and run various benchmarks for both Legacy and DLM method and determine the most influential paramters for performance.&lt;/p&gt;
&lt;p&gt;I’m very excited for the upcoming months. I know that this summer is going to be a life long experience and I’m really looking forward to do amazing things for the community and want to thank Google, OpenAstronomy, Radis and my mentors &lt;a href="https://github.com/erwanp"&gt;Erwan Pannier&lt;/a&gt;, &lt;a href="https://github.com/dcmvdbekerom"&gt;Dirk van den Bekerom&lt;/a&gt; and &lt;a href="https://github.com/pkj-m"&gt;Pankaj Mishra&lt;/a&gt; for this opportunity.
I’m ready for this amazing adventure.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;b&gt;LETS DO THIS&lt;/b&gt;&lt;br&gt;
&lt;img src="https://anandkumar-blog.netlify.app/2b4e6a4b663f4bc49d559484b8dd37b1/Start.gif"&gt;&lt;br&gt;
ps: Am a huge Spiderman Fan :p
&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210606_2240_anandxkumar/</guid><pubDate>Sun, 06 Jun 2021 21:40:32 GMT</pubDate></item><item><title>GSoC - 0</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210606_0300_gagan-aryan/</link><dc:creator>Gagan Aryan</dc:creator><description>&lt;div&gt;&lt;p&gt;I will be documenting my journey in the GSoC program under Radis (OpenAstronomy). This blog is the first in the series of those blogs and will contain a quick overview of what Google Summer of Code is, an intro to the organization I will be working with and the project I will be involved in, and what I did in the 20-day community bonding period.&lt;/p&gt;
&lt;h3 id="what-is-gsoc"&gt;What is GSoC?&lt;/h3&gt;
&lt;p&gt;I remember attending one of Programming Club IIT Kanpur’s lectures in my freshman year of college, and my senior just asked the students if they knew what GSoC was. I had no idea. But I glanced over to see if my peers knew something and saw a few of them nodding enthusiastically and a few others muttering among themselves. The senior didn’t explain what GSoC was, but he did ask us to check it out ourselves. I did. I wouldn’t save I understood the entire program back then since I didn’t even know what open source was.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;Fast forward around 9-10 months, I started contributing to open source. I really felt it helped me skill up as a developer, which motivated me to participate in GSoC.&lt;/p&gt;
&lt;p&gt;Google Summer of Code or GSoC is a program sponsored by Google that aims to connect university students worldwide with open source organizations to promote the open-source culture. Students work with an open-source organization on a 10-week programming project during their break from school and get an opportunity to contribute to high-quality code, learn new skills, and also get compensated for the work. In turn, the organizations benefit from a few extra pairs of helping hands. Any college student interested in software development should definitely check out this program.&lt;/p&gt;
&lt;h3 id="radis-and-my-project"&gt;Radis and my project&lt;/h3&gt;
&lt;p&gt;Radis&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; is a fast line-by-line code for synthesizing and fitting infrared absorption and emmision spectra such as encountered in laboratory plasmas or exoplanet atmospheres.&lt;/p&gt;
&lt;p&gt;Radis aims to provide a wide array of features and remain user-friendly at the same so. It currently supports spectral calculations on databases like HTIRAN and high-temperature databases like HITEMP, CDSD-4000 with a future plan on extending the support to ExoMol. It comes with just a one-line install and post-processing tools for analysis of the spectra. Users can also combine ranges to create a mixture of gases or calculate radiative transfer along the line-of-sight.&lt;/p&gt;
&lt;p&gt;RADIS uses Pandas dataframe for handling all the databases currently. Quoting the words of Wes (the core dev of Pandas), “pandas rule of thumb: have 5 to 10 times as much RAM as the size of the dataset” &lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;. Which makes it impossible to read, say, a database of size 5GB on a machine with a RAM of 16GB.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Pandas Meme" src="https://gagan-aryan.netlify.app/images/gsoc-0/pandas_meme.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;The goal of this project would be first to reduce the memory usage of the current calculations. Then, we replace pandas with libraries that are better suited for handling larger-than-memory databases, which would make it possible to compute spectral databases of up to billions of lines (of the scale of hundreds of GB or terabytes). I will say the core technical details of the project for the upcoming blogs.&lt;/p&gt;
&lt;h3 id="community-bonding-period"&gt;Community Bonding Period&lt;/h3&gt;
&lt;p&gt;The Community Bonding Period is an almost 20-days long period meant to serve as a warm-up or a buffer before the actual coding period begins. It can be used for a wide variety of purposes, such as getting a better understanding of the codebase and figuring out its intricacies. I started out by quickly going over Spectro-102&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt; again since I had left out a few parts the last time I did. I then studied the RADIS &lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; paper. Though I cannot really say the entire document, I did get a top-level idea of how it works and how it is different from other software.&lt;/p&gt;
&lt;h4 id="my-failed-attempts-in-wrapping-up-the-previous-work"&gt;My failed attempts in wrapping up the previous work&lt;/h4&gt;
&lt;p&gt;After my GSoC application, I started working on a feature request that asked a specific function in the code to return the wavelength and the intensity grid in sorted ascending order. I just assumed that all I need to do was sort the grids, and I did this and created a PR. I later learned that Radis, like any good codebase, has many tests that make sure things don’t break when a new change is made. Apparently, returning the wavelengths and intensity grid in the sorted order broke the physics when combining spectra.&lt;/p&gt;
&lt;p&gt;Before this PR, I was unaware of &lt;code&gt;pytests&lt;/code&gt;. I went through the documentation&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt;, ran the tests on my machine, and checked out each of the failing tests. This helped me understand different parts of the code, especially the &lt;code&gt;spectrum&lt;/code&gt; and &lt;code&gt;los&lt;/code&gt; modules of the repository. The tests passed of the &lt;code&gt;spectrum&lt;/code&gt; module passed after a few modifications. But, after I updated Erwan regarding my progress, I realized that I need to now design new tests since we cannot pinpoint where we are having problems in the codebase with the existing ones. Besides, I learned about the different types of tests (non-regression, validation, and verification) that exist in RADIS to ensure things don’t break after a brief chat with Erwan.&lt;/p&gt;
&lt;p&gt;We have decided how we will tackle this issue, but since I am required to start on my project from tomorrow, I will be getting back to this PR later and hope to find time for the same during the coding period.&lt;/p&gt;
&lt;h4 id="discovery-during--hitemp-co2h2o-download-automation"&gt;Discovery during HITEMP (CO2/H2O) download automation&lt;/h4&gt;
&lt;p&gt;In the first phase of my project, I am required to use a few hacks&lt;sup id="fnref:5"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:5"&gt;5&lt;/a&gt;&lt;/sup&gt; in the pandas and boost their memory performance. This includes dropping a few columns and changing the datatypes of a few others. Coincidentally Dirk encountered an issue while working on automating the download of CO2/H2O for HITEMP. So, CO2/H2O spectral databases contain multiple zip files, and automatic download of this was not supported in RADIS. Due to NaN values and the &lt;code&gt;np.uint&lt;/code&gt; not supporting them, the datatypes of a few columns conflicted when databases were added on top of one another. Currently, this is being handled by returning the parameters in the form of a memory inefficient &lt;code&gt;np.float64&lt;/code&gt;. I will have to bring them down to more suitable datatypes (&lt;code&gt;np.uint&lt;/code&gt;) most probably. This will probably be the first thing I will do as part of the project.&lt;/p&gt;
&lt;h3 id="the-next-two-weeks"&gt;The next two weeks&lt;/h3&gt;
&lt;p&gt;In the next two weeks I will be involved in figuring out and implement all the database pre-processing that can be done to boost pandas’ performance&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt;. I will also setup memory performance benchmarks to track these changes. I am super excited to see how this project goes. I would like to thank Google, OpenAstronomy, RADIS and my mentors &lt;a href="https://github.com/erwanp"&gt;Erwan Pannier&lt;/a&gt;, &lt;a href="https://github.com/dcmvdbekerom"&gt;Dirk van den Bekerom&lt;/a&gt; and &lt;a href="https://github.com/pkj-m"&gt;Pankaj Mishra&lt;/a&gt;. I hope to learn a lot of stuff along the way and hopefully I will deliver. So,&lt;/p&gt;
&lt;p&gt;&lt;img alt="Let the Games Begin" src="https://media1.giphy.com/media/xT0xevozBTg7ChpL44/source.gif"&gt;&lt;/p&gt;
&lt;div class="footnotes"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="https://gagan-aryan.netlify.app/tags/gsoc21/doi.org/10.1016/j.jqsrt.2018.09.027"&gt;RADIS: A nonequilibrium line-by-line radiative code for CO2 and HITRAN-like database species, E. Pannier &amp;amp; C. O. Laux&lt;/a&gt; &lt;a class="footnote-backref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fnref:1"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="https://wesmckinney.com/blog/apache-arrow-pandas-internals"&gt;Apache Arrow and the “10 Things I Hate About pandas”, Wes Mckinney&lt;/a&gt; &lt;a class="footnote-backref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fnref:2"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;&lt;a href="https://mybinder.org/v2/gh/radis/spectro101/HEAD?filepath=102_lab_spectroscopy.ipynb"&gt;Spectro102&lt;/a&gt; &lt;a class="footnote-backref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fnref:3"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;&lt;a href="https://radis.github.io/"&gt;Radis Documentation&lt;/a&gt; &lt;a class="footnote-backref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fnref:4"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:5"&gt;
&lt;p&gt;&lt;a href="https://pythonspeed.com/articles/pandas-load-less-data/"&gt;Pythonspeed article - Pandas Load less data&lt;/a&gt; &lt;a class="footnote-backref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fnref:5"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210606_0300_gagan-aryan/</guid><pubDate>Sun, 06 Jun 2021 02:00:06 GMT</pubDate></item><item><title>print(" Hello World!!! ")</title><link>http://openastronomy.org/Universe_OA/posts/2021/05/20210518_2240_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;div&gt;&lt;p&gt;Hey everyone &lt;b&gt;Anand Kumar&lt;/b&gt; this side. This is going to be a series of blogs where I will cover my Summer Journey with &lt;b&gt;Radis&lt;/b&gt; organization as a part of Google Summer of Code. Welcome to my first blog where I will be introducing myself coz that is kind of necessary :p. I’m a Junior from National
Institute of Technology, Hamirpur, India currently pursuing my BTech in Computer Science and Engineering.&lt;br&gt;&lt;/p&gt;
&lt;p&gt;I am a geek. I love life, computers and everything in between!&lt;br&gt;
&lt;!-- TEASER_END --&gt;
I have been coding since my school days and soon realized that man this thing is so cool!
I am an A.I. enthusiast and have made various projects related to Data Analysis, Machine Learning, Deep Learning
and Web Development. Also I have completed a Data Analytics Internship at Pikkal &amp;amp; Co, Singapore and a
Deep Learning Internship at Mavoix Solutions Pvt Ltd, Bangalore.&lt;br&gt;
Currently I’m a Student Developer at OpenAstronomy organization as a part of Google Summer of Code 2021. &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Wanna know a less known fact, I’m a huge hardcore video gamer! If I’m not coding, I will probably be killing some time
on my laptop or my Playstation console ;)&lt;/p&gt;
&lt;p&gt;Wanna know more about me and my work? Below are some links, do check out;)&lt;br&gt;
&lt;a href="https://www.linkedin.com/in/anand-kumar-83896717a/"&gt;LinkedIn&lt;/a&gt; | &lt;a href="https://github.com/anandxkumar"&gt;github&lt;/a&gt;
| &lt;a href="https://anandkumar.netlify.app/"&gt;Portfolio&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Also one huge shout out to the guys at &lt;b&gt;GatsbyJS&lt;/b&gt; for providing such an amazing blogging template(keep it simple and clean, they say!).
The biggest advantage of this template is that every blog is written in &lt;b&gt;Markdown&lt;/b&gt;. So its gives alot of flexibility and functionality
to the user to edit their texts. Plus their templates codebase is easy to understand so anyone can just clone and get started!&lt;/p&gt;
&lt;p&gt;Anyways I guess this should wrap up this blog. See you in the next one where I will be starting my GSoC journey and discuss my project ;)&lt;br&gt;
Till then take care and ba-bye :)&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/05/20210518_2240_anandxkumar/</guid><pubDate>Tue, 18 May 2021 21:40:32 GMT</pubDate></item><item><title>Google Summer of Code - The End!</title><link>http://openastronomy.org/Universe_OA/posts/2020/08/20200827_0614_pkj-m/</link><dc:creator>pkj-m</dc:creator><description>&lt;div&gt;&lt;p&gt;Hello everyone! It has been a while since my last blog, and for a good reason. The past few weeks have been quite productive, and I thought it might be a good idea to present one final report of the work that I did over this past month instead of breaking it into subparts. With this blog, I will also be marking the end of my journey through the Google Summer of Code program. This blog will talk about some of the changes that the work I did as a part of GSoC brought to RADIS, and how you, the user can and will benefit from it.&lt;/p&gt;

&lt;p&gt;In my last blog, I briefly mentioned what I was planning to do with the GPU code and how to integrate it with RADIS. The current RADIS code performs calculation of spectra at thermal equilibrium (and even in non-equilibrium conditions) in primarily two ways:&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;ol&gt;
&lt;li&gt;by defining a &lt;code class="language-plaintext highlighter-rouge"&gt;SpectrumFactory&lt;/code&gt; object, and then calling the method &lt;code class="language-plaintext highlighter-rouge"&gt;sf.eq_spectrum(Tgas=T)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;by passing the necessary parameters in the method &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt;, which returns a Spectrum object directly&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In our attempt to add support for GPU accelerated spectrum calculation, we wanted to keep the interface as similar to the original one as possible. Thus, the new method which we introduced to calculate the spectrum using GPU was naturally called &lt;code class="language-plaintext highlighter-rouge"&gt;eq_spectrum_gpu&lt;/code&gt;. The &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt; method, which is actually a wrapper that makes of the &lt;code class="language-plaintext highlighter-rouge"&gt;eq_spectrum&lt;/code&gt; method underneath, was also modified and a new parameter called &lt;code class="language-plaintext highlighter-rouge"&gt;mode&lt;/code&gt; was added. Depending on what the value of &lt;code class="language-plaintext highlighter-rouge"&gt;mode&lt;/code&gt; is, the calculation of spectrum could be performed either on the GPU or on the CPU.&lt;/p&gt;

&lt;p&gt;Now coming to the implementation of &lt;code class="language-plaintext highlighter-rouge"&gt;eq_spectrum_gpu&lt;/code&gt;, I tried to keep the structure of the code as similar to the current CPU implementation as possible. What it meant was that the preprocessing done was in a way quite similar to the CPU version of the method. The difference actually came in during the broadening step. Initially our implementation was different from the CPU version when it came to loading the data, primarily because the data being loaded in the GPU method was in the ‘npy’ format. This made it necessary to implement another method for loading this data, as the data loader in RADIS did not support npy files. While implementing this was not difficult, it was not seen as a very good design decision, as this type of loading and handling of data was very isolated and not compatible with the rest of RADIS’ features. Therefore, ultimately it was to keep this mpy2df method as an additional, helper method, and instead of using it as the primary source of data, we use the dataframe which RADIS already generates instead. This allowed us to keep things compatible with the current implementation to a great extent, and the only downside, if it can be considered that, was the need to now compute the parameters before the spectrum is calculated, which in case of npy files were already present for us. This however, was virtually a non-problem since this step was not even remotely close to the bottleneck, and the flexiblity it provided in terms of loading and preprocessing data outweighed this extra computation easily. At this point we had the data loaded in memory, either through the legacy data loader using the dataframe, or by passing the location of the npy files in the system and loading them directly. After this, we had to pass this data to the GPU module. The GPU module, titled py_cuFFS, is actually a Cython file with some CuPy, which serves as the complete host+device code for the computation of the spectra. Using Cython over Python allows us to compile the module prior to using it, which gives an added performance boost. The compilation however, is a machine-specific process and cannot have a single-file-handles-all kind of implementation. Thus, instead of sharing the binary file with the users, we instead share the source code. Whenever the user calls the GPU accelerated methods on their system for the first time, RADIS automatically compiles the source code into the binary, which then gets compiled according to the system environment of the user. Now, the compiled binary is imported by RADIS, and the input parameters such as the temperature, pressure and the partition function are passed on to the GPU.&lt;/p&gt;

&lt;p&gt;The GPU module returns the spectrum to RADIS, which then computes other quantities, such as the absorbance and transmittance from this. From this stage onwards the code for &lt;code class="language-plaintext highlighter-rouge"&gt;eq_spectrum&lt;/code&gt; and &lt;code class="language-plaintext highlighter-rouge"&gt;eq_spectrum_gpu&lt;/code&gt; is identical. Both the methods update the metainformation such as the calculation time, number of lines calculated, etc. In order to read more about the GPU module and how to use it, I highly recommend the users to go through the &lt;a href="https://radis.readthedocs.io/en/latest/lbl/gpu.html"&gt;documentation&lt;/a&gt; which has not just examples but also a guide on how to setup your system in order to make use of these GPU accelerated methods. If you’d just like to observe for now, I would recommend going through this &lt;a href="https://github.com/radis/radis-benchmark/blob/master/TEST1.ipynb"&gt;notebook&lt;/a&gt; on radis-benchmark which gives an example of how to use these methods to calculate the spectrum on the GPU, and an impressive speed test between the GPU and the CPU methods when calculating spectra with 5M lines.&lt;/p&gt;

&lt;p&gt;This wraps up my journey with RADIS a Google Summer of Code participant. It has been an excellent experience with a great deal of learning involved. I had prior experience of working with CUDA for deep learning pipelines but this was a completely new domain. In addition to the programming itself, I got exposed to the world of spectroscopy which was also very interesting. While my contribution to RADIS under the GSoC aegis comes to an end with this blog, I am still really excited to be a part of RADIS as it grows further. My GSoC project started what would hopefully end with a completely GPU-accelerated RADIS, but there is still plenty of work before we can say that. My GSoC project implemented the thermal equilibrium variant of the spectrum calculation method, but we still need to work on non-equilibrium methods. In addition, we also need to modify the GPU code itself to allow support for weighted air- and self-collision factors, among other things. There’s a lot to do, but I think we’ve had a good start. Most importantly, I am happy with the way I am ending this project. The code is ready and we have proper guide, documentation and examples in place so any new user can easily try this feature out themselves! I am really excited about the feedback and what users have to say about this GPU implementation. Finally, none of this would have been possible without the constant support and assistance from my amazing mentors, Erwan ( &lt;a href="https://github.com/erwanp"&gt;@erwanp&lt;/a&gt; ) and Dirk ( &lt;a href="https://github.com/dcmvdbekerom/"&gt;@dcmvdbekerom&lt;/a&gt; ). It would be hard to understate the contribution they’ve made to my project, helping constantly not just with the code but also in designing and planning the next steps. My lack of knowledge in the domain of spectroscopy was a huge pain at times which led to extremely long periods of slow progress due to painful debugging, but they always took out time from their packed schedules to help me out. Once again, thank you! It has been a wonderful experience, working with RADIS and I am really excited to see what how RADIS grows in the future!&lt;/p&gt;

&lt;p&gt;P.S. for anyone who’d like to go through the code of the project, you can find the pull request here: https://github.com/radis/radis/pull/117. And more importantly, if you’d like to know more, want to contribute, or just talk to the team, feel free to join us at our slack &lt;a href="https://radis.github.io/slack-invite/"&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2020/08/20200827_0614_pkj-m/</guid><pubDate>Thu, 27 Aug 2020 05:14:12 GMT</pubDate></item><item><title>Google Summer of Code - Blog #4!</title><link>http://openastronomy.org/Universe_OA/posts/2020/07/20200728_1823_pkj-m/</link><dc:creator>pkj-m</dc:creator><description>&lt;div&gt;&lt;p&gt;Hello! Welcome to the latest blog post in my Google Summer of Code series. With this blog, we’ll marking the end of the second phase of evaluations! Additionally, this blog is going to be a little different from all the previous ones as we finally start to work and discuss about Python and the actual RADIS code base.&lt;/p&gt;

&lt;p&gt;Honestly, I am actually really glad that we’re finally at this stage. The monotonity of Cython and working on the same, huge Cython+CuPy file with more than a thousand lines of code was getting very frustrating :P&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;p&gt;In this blog, I shall be discussing more about the way we will integrate the GPU code with RADIS. But before we do that, let us first understand how RADIS handles this part right now, performing computations purely on the CPU. In order to compute the spectra of a molecule, we make use of module defined in RADIS known as &lt;code class="language-plaintext highlighter-rouge"&gt;Line-by-Line&lt;/code&gt; Module or LBL for short. This module contains numerous methods which are used for calculations of spectras. One of the most standard methods for computing the spectras is known as &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt;, which takes in inputs such as the molecule, isotopes, temperature, pressure, waverange over which the spectra needs to be calculated (and many other depending on the user’s requirements) and returns the result in the form of &lt;code class="language-plaintext highlighter-rouge"&gt;Spectrum&lt;/code&gt; object which neatly packs the information in single object which can then be processed as needed. This is what the user sees when they use RADIS. However, for us to develop and contribute, we also need to understand what goes on under the hood when a method such as &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt; is called.&lt;/p&gt;

&lt;p&gt;The first thing that occurs when &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt; is called is the validation and conversion of physical quantities (such as temprature and pressure) into default units that are assumed in the rest of the code. This is followed by the instantiation of an object from the &lt;code class="language-plaintext highlighter-rouge"&gt;SpectrumFactory&lt;/code&gt; class, which will contain all the information about the spectrum to be computed. This is all standard, something that will happen each time &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt; gets called. However, from here on, things start to get interesting as we look into the databank. Let us first understand what the purpose of a databank is: in order to compute a spectra for a specfic molecule, we need information about it. This could include data like line positions, line intensities, pressure-broadening parameters, etc. We don’t have to understand what each of these quantities mean or represent (to be honest, I don’t either) but the main idea we need to internalize is that we can’t compute a spectra without any information. So while it might be fair to say that a spectra can literally be generated from thin air, metaphorically that does not hold true.
This data, used to compute the spectra, can often be huge and therefore we try to avoid moving it around as much as possible. RADIS has implemented cache features in order to minimize the computations to be done on the raw databank. The important idea we need to focus on, is that once a databank has been loaded, we don’t have to load it once more if we ever call &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt; again for the same data (with maybe different waverange, etc.) This is done using extra memory: the original data that is loaded in the memory is saved as a dataframe, and every call made to &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt; first creates a copy of this data and works on it instead. This preserves the original dataset and allows us to use it again in case it is required, as the cost of the extra memory needed to save the copy. This was also the first problem I had to tackle: the GPU code we were going to use made use of &lt;code class="language-plaintext highlighter-rouge"&gt;npy&lt;/code&gt; formatted arrays to load the data, whereas RADIS was designed to load &lt;code class="language-plaintext highlighter-rouge"&gt;h5&lt;/code&gt; or &lt;code class="language-plaintext highlighter-rouge"&gt;par&lt;/code&gt; files. This meant I had to modify the databank loader RADIS used to support npy files. This was not a very significant problem however, as we could simply load the data separately instead of using the databank. The only issue with this approach would have been the repeated loading of data everytime the function call was made, but since we were using a small dataset, it wasn’t a significant issue and we decided to push it back and first focus on getting the GPU code to work with independent data being loaded.&lt;/p&gt;

&lt;p&gt;Once we had the data, the next step was to perform the computations. Now I won’t go into the details in this part since the mathematics behind these computations was something which: 1. I didn’t understand completely, and 2. I didn’t need to understand completely. Given my project already had a proof-of-concept code with the mathematics inplace, and my job was to make that code compatible with RADIS, I didn’t need to understand the nuances of it. I did go through the paper draft to understand the general idea behind the parallel approach we used, but I didn’t even try with the serial code since I had absolutely nothing to do with it. This mathematics was actually abstracted and encapsulated in another method &lt;code class="language-plaintext highlighter-rouge"&gt;eq_spectrum&lt;/code&gt;. Once this method finished execution, we would have the spectrum ready with all the information ready for the user. Within this method, there was one specific method that was of particular interest to us: &lt;code class="language-plaintext highlighter-rouge"&gt;_calc_broadening&lt;/code&gt;, responsible for the broadening step in the spectra calculation pipeline. This method was the bottleneck in the entire process, and the GPU code was primarily helping us speed up this particular portion of the process. My job here was to create another analogous method &lt;code class="language-plaintext highlighter-rouge"&gt;eq_spectrum_gpu&lt;/code&gt; which would do the exact same job as &lt;code class="language-plaintext highlighter-rouge"&gt;eq_spectrum&lt;/code&gt;, except instead of calling the sequence of serial methods to compute the spectra, I’d simply call the GPU code which was ready for us, and obtain the result. Some further processing of this result (which is computationally very inexpensive compared the rest of process), we have our spectra ready. It is a lot simpler and significantly easier than the CPU equivalent since we off-load the entire process to the Cython-compiled binaries and don’t have to worry about anything else. One problem I had with this (which is still not resolved as of writing this) is how do we fill in the missing information gaps in the Spectrum object, which in case of the CPU code were filled sequentially as the pipeline proceeded. Since our GPU code would only return the final result, I am not sure if we would be able to recover the preceding information bits which we didn’t obtain. Hopefully I’ll have the answer to it tomororrow when Erwan replies. That would pretty much finish my job here.&lt;/p&gt;

&lt;p&gt;However, another important bit of information that we’re skimming over is how we call the compiled binary file. This part is in fact what I will be working on over the coming week. The idea is that although right now I am using a simple binary file located in the same file to run the code, it is possible (and highly likely) that the same binary file will not work properly (or at all) in another system it is not meant for. As a consequence, the entire method might crash. In order to resolve this, we will be looking into placing the source code instead of the binary file in the RADIS codebase, and everytime a user needs to use the GPU version of &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt;, the code will first, automatically, compile the Cython file on their system, obtain the binary file meant for their system, and use that instead. The details regarding the implementation of this bit are still not clear as it is something I will be working on next week!
The next blog will cover the implementation details about this runtime-compilation setup along with a discussion on what is next! Thank you!&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2020/07/20200728_1823_pkj-m/</guid><pubDate>Tue, 28 Jul 2020 17:23:12 GMT</pubDate></item><item><title>Google Summer of Code - Blog #3!</title><link>http://openastronomy.org/Universe_OA/posts/2020/07/20200714_0804_pkj-m/</link><dc:creator>pkj-m</dc:creator><description>&lt;div&gt;&lt;p&gt;Hey! Continuing from where we left off last time, in this blog I’ll be talking about the work I did in the period between 1st - 15th July. As I had mentioned in the previous blog, my primary task for this period was to get the Cython/CuPy version of our proof-of-work code to produce the correct output. I had initially expected the work to be relatively easy, as the output was already there; I simply had to check the logic part of the program to identify why it wasn’t correct. However, the work actually turned out to be a lot more painful that I’d have imagined. I had briefly touched upon this fact in the last blog as well, but the first thing I had to do in order to finish this task was to ensure that the debugging tools and methods were in place. This, fortunately, was one of the easier parts of the problem. The problem was two fold, and thus required 2 different solutions as well.
First, the loading of the dataset on to the RAM was a challenging task for my computer. I am not sure exactly why that was the case, but it certainly was nowhere as performant as the C++ code which did the same task of loading the exact same data in a relative breeze. I looked around the internet to understand what could possibly be slowing down the loading step so much, which was quite simply a single line : &lt;code class="language-plaintext highlighter-rouge"&gt;v0 = np.load(dir+path+'v0.npy')&lt;/code&gt;. The file itself was around 400MB, and there were a total of 8 of them, pushing the total memory required to around 3 GB. The solution to this problem was relatively straight forward, and felt almost as if it was hiding in plain sight when I did find it. The core idea is that when we try and load a numpy array the way I was doing without having declared the variable previously as a c-type, Cython quite naturally assumes it to be a pure Python variable and therefore fails to deliver the performance boost it promises on compilation. This however, was a trivial issue to resolve. All I had to leverage the advantage promised by Cython was to declare the numpy arrays prior to loading them with the datasets. This was done with a simple line &lt;code class="language-plaintext highlighter-rouge"&gt;cdef np.ndarray[dtype=np.float32_t, ndim=1] v0 = np.zeros(N_points, dtype=np.float32)&lt;/code&gt;. That was it! With just this single addition, the array was now a c-type variable and thus was processed significantly faster than the older pure numpy arrays. Unfortunately I didn’t benchmark the difference as I still have some things which I am not super confident about related to this part. I am not sure if it’s due to an observation bias or some other external factor, but I felt that the speed of loading the data itself varied quite significantly even with the same binaries. I am not sure if this is due to some caching/optimizations being done under the hood by the compiler itself, but whatever it is, certainly would make aimless benchmarking without controlling these external factors a futile exercise.
The second issue which I faced which was making debugging difficult was the inability of my GPU to automatically kill the Python/CuPy tasks once the program finished execution. I searched around stackoverflow and found that it is actually a rather common issue with CuPy. As a result, it also didn’t take a long time before I found a makeshift solution for this problem as well. All I had to do once the program had finished execution was to call some specific CuPy methods to free the memory, and it worked just fine! With these two issues sorted, I had a much better setup in place to try and debug the code without being forced to restart the computer or wait 10 minutes for the RAM to clear up everytime the program finished execution!&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;p&gt;Now moving on to the actual issues in the code! It’d be an understatement to say the code wasn’t riddled with a number of bugs. As some of you might remember, I explained in the previous blog how we had decided to not worry about trying to fix variables into structs before passing and instead adopted a slightly different approach of using each attribute as an independent variable. While this wasn’t such a bad idea, and infact allowed me to proceed quite quickly, it was annoying with respect to how much had to be typed just to pass a single variable. Everytime you need to call the variable, you’d have to ensure the variable had been specified with the global identifier before being used. Naturally, all this led to a huge scope of facing some issues and I certainly did. After  writing the kernels and compiling the code, I tried to run the program for the first time. The initial few errors I faced were trivial bugs that were solved almost instantly. But the first major problem that I ran into was an IllegalMemoryAccess error that was thrown by CuPy/CUDA. As the name suggests, this was happening because at some place in my kernel, I was trying to access an address in the memory that just hadn’t been declared. The problem in debugging this issue was that I had no idea which memory load operation was throwing the error, neither the variable nor the line number. This proved very difficult to debug because of this reason, along with the fact that I simply couldn’t get the kernels to print anything on the terminal using CuPy. I was probably a few days into this problem before I approached my mentors to discuss about it. While my mentors were very helpful and went through the code and provided feedback on a lot of other aspects of the code which had room for improvement, none of their suggestions were aimed directly towards the problem itself. We had a few places we knew we could try to inspect and check if they’re the source, but it’d have been a difficult process. Fortunately, though, something really unexpected happened at this time. Dirk found out a way to get the structs to work in CuPy, as well as the syntax to save them in GPU’s constant memory. While this didn’t directly affect the kernel code itself, it did make the code a lot neater and easier to inspect. However, the biggest advantage wasn’t that! For some weird reason which I am still not sure of, the modification in the code to store variables under structs and saving the two constant structs in the constant memory somehow solved the illegal memory access!! I was so stunned the first time it happened. I had been banging my head against the wall for over a week because of that issue, and somehow making another adjustment in the code which I thought was completely unrelated to the issue resolved it instantly. The feeling was so satisfying it’s hard to put it in words!&lt;/p&gt;

&lt;p&gt;However, the battle was still not over. Although the code went on to execute till the end after this error was resolved, it still failed to produce the correct output. I was getting an output, an array which ideally should have been the spectrum produced according to the conditions specified and the data passed, but it just didn’t match with the output of the C++ program. More than just a mismatch in the values, it felt particularly bad since the values I was getting as the output were a mixed bunch of both positive and negative numbers, which especially didnt make any sense as the output was supposed to represent the intensity, so a negative value had no real physical meaning. Once again I was stuck in another long arduous cycle of trying to identify what’s going wrong in those 1000 lines of code. However, this time I was determined to solve it myself and thus started the debugging process in the most rudimentary manner possible. I decided to follow the execution of the program, one step at a time and print the values obtained till that step. Sounds pretty simple, doesn’t it? Except the problem was this: the valus that were being processed and passed on to the next methods in this sequence of steps weren’t separate primitive values like floats or ints, but instead arrays with more than 50,000 floats or at times, even millions of them. Thus while the idea was pretty simple, the execution was just as exhausting. I had to print the values not just for the Cython code I had written, but also for the C++ program at each step, since I didn’t have anything else to refer too. This was just as cumbersome as it sounds. After printing the thousands upon thousands of values in text files and saving them, I was also left with the task of checking them to see if they match or not. Again, since these were too many rows to compare directly, I had to write scripts in Python to do just that: load arrays with hundred thousand elements from text files, and compare them. While the process wasn’t difficult in itself, it was what people might call monotonous and extremely draining since each step took time and almost ironically, couldn’t be carried out in parallel as I just didn’t know at what stage the error creeped in. Thus, after hours and hours of executing the program, saving the file and comparing the values, I was able to narrow down my search to a single method in the huge file. Infact, I knew exactly where the error was. It was the output array from the first kernel, which we called &lt;code class="language-plaintext highlighter-rouge"&gt;host_params_h.DLM_d_in&lt;/code&gt;. Now that I knew where the error was, I was sure that the error originates from somewhere in the first kernel as it failed to produce the correct output. Thus, I once again started this whole process of comparing the variable values one at a time until I could see where the difference lies. However, this time the search failed to bring any results as literally every single input variable for the kernel was identical to the values that were being passed to the C++ kernel. There was no chance of the kernels behaving differently in the two programs as the code was literally the same for both. And with that I hit a dead-end. Again lost with no ideas left to pursue, I approached my mentors and they offered a lot of suggestions as to what might be going wrong, and once again it was something which they suggested that resolved this issue. Turns out, the array that we were using in the program, which was a three-dimensional numpy array, was not being read properly by the kernel. So as all you might know, even though we often use multidimensional arrays in our programs, the memory in the computer is arranged in a single dimension. Therefore, multidimensional arrays bring with them the question of how to store them in the memory. This exact idea was the logic behind the painpoint in my code. While numpy arrays that we defined in the Cython portion of our code was for all purposes normal 3D arrays which could be indexed using the standard arr[i][j][k] notation, under the hood things were being done in a different manner than we thought. However, once we realized that this was the source of the error, it was quite easy to resolve by simply specifying, during the declaration of the array,  the type of ordering we want for our elements in the memory. In numpy, there are two options for order, ‘C’ and ‘F’. C refers to the style in which C stores multidimensional arrays in memory, in a ‘row-major’ fashion while F refers to Fortran, which instead adopts the ‘column-major’ option. Now that this was over, I was expecting, as you might be expecting, that everyone was happy and working exactly the way we were expecting it to. But this problem was still far from over! I had successfully managed to get the values of &lt;code class="language-plaintext highlighter-rouge"&gt;host_params_h.DLM_D_in&lt;/code&gt; to match, and yet the final output didn’t match with the expected value. At this point I was extremely frustrated since the mismatch in the values meant that the error was being generated somewhere in the 10 odd lines of code that were left after the first kernel’s execution. Out of those few lines, a couple were function calls to the fourier transform libraries inbuilt in CuPy, and I was certain that couldn’t possibly be the source of error. The only possibility left was the second kernel call. However, unlike the first kernel, the second kernel was fairly straightforward and it wasn’t clear what could possibly be going wrong in those 15 lines or so. This also went on for a few more days until one day my mentor, Dirk, suggested that since we are so close to the final working code, that we do a live-collaborative coding session to fix the final problems and be done with this. I couldn’t have asked for more! For me who had been struggling with these last few annoying bugs for so long, Dirk offering to help me out over not just over text but on a video conference was a silver lining. We quickly decided on a date and were ready to squash those bugs. While we made some progress in the first hour or so, eventually we also hit a dead end where neither of us could figure out what could be going wrong. And then, with a stroke of luck that some might consider divine intervention ( :P ) the strange thought of trying to compute the FFT on the CPU came to our mind. It was something that we thought of due to pure desperation to do something, to check something. At that point we’d done everything we could think of to see where the bug came from but it was to no avail. I had absolutely zero expectations from the change in FFT that we were trying to accomplish, for all we did was change the processor on which the FFT was being computed. Instead of computing it on the GPU using CuPy, we did it directly on the CPU using Numpy. And once again, as the program executed and produced the correct shape on the plot produced in the output, we were left speechless. We had absolutely no idea what had happened. The change that we’d made a few moments ago was definitely the last thing I could have thought of as responsible for the bug, but somehow, that was it. But apart from the surprise, we were also happy and relieved to have finally identified the source of the error. It was either the forward FFT or the inverse FFT. Another hit and trial experiment narrowed it down further and we were sure that at this point, it was the forward FFT which was the source of error. However, we still couldn’t comprehend why that might be the case. We did a lot of googling, a lot of playing around with &lt;code class="language-plaintext highlighter-rouge"&gt;np.fft.rfft&lt;/code&gt; but it didn’t help. Even though we knew the problem was somewhere in the way the data was being read (as we’d previously noted during the other indexing issue), we didn’t know why or how it was that way. Effort to print the order of the array didn’t work as the numpy class apparently does not have an attribute called &lt;code class="language-plaintext highlighter-rouge"&gt;order&lt;/code&gt;, and instead it is something which can only be passed during the declaration. After possibly sitting there for maybe another hour or so, Dirk came up with something absolutely amazing. In order to check the way the data was being mapped to the memory without having something like &lt;code class="language-plaintext highlighter-rouge"&gt;order&lt;/code&gt; to print, Dirk found out and suggested the use of &lt;code class="language-plaintext highlighter-rouge"&gt;flags&lt;/code&gt;. Flags was infact the attribute we were looking for, containing all the information about the way the data was being mapped to the memory. A quick look at the flag from the output of the FFT confirmed our suspicions. The returned array was F-continous, but not C-continous. What this meant was that the data was being stored in a column major format, while the kernel had been written for C data. It was an easy issue to resolve once the error itself was clear and within a minute, after changing the indexing in the kernel body, we were done! The code compiled without any hiccups, the binaries ran without any issues and out came within a minute the plot I had been waiting for! It was a great moment to be very honest, seeing the work I had been doing for the past month or so finally working the way we’d always expected it to. With that, I finally had a working Python-compatible version of our code to produce spectra using GPUs.&lt;/p&gt;

&lt;p&gt;Now that we’re done with this part of the project, the upcoming weeks will be spent on making this work with RADIS. While the code, the way it is right now, is capable of calculating the spectra by itself without the need or support of any external code except the modules we use, it might not remain in such an independent form after its integration with RADIS is complete. I am not very certain about the details of this part of the project yet, but I will be having a meeting with my mentor, Erwan soon to discuss it. In the next blog, I think you guys can expect some discussion on the integration part of this project along with the details of how RADIS will start supporting GPU compatible methods! With that, I would like to conclude this blog! Disqus is always open for feedback and ofcourse, thanks for your time!&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2020/07/20200714_0804_pkj-m/</guid><pubDate>Tue, 14 Jul 2020 07:04:56 GMT</pubDate></item><item><title>Google Summer of Code - Blog #2!</title><link>http://openastronomy.org/Universe_OA/posts/2020/06/20200629_2135_pkj-m/</link><dc:creator>pkj-m</dc:creator><description>&lt;div&gt;&lt;p&gt;Hello! This blog marks the end of the first phase of Google Summer of Code. The journey so far has been challenging but also extremely rewarding. The knowledge gained as a by-product of the work I’ve been doing on my project so far is unbelievable, but more importantly has been a better, more pleasant experience compared to the traditional system of gaining knowledge by reading books and tutorials. In this blog, I will be summarising the work that I’ve been doing for the past 2 weeks, update the readers on my current position and give an idea of what lies ahead.&lt;/p&gt;

&lt;p&gt;As I discussed in my previous blog, the work for the first two weeks mostly involved using Cython to translate the host code into something which could be compiled to provide better performance. More importantly, apart from the Cython part, I had to also start working on porting the kernels from their pure CUDA C form into something which Python/Cython could also understand. While the actual work to do so did not take long or came across as very challenging, the most difficult part in the whole process, undeniably, was to get the two, Cython and CuPy to talk to one another.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;p&gt;When I finished writing the previous blog, I was still left with quite a large volume of host code which was waiting to be converted to the Cython equivalent. Going through the previous blog, I’d like to make one correction in the part where I said I found Cython quite confusing. Infact, calling it a correction would not be correct. Instead, I should say that Cython isn’t actually all that difficult as I might have made it look like in the last blog. After a fair amount of reading and writing code in Cython, I think the developers of Cython have actually done an excellent job. After the initial barrier is crossed (and it happens fairly naturally), writing Cython code feels just as normal and second nature as pure Python code. Infact, I’d go so far as to claim that Cython feels more natural to me now than Python itself because of my previous experience in C++. This newly found familiarity allowed me to proceed quite quickly with this part of the project. The only problems that I encountered were: (1) the inter-conversion between arrays/vectors and pointers that we are used to in C++ is not possible in case of Cython, and this resulted in me being forced to make some slight changes in the host code, and (2) using structs in Cython isn’t as direct and straighforward as in C/C++. The reason behind this is the fact that Cython tries to estabilish relationships between C-type data structures and Python objects. While this is quite trivial for objects of types int, float, char, and even arrays to some extent, in case of structs this is nowhere near as easy. While I am sure there must have been some hacky way around this problem too, me and my mentors decided it is not something worth wasting time on, and thus we decided to bypass all structs and directly used each of their attributes as variables with the struct name attached as prefix, so &lt;code class="language-plaintext highlighter-rouge"&gt;host_params_h.shared_size&lt;/code&gt; became &lt;code class="language-plaintext highlighter-rouge"&gt;host_params_h_shared_size&lt;/code&gt;. While not the exact same thing, it allowed us to achieve the same objective without a lot of modifications to the code, either in terms of declaration or syntax. The only downside to this whole approach was that it made the code quite verbose, as instead of passing a single struct with 10 fields inside it, we were forced to pass 10 variables for each struct. This unique problem was further aggravated as we were using global variables instead of passing them around as arguments, and as every Python user would know, this meant adding the line &lt;code class="language-plaintext highlighter-rouge"&gt;global &amp;lt;varname&amp;gt;&lt;/code&gt; before every function body, which when done for every method and every variable, meant a lot of lines which could have been avoided. Apart from these two major issues and couple of minor problems here and there, the whole process was fairly straight forward. At the end of this step, I was left with a Cython file which compiled just fine, but didn’t really accomplish much. The key ingredient that was missing, was the very the heart of the project: the kernels.&lt;/p&gt;

&lt;p&gt;Kernel is actually nothing but a fancy word for the part of the code which actually executes on the GPU. Given the project title, it’d be fairly obvious that in our project, the kernels are actually where the magic happens. While this is not meant to discount the importance of the host (or the CPU code), the kernels are ulimately the part of the program which are responsible for the performance boosts that we observe. Kernels, atleast those which are meant to be executed on Nvidia’s GPUs, are usually written in a language known as CUDA C. This is a special language that is written on top of the original C language, but with extra set of features, classes and methods which provide us an abstracted interface to control various aspects of the program and the way it is implemented on the GPU, more so than a conventional serial algorithm meant to be implemented on the GPU. While using CUDA C is quite straightforward, especially with the large community support and well-written documentation, we unfortunately could not make use of that as we wanted something that was written and compatible with Python and things written on top of Python. Thus, after a lot of deliberation and discussion, my mentors and I agreed to use something known as CuPy to handle the CUDA C part of our code.  CuPy is an incredibly well-written module with neat documentation and decent community support, which made things a lot simple for us. However, more than anything, the biggest advantage of using CuPy was its RawModule method. The idea behind RawModule was to allow users who already have a CUDA C file written to do some specific task (us!) could simply re-use their code and get away with the whole problem of running kernels in Python very, very easily. Let me demonstrate it using an example and that would perhaps make things even more clear:&lt;/p&gt;

&lt;div class="language-python highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;
&lt;span class="n"&gt;loaded_from_source&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;r'''
extern "C"{
...
__global__ void test_sum(const float* x1, const float* x2, float* y, \
unsigned int N)
{
unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;
if (tid &amp;lt; N)
{
y[tid] = x1[tid] + x2[tid];
}
}
}'''&lt;/span&gt;

&lt;span class="n"&gt;module&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;RawModule&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;code&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;loaded_from_source&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ker_sum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;module&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;'test_sum'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;N&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;span class="n"&gt;x1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;x2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cp&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;float32&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;ker_sum&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;   &lt;span class="c1"&gt;# y = x1 + x2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;While the code looks fairly self-explanatory, I’ll give a quick runthrough anyway. The idea behind CuPy’s RawModule method, true to it’s name, is to allow the raw CUDA source to work with Python, which in our case is the string named &lt;code class="language-plaintext highlighter-rouge"&gt;loaded_from_source&lt;/code&gt;. Using RawModule, we process the string as a CUDA C source file, extract the relevant kernels from the file, &lt;code class="language-plaintext highlighter-rouge"&gt;test_sum&lt;/code&gt; in our case, and we’re done! The function is stored as &lt;code class="language-plaintext highlighter-rouge"&gt;ker_sum&lt;/code&gt; in Python, and is ready for use just like any other Python method. In order to keep the look and feel of the module as close to the original CUDA C source codes, even the way kernels are called is quite similar to how they’re called in C. The first two parameters, are the grid and block dimensions, and finally we follow through with the actual parameters to the kernel. Clearly, this allowed us to fast-track a lot of the kernel porting work, and quickly develop a Python version of our original proof-of-concept code.
However, like everything else, even this wasn’t going to work as easily as we’d initially expected. Instead, I faced a new challenge, which was the use of a struct written in the CUDA library, used for fast fourier trasforms, known as &lt;code class="language-plaintext highlighter-rouge"&gt;cufftComplex&lt;/code&gt; and &lt;code class="language-plaintext highlighter-rouge"&gt;cufftReal&lt;/code&gt;. Like I explained in the previous paragraph, this problem was quite similar in nature to the whole Python object to C object transformation and vice versa. If anything, this probably felt more explicit in nature since we could clearly see the actual C code written (as the RawModule’s string). The problem was that the structs are very specific to their definition, and it is simply not possible to pass anything to the RawModule and expect it to process it as a struct. Even though a numpy array of datatype complex64 might be storing the exact same data as an array of &lt;code class="language-plaintext highlighter-rouge"&gt;cufftComplex&lt;/code&gt;, the two cannnot be interconverted like vectors of integers and other primitive types can be. This again posed a challenge as it would mean a deviation from the original code. Finally, after reading a lot of stackoverflow and still not being satisfied with any of the answers, I let my mentors help me out with the code. The final solution that came out did a couple of things: first we got rid of the &lt;code class="language-plaintext highlighter-rouge"&gt;cufftComplex&lt;/code&gt; and &lt;code class="language-plaintext highlighter-rouge"&gt;cufftReal&lt;/code&gt; structs, and instead introduced the other, C datatype, serving the same purpose, a &lt;code class="language-plaintext highlighter-rouge"&gt;complex&amp;lt;float&amp;gt;&lt;/code&gt;. This amazing datatype did the exact same thing, except we could pass a numpy (or cupy) array of type complex64 and it would automatically read it as a &lt;code class="language-plaintext highlighter-rouge"&gt;complex&amp;lt;float&amp;gt;&lt;/code&gt; array! It quite literally solved the whole problem in an instant, and the only modifications we had to make were change how the real and imaginary parts of the complex numbers were being handled. While the struct definition required us to handle both separately, the new format made things even simpler by letting us perform calculations on both the real and imaginary parts simultaneously (basically how you’d expect to work with a complex number anyway!). With this, and again a few minor changes here and there to ensure the data transfer between Cython and CuPy wasn’t throwing errors, I was done! However, by now I had a Cython file with 1300+ lines with a &lt;em&gt;lot&lt;/em&gt; of room for bugs and unexpected errors and behavior.&lt;/p&gt;

&lt;p&gt;This brings us to the current time. The current objective for me is to get the file to work properly and produce the right about, so basically debugging. However, unlike smaller programs and files which are debugged with usually a single method in focus, I have to constantly let the whole chain of methods to execute even if I know that the bug is in some specific method, simply because its impossible to recreate the testing environment otherwise. For instance, I have been debugging the code since yesterday, and except one time where I got a segmentation fault, every time the program crashes, I am being forced to restart my computer just to start the debugging process again. Reason you ask? The program is working on a small subset of an already trimmed dataset, occupying around 3 GB of space. However, whenever I execute the program and try to run it, it is loading all that data first in the RAM (which slows the computer to a crawl almost instantly due to the limited 8GB RAM), and subsequently to the GPU (where it takes up 3/4 GB VRAM present). On force closing the program, while the RAM does free up after some time to a state where I can start using the computer again, the GPU does not!! I am yet to read into the nitty-gritty of this, but from what I understood, we need to explicitly clear the VRAM occupied by CuPy using methods given in the documentation. However, when the program does not work as expected (which is currently 100 percent of the times I have run it), the program simply crashes before executing the lines which free up the memory. The result? A GPU which is loaded up and unable to free its VRAM. Resetting seems to not work for some reason, and the only option I have found so far (admittedly I havent researched well enough but its only been 24 hours since I reached this stage) which does work is restarting my computer. This, as I am sure you can already feel, is a very frustrating way to debug things, but I am happy to say that I am still making progress, albeit a little slowly than I would have liked to.&lt;/p&gt;

&lt;p&gt;That pretty much sums up everything that I have been upto for the past 2 weeks. The progress is a little on the slower side, as I expected to have the demo working &lt;em&gt;and&lt;/em&gt; producing the correct output by now, but unfortunately the code still needs debugging. Hopefully this should be over soon, and we can then move to integrating it with RADIS and making changes that should allow the user to make use of our program. After that, we would be focussing on implementing other methods of calculating spectra, and possibly also methods which support non-equilibria conditions. Hopefully I should have a lot more to tell you guys 2 weeks from now! Till then, adios! And thanks for making it this far (if you actually did so :P) Cheers.&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2020/06/20200629_2135_pkj-m/</guid><pubDate>Mon, 29 Jun 2020 20:35:56 GMT</pubDate></item><item><title>Google Summer of Code - Blog #1!</title><link>http://openastronomy.org/Universe_OA/posts/2020/06/20200613_2135_pkj-m/</link><dc:creator>pkj-m</dc:creator><description>&lt;div&gt;&lt;p&gt;Hey there, welcome to the second blog of the series, and the first one to document the coding period. The Community bonding period which I described in my previous blog ended on 31st May and paved the way for the official coding period of the Google Summer of Code. These past two weeks were my first where I spent most of my time working on the actual code that will be a part of my project. My primary objective over these two weeks was to study the proof of work code that implements the spectral matrix algorithm to compute the spectra and execute it on a GPU. This was followed by a period of studying the different mechanisms with which RADIS calculates the spectras, and to understand the differences between each of them. This was important as implementing GPU compatible methods for all these distinct pipelines is my final objective and it is essential for me to understand the differences between these methods at the very onset of my project. Finally, the remaining time was spent on back and forth discussions with my mentors on various languages and libraries that could have been possible choices for undertaking this project. Once we had made our decision, I spent the time going through the library’s documentation, source code and tutorials to familiarize myself with these tools.&lt;/p&gt;

&lt;p&gt;The first major objective for these two weeks focussed on studying and executing the proof of work code. This was a single CUDA C file which demonstrated the idea of using a spectral matrix to compute a spectra while making use of a GPU could offer performance boosts of multiple orders over the naive methods. I initially planned on executing the code and running it on my personal computer, but the idea was quickly dismissed because of reasons I already discussed in my previous blog. As a result, I ended up using Google Colab for this experimentation which came with its own fair share of discomforts. The first, and most significant of which, was the lack of persistent storage on Colab and thus being forced to resort to Google Drive for saving our database instead. This was costly in terms of both, the time it took to store the data on the cloud and also on the overall performance of the code as the time taken to load the data to memory increased significantly compared to a single CPU-GPU system like my personal laptop. This however, was not detrimental to the fundamental objective as the benchmarking could be done for each part of the code separately, and thus it did not influence or affect the execution of the device code or its perfomance in any way. Another task which popped up when using Colab to run CUDA was to setup the system so it could run native CUDA C files along with the Python code as well. This fortunately was not very difficult to solve and a couple of google searches gave us the list of all the necessary packages we needed to compile and execute C files on Colab. Once that was set up, the only thing that was left for me to do was transfer the data from my laptop to Google Drive. This once again posed a problem that I had not anticipated. Uploading 8GB of data takes &lt;em&gt;much&lt;/em&gt; longer than downloading the same amount of data! As soon as that realization hit me, I decided to adopt another approach. I copied the code that I used to download the data from the FTP server to my local storage and ran it on Google Colab! This allowed me to once again redownload the entire data (which in the raw format was ~ 30GB) directly on my Drive instead. The process was much faster than I had anticipated and I soon had the raw data on my Drive. After running another couple of scripts to format and repartition the data into separate numpy arrays, I was ready to go. Execution of the code went smoothly except for a few hiccups surrounding the matplotlibcpp library that was being used to plot the output spectra. I wasn’t able to solve this problem immediately like the others and talked to my mentors about it. They advised me to not worry too much about it right now as it really wasn’t the critical part of the project. The major part, the kernel that was supposed to run on the GPU ran as expected and the results we obtained by timing the kernel performance were very positive! Now that I had successfully executed the code, what followed was a series of different runs of the same code, only this time with a different aim to test how far we could take this GPU compatible code. To give some numbers here, the original proof of work code that crunched the 8GB processed database computed a total of 240 million lines in less than a second! To be more specific, it took 120 ms on average to achieve that number. To put that into perspective, a naive implementation of the same code, that does not make use of the optimizations we did here, would take 10,000x longer to produce the same results! That in itself makes the naive approach an impractical solution to the problem. Compared to the current RADIS implementation, the performance gain was still significant with upto 50x gain in terms of time spent for computing the spectra. In order to see how far we could take this code, we also tried it with it a bunch of different ranges from the same dataset. While the original code was tested on a range that spanned from 1750 to 2400 cm-1 wavenumber, we took it as far as 1250-3050 cm-1. Surprisingly, the code scaled pretty well with the increase in the number of lines being computed, going from the original 120 ms taken to compute 240M lines to ~ 220 ms to compute 330M lines. Testing such a wide range and getting such positive results was sufficient proof for us to pack up the analysis part and move on to the actual implementation.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;p&gt;In order to integrate the GPU compatible spectral matrix method with the RADIS code base, the first thing that needed to be worked on was the language itself. The proof of work had been written completely in CUDA C, while RADIS is pure-Python. In order to bridge this gap, we had multiple options. The first and most obvious was to simply rewrite the entire code using Python with the help of some CUDA library. This, however, meant a lot of work in re-implementing the multiple methods, and more importantly, did not allow us to reuse the code that already existed. Therefore, in order to maximize our efficiency and also get the best performance possible, we decided to use a new language, or more specifically – a language extension for Python, known as Cython. The idea behind Cython is to use Python with a static compiler, which allowed Python programs to be precompiled into binaries, which could then be imported to other Python programs and achieve performance on par with native C code, because that is the intermediary code Cython converts the Python code into! Thus, by extension, any code that was already written in C was directly compatible with Cython. The main task now was to get the C code we had with us to talk to Cython with as few modifications as possible. This infact, is something that is still ongoing and would be finished as a part of my first evaluation. The last few days of this period have mostly been spent on learning Cython and its nuances. While the idea of Cython is to provide a smooth experience for Python users to gain C-level performance, ironically I had the opposite experience with it. I found Cython quite confusing at the beginning, and while most resources and tutorials focussed on making Python code achieve C-level performance, I was genuinely surprised by the lack of documentation/tutorials explaining how to export C code that already exists to Python. The few examples that were mentioned on the website were very generic and did not help much in terms of my requirements, where I needed to use things like references of vectors, etc. However, with more research and googling, I was able to find a compromise solution that worked well and thus allowed me to execute the method in Cython with minimal modifications to the original code.&lt;/p&gt;

&lt;p&gt;Overall, I think its been a good two weeks with a lot of progress made on the knowledge front. Apart from the objectives mentioned above, I also went through the draft of the paper my mentors have been working on which goes into the mathematics of the method and explains how it works. While I wasn’t able to comprehend everything properly, it did give me a good high-level idea of what exactly we’re trying to accomplish with our kernels. With this, I think I’d like to conclude this blog. Over the next two weeks, the end of which will also mark the completion of my first evaluation, I’ll continue to work on Cython-izing our host code, and start looking into CuPy as an alternative to CUDA C for our project! More about that in the next blog! Thanks!&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2020/06/20200613_2135_pkj-m/</guid><pubDate>Sat, 13 Jun 2020 20:35:56 GMT</pubDate></item></channel></rss>