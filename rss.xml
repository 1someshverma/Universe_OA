<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Universe OpenAstronomy</title><link>http://openastronomy.org/Universe_OA/</link><description>This is an aggregator of openastronomy people</description><atom:link href="http://openastronomy.org/Universe_OA/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Wed, 23 Jun 2021 04:34:20 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>astropy@GSoC Blog Post #3, Week 3</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210622_2223_suyog7130/</link><dc:creator>Suyog Garg</dc:creator><description>&lt;div&gt;So, it's the start of the 3rd week now. I will be virtually meeting Aarya and Moritz again Tom.&lt;br&gt;&lt;br&gt;For the past few weeks now, I have been pushing commits to a Draft PR¬†&lt;a href="https://github.com/astropy/astropy/pull/11835"&gt;https://github.com/astropy/astropy/pull/11835&lt;/a&gt;¬†on GitHub. I wanted to have something working quite early in the project, in order to be able to pinpoint accurately when something doesn't work. This is why I started with directly adding the &lt;b&gt;cdspyreadme&lt;/b&gt; code within Astropy. Afterwards, I am also writing the code from scratch. As more of the required features from &lt;b&gt;cdspyreadme&lt;/b&gt; get integrated into &lt;i&gt;cds.py&lt;/i&gt;, those files and codes added earlier will be removed.&lt;br&gt;&lt;br&gt;About the reading/writing to Machine Readable Table format, in fact I wrote about it briefly in my GSoC Proposal that I could attempt it as an extension. I don't have an opinion on whether or not it should have it's own format classes etc. However, since the title of my GSoC project is to &lt;b&gt;Add a CDS format writer to Astropy&lt;/b&gt;, I would prefer to work on the CDS format writer first and then on the MRT format. The MRT header anyway appears to be a bit simpler than the CDS header, so there shouldn't be much difficulty in the extension.&lt;br&gt;&lt;br&gt;So, in a nutshell, this is my workflow:&lt;br&gt;&lt;ul style="text-align: left;"&gt;&lt;li&gt;Try out directly using &lt;b&gt;cdspyreadme&lt;/b&gt; from within Astropy.&lt;/li&gt;&lt;li&gt;Add CdsData.write method.&lt;/li&gt;&lt;li&gt;Add a ByteByByte writer.&lt;/li&gt;&lt;li&gt;Write features to add complete ReadMe to the Header, starting off with having both ReadMe and Data in a single file.&lt;/li&gt;&lt;li&gt;Have features for writing separate CDS ReadMe and Data file.&lt;/li&gt;&lt;li&gt;Further work on some specific table columns, for instance, those containing Units and Coordinates.&lt;/li&gt;&lt;li&gt;Add appropriate tests along the way.&lt;/li&gt;&lt;li&gt;Resolve other issues that come up.&lt;/li&gt;&lt;li&gt;MRT format reader/writer.&lt;/li&gt;&lt;/ul&gt;&lt;br&gt;I have completed the first three tasks and will now work on the fourth. I think by the time this finishes, a separate &lt;i&gt;CDSColumn.py&lt;/i&gt; won't be required. I can open another PR which adds the Data writer, in the meantime.&lt;div&gt;&lt;br&gt;&lt;/div&gt;&lt;div&gt;Let's see how it goes!&lt;/div&gt;
&lt;!-- TEASER_END --&gt;&lt;/div&gt;</description><category>Astropy</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210622_2223_suyog7130/</guid><pubDate>Tue, 22 Jun 2021 21:23:00 GMT</pubDate></item><item><title>Rotation and Coordinates</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210622_0048_jeffreypaul15/</link><dc:creator>Jeffrey Paul</dc:creator><description>&lt;div&gt;&lt;p&gt;Finally, the official ‚Äúcoding period‚Äù of &lt;strong&gt;GSoC&lt;/strong&gt; finally began a couple of days ago. From where we started of with Sunkit-Pyvista, to where we are today makes me feel a tad bit happy!¬†üòÑ&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/985/1*-X2tw67TTrMoj3F43QS0uA.jpeg"&gt;&lt;/figure&gt;&lt;p&gt;Weeks 1 and 2 were initially set out for me to complete adding rotation functionality to the library, which started off great, but ended up causing some confusion üòÖ.&lt;/p&gt;
&lt;p&gt;This was quickly sorted out and we went with not having to implement rotation functionality and moved on, learning that not everything will go according to plan and it‚Äôs okay for stuff to not work out at¬†times.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;The rest of the work that I had set out to do was completed well and it was all smooth sailing from then¬†on.&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;We worked on implementing the setting of the initial camera position from Astropy‚Äôs Skycoord.&lt;/li&gt;&lt;li&gt;A few 2D methods were converted to it‚Äôs 3D counter part to be¬†used.&lt;/li&gt;&lt;li&gt;Unit tests for the implemented methods were added as¬†well.&lt;/li&gt;&lt;/ol&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/225/0*alo7nZjT7uTO6BzJ"&gt;&lt;figcaption&gt;Mid-level solar flare, observed on Jan. 12,¬†2015.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;I‚Äôm don‚Äôt know the first thing when it comes to astrophysics or astronomy, I do know that there is some pretty cool stuff going on out there though! I may not know what that is, but there‚Äôs a small sense of satisfaction in knowing that maybe whatever I‚Äôm doing is going to help someone out there do their work¬†better.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;So far, so¬†good.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=1c9683c38461" width="1"&gt;&lt;/div&gt;</description><category>SunPy</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210622_0048_jeffreypaul15/</guid><pubDate>Mon, 21 Jun 2021 23:48:18 GMT</pubDate></item><item><title>Chapter 1: First Flight</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_2240_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;div&gt;&lt;p&gt;Hey! Missed me? I‚Äôm back with another blog, the first related to the Coding Period. Got some progress and interesting observation to share!&lt;/p&gt;
&lt;h3&gt;Ready -&amp;gt; Set -&amp;gt; Code -&amp;gt; Analyze&lt;/h3&gt;
&lt;p&gt;The first thing I did in the coding period, was analyse the problem and get a feasible approach to resolve it.&lt;br&gt;&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; Find the complexity of the Legacy and LDM method.&lt;br&gt;
&lt;strong&gt;Solution:&lt;/strong&gt; Run some benchmarks and find the bottleneck step.&lt;br&gt;&lt;/p&gt;
&lt;p&gt;First I chose the &lt;strong&gt;Legacy&lt;/strong&gt; method because if its simpler architecture. I ran some benchmarks varying the &lt;code class="language-text"&gt;spectral range&lt;/code&gt; of &lt;code class="language-text"&gt;OH&lt;/code&gt; and &lt;code class="language-text"&gt;CO2&lt;/code&gt; molecule to get similar number of lines. I kept parameters like &lt;code class="language-text"&gt;pressure&lt;/code&gt;, &lt;code class="language-text"&gt;temperature&lt;/code&gt;, &lt;code class="language-text"&gt;broadening_max_width&lt;/code&gt;, &lt;code class="language-text"&gt;wstep&lt;/code&gt;, etc constant to see the dependence of Legacy method on &lt;strong&gt;Spectral range&lt;/strong&gt;. &lt;br&gt;&lt;/p&gt;
&lt;p&gt;In order to get similar number of lines, I created a function which will take the &lt;strong&gt;Spectrum Factory&lt;/strong&gt; &lt;code class="language-text"&gt;dataframe&lt;/code&gt; and select the target number of lines. But the issue with Pandas dataframe is that when modify the dataframe there are chances that the metadata will get lost and we will no longer be able to do Spectrum calculation. To avoid this we have to drop the right number of lines with &lt;code class="language-text"&gt;inplace=True&lt;/code&gt;. So we will need to fix the number of lines and then we can proceed ahead with the benchmarking. Every parameter is the same except the Spectral Range.  Full code &lt;a href="https://gist.github.com/anandxkumar/cbe12f47170e1d71a82f4b246bd01dcc"&gt;here&lt;/a&gt;.&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Earlier we assumed that the complexity of Legacy method is: &lt;br&gt;
&lt;strong&gt;&lt;code class="language-text"&gt;Voigt Broadening = Broadening_max_width * spectral_range/math.pow(wstep,2) * N&lt;/code&gt;&lt;/strong&gt; &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Thus I was expecting to have different calculation time for both benchmarks. But to my surprise the computational times were almost equivalent! I re-ran each benchmarks &lt;strong&gt;100 times&lt;/strong&gt; just to be sure and more precise about it. Following were the observations:&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of lines - &lt;b&gt;{‚ÄòOH‚Äô: 28143, ‚ÄòCO‚Äô: 26778}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;Total Calculation time(Avg) -  &lt;b&gt;{‚ÄòOH‚Äô: 4.4087, ‚ÄòCO‚Äô: 3.8404000000000003}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;Total Voigt_Broadening TIME(Avg) - &lt;b&gt;{‚ÄòOH‚Äô: 3.1428814244270327, ‚ÄòCO‚Äô: 3.081623389720917}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;spectral_range - &lt;b&gt;{‚ÄòOH‚Äô: 38010, ‚ÄòCO‚Äô: 8010}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;Legacy_Scale - &lt;b&gt;{‚ÄòOH‚Äô: 4x10^14, ‚ÄòCO‚Äô: 8x10^13}&lt;/b&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are some inference we can make from the above observation:&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A)&lt;/strong&gt; The bottleneck step(Voigt Broadening) loosely depends on &lt;code class="language-text"&gt;Spectral Range&lt;/code&gt;.&lt;br&gt;
&lt;strong&gt;B)&lt;/strong&gt; The complexity of Voigt Broadening needs to be modified because there is a difference of order of &lt;strong&gt;~10&lt;/strong&gt; in the Legacy Scaled value of OH and CO2.&lt;br&gt;&lt;/p&gt;
&lt;p align="center"&gt;
&lt;span class="gatsby-resp-image-wrapper" style="display: block; margin-left: auto; margin-right: auto;"&gt;
&lt;a class="gatsby-resp-image-link" href="https://anandkumar-blog.netlify.app/static/d9b32b4e96e6cd9a91016a49ad940239/0b533/Blog2.png" rel="noopener" style="display: block;" target="_blank"&gt;
&lt;span class="gatsby-resp-image-background-image" style="padding-bottom: 100%; display: block;"&gt;&lt;/span&gt;
&lt;img alt="Blog2" class="gatsby-resp-image-image" src="https://anandkumar-blog.netlify.app/static/d9b32b4e96e6cd9a91016a49ad940239/0b533/Blog2.png" style="width: 100%; height: 100%; margin: 0; vertical-align: middle;" title="Blog2"&gt;
&lt;/a&gt;
&lt;/span&gt;&lt;br&gt;
&lt;b&gt;Credits - Me :p&lt;/b&gt;&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;So in order to do some analysis, we first need data of different steps in the broadening phase and conditions of various Spectrum which brings me to the &lt;strong&gt;Code&lt;/strong&gt; part in &lt;strong&gt;Coding Period.&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Profiler Class&lt;/h3&gt;
&lt;p&gt;The aim of this class is to replace all the print statements by a common &lt;code class="language-text"&gt;start&lt;/code&gt;, &lt;code class="language-text"&gt;stop&lt;/code&gt;, &lt;code class="language-text"&gt;_print&lt;/code&gt; method. Earlier each step computational time was done using &lt;code class="language-text"&gt;time()&lt;/code&gt; library. Now the whole codebase is being refactored with the Profiler class that will do all the work based on the &lt;code class="language-text"&gt;verbose&lt;/code&gt; level. In addition to this the biggest benefit is that each step will be stored in a dictionary with its computational time that will help me gather data to find which step is in actual bottleneck and further which part of the function is the most expensive time wise. A simple example is below:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Before:&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;if __debug__:
t0 = time()
..........
..........
if __debug__:
t1 = time()
.........
.........
if __debug__:
if self.verbose &amp;gt;= 3:
printg("... Initialized vectors in {0:.1f}s".format(t1 - t0))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;After:&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;self.profiler.start(
key="init_vectors", verbose=3, details="Initialized vectors"
)
.........
.........
self.profiler.stop("init_vectors")&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So using a common key we can make it happen. This will be stored in the conditons of &lt;code class="language-text"&gt;Spectrum&lt;/code&gt; object in the &lt;code class="language-text"&gt;'profiler'&lt;/code&gt; key. All these Spectrums and their conditions can be exported using a &lt;a href="https://radis.readthedocs.io/en/latest/spectrum/spectrum.html#spectrum-database"&gt;SpecDatabase&lt;/a&gt;. This will create a csv file comprising of all the parameters of all Spectrums which will be useful in getting some insights.
-&amp;gt; &lt;a href="https://github.com/radis/radis/pull/286"&gt;PR LINK&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Digging in whiting_jit&lt;/h3&gt;
&lt;p&gt;Based on several benchmarks, it is estimated that around &lt;strong&gt;70-80%&lt;/strong&gt; time is spent on calculating the broadening. The broadening part has the following hierarchy:&lt;br&gt;
&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;_calc_broadening()
-&amp;gt; _calc_lineshape()
-&amp;gt; _voigt_broadening()
-&amp;gt; _voigt_lineshape()
-&amp;gt; whiting_jit()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;On close inspection we observed that &lt;strong&gt;80-90%&lt;/strong&gt; time is spent on &lt;code class="language-text"&gt;whiting_jit&lt;/code&gt; process. Going further down in &lt;code class="language-text"&gt;whiting_jit&lt;/code&gt;, &lt;strong&gt;60-80%&lt;/strong&gt; time is spent on &lt;strong&gt;lineshape calculation.&lt;/strong&gt; Below is the formula:&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;lineshape = (
(1 - wl_wv) * exp(-2.772 * w_wv_2)
+ wl_wv * 1 / (1 + 4 * w_wv_2)
# ... 2nd order correction
+ 0.016 * (1 - wl_wv) * wl_wv * (exp(-0.4 * w_wv_225) - 10 / (10 + w_wv_225))
)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The whole process can be divided into 4 parts:&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;    part_1 =   (1 - wl_wv) * exp(-2.772 * w_wv_2)

part_2 =    wl_wv * 1 / (1 + 4 * w_wv_2)

# ... 2nd order correction
part_3 =  0.016 * (1 - wl_wv) * wl_wv * exp(-0.4 * w_wv_225)

part_4 =  - 10 / (10 + w_wv_225)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The complexity of each part comes out: &lt;br&gt;
&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;    o1 = broadening__max_width * n_lines / wstep

O(part_1) = n_lines * o1
O(part_2) = n_lines * 4 * o1
O(part_3) = (n_lines)**2 * o1
O(part_4) = o1 &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Running several benchmark showed us that &lt;strong&gt;part_3&lt;/strong&gt; takes the most time out of all steps. So clearly we can see that the complexity of Legacy method is not dependent on
Spectral Range but rather &lt;code class="language-text"&gt;Number of Calculated Lines&lt;/code&gt;,&lt;code class="language-text"&gt;broadening__max_width&lt;/code&gt; and &lt;code class="language-text"&gt;wstep&lt;/code&gt;. It may seem that the complexity of Legacy method is:&lt;br&gt;&lt;/p&gt;
&lt;p align="center"&gt;&lt;b&gt; n_lines^2 * broadening__max_width * n_lines / wstep&lt;/b&gt;&lt;/p&gt; &lt;br&gt;
&lt;p&gt;But inorder to prove this we need more benchmarks and evidence to verify this and it may involve normalization of all steps in lineshape calculation!&lt;br&gt; &lt;/p&gt;
&lt;p&gt;So the goal for the next 2 weeks is clear:&lt;br&gt;
&lt;b&gt;i)&lt;/b&gt; Refactor the entire codebase with Profiler.&lt;br&gt;
&lt;b&gt;ii)&lt;/b&gt; Find the complexity of &lt;strong&gt;Legacy Method&lt;/strong&gt; with the help of more benchmark and analysis.&lt;br&gt;
&lt;b&gt;iii)&lt;/b&gt; Do the same for &lt;strong&gt;LDM Method&lt;/strong&gt;!&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Ok I guess time‚Äôs up! See you after 2 weeks :)&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_2240_anandxkumar/</guid><pubDate>Mon, 21 Jun 2021 21:40:32 GMT</pubDate></item><item><title>About my Google Summer of Code Project: Part 1</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_1658_adwaitbhope/</link><dc:creator>Adwait Bhope</dc:creator><description>&lt;div&gt;&lt;p&gt;I had been eyeing Google Summer of Code last year (and the year before that), but never really got around to doing anything about it. It‚Äôs a wonderful learning experience and being in my final year of college this was the last opportunity I was going to get. So I decided to give it a¬†shot.&lt;/p&gt;
&lt;p&gt;I started late, sometime during late February. I picked out a few organizations that looked interesting to me. &lt;a href="https://openastronomy.org/"&gt;openastronomy&lt;/a&gt; particularly caught my eye because I was working on another project of mine related to Astronomy. In fact we were using one of the Python libraries under openastronomy. Now this is an umbrella organization, which means that there are multiple sub-organizations‚Ää‚Äî‚Ääsunpy, astropy, radis, poliastro, and a few more. They‚Äôre used extensively by the scientific community in their research. The project I selected was under sunpy, which is a Python library for solar data analysis. The project is about resampling data to increase or decrease its resolution‚Ää‚Äî‚Äämore on that later. Again, I had recently performed this operation in one of my projects, so it seemed only natural for me to go with this one. I worked on some issues on GitHub and submitted PRs, tried to get a hold of the codebase, put together a proposal, got feedback from the project mentors and friends, and submitted it. After about a month of impatient waiting, I received an email saying that my proposal was accepted! Awesome!&lt;/p&gt;
&lt;p&gt;Now, I plan to continue writing these blogs throughout the project and since this is the first one, let me take a moment to talk about the project. So, there‚Äôs a sunpy-affiliated package called ndcube, which exists to provide users an easier way of handling coordinates. Astronomical data like images taken from cameras are usually stored as n-dimensional arrays. A dimension could represent spatial or temporal axes. In such an array, the pixel coordinates map to some coordinates in the real world. These could be RA and Dec, or in the case of solar data, Helioprojective Latitude and Longitude. Nevertheless, there needs to be a mapping from the pixels to the real world. This is given by the World Coordinate System, which is just a set of (complicated) mathematical transformations. ndcube is a package that correlates the actual data with its transformations in such a way that you can manipulate the data, and the transformations will continue to remain consistent. It can be used with any type of data like images, spectra, timeseries data, and so¬†on.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*MHKTGHeR4F3nHd2gO1ocUw.png"&gt;&lt;figcaption&gt;Open Astronomy and¬†SunPy&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Often, researchers like to upsample or downsample the resolution of their data, perhaps to improve the Signal to Noise Ratio or even just to get their data onto the same grid. My project under Google Summer of Code is exactly this‚Ää‚Äî‚Ääto implement this functionality under ndcube. Luckily, there exists a package called reproject that has a few algorithms implemented already. My job would be to expose this through a succinct API under¬†ndcube.&lt;/p&gt;
&lt;p&gt;So far, my mentors and I have broken down this work and set smaller and more achievable targets to begin with, and I‚Äôve started working on them. Unfortunately, my Community Bonding Period was quite stagnant thanks to my college commitments, but now that they‚Äôre out of the way, I have more time on my hands to devote to the project. I‚Äôll be publishing more blogs about my progress in the coming weeks, hopefully more frequently. Talk to you in the next¬†one!&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=b56e7277046e" width="1"&gt;&lt;/div&gt;</description><category>SunPy</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_1658_adwaitbhope/</guid><pubDate>Mon, 21 Jun 2021 15:58:34 GMT</pubDate></item><item><title>GSoC Progress Report? Almost Done!</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_1523_dhruv9vats/</link><dc:creator>Dhruv Vats</dc:creator><description>&lt;div&gt;&lt;p&gt;With only 2 weeks into the coding period, it feels good to say that the bulk of the work associated with the 2 milestones is &lt;em&gt;almost &lt;/em&gt;done. Almost because the newly added functionality is yet to be battle-tested and while there is always room for change and improvements, my focus will be shifting from the main objective to the optional objectives.&lt;/p&gt;
&lt;h5&gt;The Project&lt;/h5&gt;&lt;p&gt;Scientific jargon¬†ahead!&lt;/p&gt;
&lt;p&gt;The study and interpretation of time-series data have become an integral part of modern-day astronomical studies and a common approach for characterizing the properties of time-series data is to estimate the power spectrum of the data using the periodogram. But the periodogram as an estimate suffers from¬†being:&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;ol&gt;&lt;li&gt;Statistically inconsistent (that is, its variance does not go to zero as the number of data samples reach infinity),&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;2. Biased for finite samples,¬†and&lt;/p&gt;
&lt;p&gt;3. Suffers from spectral¬†leakage.&lt;/p&gt;
&lt;figure&gt;&lt;a href="https://github.com/StingraySoftware/stingray"&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/700/1*5GHLU2O-S9d06amIH5ESBA.png"&gt;&lt;/a&gt;&lt;figcaption&gt;Stingray is a spectral-timing software package for astrophysical X-ray (and other)¬†data.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;My project aimed at implementing and integrating a superior spectral estimation technique, known as the Multitaper periodogram, into a Software Package called Stingray, a sub-organization of OpenAstronomy.&lt;/p&gt;
&lt;p&gt;This Multitaper algorithm uses windows or tapers (bell-shaped functions), which are multiplied with the time-series data before finding its frequency domain estimate. These windows, called the discrete prolate spheroidal sequences (DPSS), help mitigate the problems mentioned above.&lt;/p&gt;
&lt;p&gt;Any more technical stuff and this blog will start taking the shape of my proposal, so I‚Äôll leave it here, but for anyone more interested, &lt;a href="https://www.dropbox.com/s/g2m2p10en8ygpmz/Proposal.pdf?dl=0"&gt;here&lt;/a&gt; is the proposal.&lt;/p&gt;
&lt;h5&gt;The Process&lt;/h5&gt;&lt;p&gt;I started working on the project in early March, before submitting the proposal, and it initially included writing a wrapper around an external package to make the use of this tool coherent with the rest of the project. While a proof-of-concept implementation was put together, it was later decided to use SciPy to do the grunt work, as it already was a dependency, somewhat changing the initial milestones.&lt;/p&gt;
&lt;p&gt;So in a way, I was already working on the project before the results were announced and ended up opening a pull request with the newly added method 1 week into the coding period and it got merged a week later, which happened to be fairly early in the GSoC program coding timeline. Perks of open-source?!&lt;/p&gt;
&lt;p&gt;So while there are sure to be improvements and additions in terms of coherency and features, this does give a bit more breathing room and time for experimentation and exploration, and I‚Äôll try and make good use of it, primarily by working on the optional but quite good-to-have features.&lt;/p&gt;
&lt;p&gt;For anyone interested, &lt;a href="https://github.com/StingraySoftware/stingray/pull/578"&gt;here&lt;/a&gt; is the¬†PR.&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=6239f301b23" width="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_1523_dhruv9vats/</guid><pubDate>Mon, 21 Jun 2021 14:23:27 GMT</pubDate></item><item><title>The first weeks of X-Rays and Electron.Js</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210620_1829_rachittshah/</link><dc:creator>Rachitt Shah</dc:creator><description>&lt;div&gt;&lt;p&gt;Python, X-rays, and JSON files that don‚Äôt comply and make you¬†cry.&lt;/p&gt;
&lt;p&gt;GSoC has officially started!&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/498/0*AXMzJYLpLP0sFPgE"&gt;&lt;figcaption&gt;We‚Äôre going to¬†code!&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Since is a Desktop App made with Electron and Python, my first priority was getting DAVE to compile again. The process was tedious and took a good amount of time to fix the flask backends which power¬†DAVE.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;Another error that came while getting DAVE up was that DAVE uses Stingray‚Äôs APIs which are lower than 2.0, which gave a lot of broken packages. Fixing this was something that would be covered in the POC enhancements, however, it was important to tackle this to build the¬†MVP.&lt;/p&gt;
&lt;p&gt;Looked at some major changes in the coming 0.3 for Stingray and how to accommodate them, so we don‚Äôt have issues¬†later.&lt;/p&gt;
&lt;p&gt;Had exams alongside, so the first week was less effort extensive for¬†GSoC.&lt;/p&gt;
&lt;p&gt;Waiting for the next week to write more about my progress and¬†journey!&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=84f7557d97a7" width="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210620_1829_rachittshah/</guid><pubDate>Sun, 20 Jun 2021 17:29:09 GMT</pubDate></item><item><title>GSoC Post 1</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210620_1617_ndanzanello/</link><dc:creator>ndanzanello</dc:creator><description>&lt;div&gt;&lt;p&gt;Hey there. I am working on the Astrometry project from Gnuastro and I will explain below the first things that I have been doing.&lt;br&gt;&lt;br&gt;Basically, we have two catalogs: one is the query catalog, which we want to find its wcs, and the reference catalog, that gives some stars positions in celestial coordinates. We begin finding ‚Äúquads‚Äù, a group of 4 stars, on both catalogs. This part was already done, but the matching part between the quads needed some fixes.&lt;/p&gt;


&lt;!-- TEASER_END --&gt;

&lt;p&gt;The first thing we needed to fix was the vertices found on each catalog. It‚Äôs very important that all the vertices are labeled the same. First, we label the A and B vertices as the most separated ones. In the query catalog it‚Äôs just the Euclidean distance between the points, but on the reference catalog we have to use the angular distance between the points to get the same vertices. Prior to that, it was also using the Euclidean distance to the vertices on the reference catalog, so it would give different most separated A and B for the two catalogs.&lt;br&gt;After that, we have to choose the C and D vertices. First we label randomly the two remaining vertices as C and D and then we compare the ACB and ADB angles that are less than 180 degrees and choose C to be the one that has the lesser angle.&lt;/p&gt;



&lt;p&gt;Now, we have the A, B, C and D vertices to be the same when dealing with the same quads and we have to compute their hashes. The hashes were calculated using Cx = (c1-a1)/(b1-a1), where a1, b1 and c1 are the coordinates along the axis 1. Now we have the problem related to the rotations: the distance between the points is the same, but the distance along each axis is not the same! So the Cx would be different for different axis. The same would happen for Cy, Dx and Dy.&lt;/p&gt;



&lt;p&gt;To solve this, first we transform the celestial coordinates of the reference catalog into projection plane coordinates (TAN projection) using the midpoint of AB as the coordinates of the native pole.&lt;br&gt; We proceed defining new two axis (x and y, where the hashes will be calculated) using the A-B vector as a 45 degrees line contained in these axis. Then, we project the C-A and D-A vectors in these axis and get the hashes.&lt;/p&gt;



&lt;p&gt;The image below show an overview of the steps explained above.&lt;/p&gt;



&lt;figure class="wp-block-image size-large"&gt;&lt;img alt="" class="wp-image-56" src="https://ndanzanello.files.wordpress.com/2021/06/match_overview.png?w=1024"&gt;&lt;/figure&gt;



&lt;p&gt;&lt;/p&gt;



&lt;p&gt;&lt;/p&gt;&lt;/div&gt;</description><category>gnuastro</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210620_1617_ndanzanello/</guid><pubDate>Sun, 20 Jun 2021 15:17:45 GMT</pubDate></item><item><title>GSoC - 1</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210620_0300_gagan-aryan/</link><dc:creator>Gagan Aryan</dc:creator><description>&lt;div&gt;&lt;p&gt;This is the first blog that documents the coding period of my GSoC21 journey. I learnt a few interesting things in these two weeks, as I expected I would. So, let‚Äôs dive in and see if you knew few of these stuff I learnt.&lt;/p&gt;
&lt;h3 id="starting-off-"&gt;Starting off !!!&lt;/h3&gt;
&lt;p&gt;I started off by getting a brief idea of the scope of the changes that could be done to the dataframe. This was the task I had decided on for the first week. Whenever we are involved in a project that runs for a period of anywhere between 2-4 months it is important to have a timeline or a roadmap of sorts to be able to look back to. This doesn‚Äôt really have to be something rigid. We can chose to deviate from it and infact deviations are bound to happen due to multiple reasons. It can happen because of an unexpected bug in between, or because you came across some alternative that you did not consider at the start or simply because it is one of those projects that gives better insights as you dwell into it.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;Every good GSoC proposal consists of a tentative timeline that depicts the work we plan on doing as the weeks progress. Here is the timeline I had submitted in my proposal.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Timeline1" src="https://gagan-aryan.netlify.app/images/gsoc-1/timeline1.png"&gt;&lt;br&gt;
&lt;img alt="Timeline2" src="https://gagan-aryan.netlify.app/images/gsoc-1/timeline2.png"&gt;&lt;/p&gt;
&lt;p&gt;So as per this I was supposed to finish off the refactors to the dataframe and also finish setting up the benchamarks. But I was unable to complete these. I had underestimated the work it would take to complete them. Nonetheless, I also did have some time to look up at the things I am supposed to do in the second half of the coding period.&lt;/p&gt;
&lt;h3 id="memory-and-time-performance-benchmarks---tic-tok"&gt;Memory and Time performance benchmarks - Tic-Tok&lt;/h3&gt;
&lt;p&gt;Before making any changes to the codebase Erwan suggested me to have the benchmarks setup. So what do I mean by this? To make sure that the changes I am making to the code are indeed reducing the memory consumption of the computations we use a few tools that help us track the memory consumption for various calculations as a function of git commits. There are multiple tools that help us do this. Radis already used a tool developed by &lt;a href="https://github.com/airspeed-velocity/asv"&gt;airspeed velocity&lt;/a&gt; to track the memory computions. I ran into a lot of troubles in setting these up and a lost a lot of valuable time in the process ultimately Erwan fixed it and I was able to run the benchmarks on my machine.&lt;/p&gt;
&lt;p&gt;The benchmarks still seem to take a lot of time to run though and for them to be feasible to be used a tool through which I can check the performance regularly there are a few things I need to learn. I hope to pick these up in the next few days.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Performance" src="https://gagan-aryan.netlify.app/images/gsoc-1/brace-yourselves.png"&gt;&lt;/p&gt;
&lt;p&gt;We are also trying to look at a few other alternatives that can be used instead of asv. I will update the you guys regarding this in the next blog post.&lt;/p&gt;
&lt;h3 id="oh-pandas-here-i-deal-with-you-"&gt;Oh Pandas here I deal with you !&lt;/h3&gt;
&lt;h4 id="lets-ditch-a-few-columns"&gt;Let‚Äôs ditch a few columns&lt;/h4&gt;
&lt;p&gt;We can reduce the memory usage of pandas by using one really simple trick - avoid giving loading the columns that are not required for computation. Below I demostrate how just dropping a few columns can provide significant improvement in the memory consumption. I am using &lt;code&gt;HITEMP-CH4&lt;/code&gt; database for demonstration.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt; 1
&lt;/span&gt;&lt;span class="lnt"&gt; 2
&lt;/span&gt;&lt;span class="lnt"&gt; 3
&lt;/span&gt;&lt;span class="lnt"&gt; 4
&lt;/span&gt;&lt;span class="lnt"&gt; 5
&lt;/span&gt;&lt;span class="lnt"&gt; 6
&lt;/span&gt;&lt;span class="lnt"&gt; 7
&lt;/span&gt;&lt;span class="lnt"&gt; 8
&lt;/span&gt;&lt;span class="lnt"&gt; 9
&lt;/span&gt;&lt;span class="lnt"&gt;10
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre class="chroma"&gt;&lt;code class="language-python"&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;radis.io.hitran&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;hit2df&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hit2df&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"06_HITEMP2020_2000.0-2500.0.par"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;memory_usage&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"deep"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="n"&gt;memory&lt;/span&gt; &lt;span class="n"&gt;usage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;30.5&lt;/span&gt; &lt;span class="n"&gt;MB&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s2"&gt;"id"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"iso"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;memory_usage&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"deep"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="n"&gt;memory&lt;/span&gt; &lt;span class="n"&gt;usage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;25.4&lt;/span&gt; &lt;span class="n"&gt;MB&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;The peak memory usage before dropping the columns was 30.5 MB and once I remove a few columns the peak memory usage becomes 25.4 MB. I have already implemented the dropping of id column and handled the case of single isotope as well by dropping the column and istead just storing the information of the isotope as a meta attribute. We have also finalised on the discarding of the other columns by considering the physics of these quantities. Let‚Äôs check out a few of them. Since I haven‚Äôt already implemented the optimisations that follow I will save the implementation details for the next blog.&lt;/p&gt;
&lt;h4 id="einsteins-coeffecients-and-linestrengths"&gt;Einstein‚Äôs Coeffecients and Linestrengths&lt;/h4&gt;
&lt;p&gt;There are four parameters of interest to describe the intensity of a line : Linestrength $(int)$, Einstein emission coefficient $(A)$ and Einstein absorption coefificent $(B_{lu})$, Einstein induced emission coefficient $(B_{ul})$. All of them are somehow linked to the Squared Transition Dipole Moment $(R)$. &lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;$$ B_{lu}=10^{-36}\cdot\frac{8{\pi}^3}{3h^2} R_s^2 \cdot 10^{-7} $$&lt;br&gt;
$$ B_{ul}=10^{-36}\cdot\frac{8{\pi}^3}{3h^2} \frac{gl}{gu} R_s^2 \cdot 10^{-7} $$&lt;br&gt;
$$ A_{ul}=10^{-36}\cdot\frac{\frac{64{\pi}^4}{3h} {\nu}^3 gl}{gu} R_s^2 $$&lt;/p&gt;
&lt;p&gt;So now the idea would be to drop the $int$ column and use $A_{ul}$ to calculate the value of $int$ from it. The reason to drop $int$ and not $A_{ul}$ some databases like &lt;code&gt;ExoMol&lt;/code&gt; databases only provide the value of $A_{ul}$.&lt;/p&gt;
&lt;h4 id="concat-better"&gt;Concat better&lt;/h4&gt;
&lt;p&gt;For anyone who wants concate multiple datafiles pandas tends to become useless as the memory scales up. I started out experimenting concat operations inorder to cluster the isotopes of each type, run computations on them and later concat them. But I later learnt that since this data is already in the form of a single dataframe, indexing is a better parameter to track the memory consumption. Nonetheless there are a few other places in Radis where we process multiple files and concat them, hence this experiment would help us decide how we can chose to replace the current approach with a better one. I tried out three methods. I was using some random dummy datafiles of around 780 MBs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Normal &lt;code&gt;pandas.concat&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Concat with a doubly ended queue&lt;/li&gt;
&lt;li&gt;Concat with parquet&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here are the results of each of these methods -&lt;/p&gt;
&lt;div class="tab" id="cbbfde2b70afc7e2"&gt;
&lt;div class="tab__links"&gt;
&lt;button class="tab__link"&gt;pandas.concat&lt;/button&gt;
&lt;button class="tab__link"&gt;deque&lt;/button&gt;
&lt;button class="tab__link"&gt;parquet&lt;/button&gt;
&lt;/div&gt;
&lt;div class="tab__content" id="c6b1e010ed52c9f8"&gt;
&lt;h4 id="pandasconcat"&gt;pandas.concat&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;CPU Time - 0:02:43.797588
Peak Memory Usage - 4.1050 GB
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="tab__content" id="a412292f0cf68165"&gt;
&lt;h4 id="pandasconcat-with-a-doubly-queue"&gt;pandas.concat with a doubly queue&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;CPU Time - 0:02:34.484612
Peak Memory Usage - 3.7725 GB
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="tab__content" id="c655ff276a8106d7"&gt;
&lt;h4 id="concat-with-parquet"&gt;Concat with parquet&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;CPU Time - 0:01:37.984875
Peak Memory Usage - 1.6829 GB
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Looking at the results, parquet seems like a really good option to me. But we will run for a few more examples and later check which one suits the best.&lt;/p&gt;
&lt;h3 id="the-next-two-weeks"&gt;The next two weeks&lt;/h3&gt;
&lt;p&gt;The project is making progress in all fronts. I feel I need to reorganize my thoughts a bit. My main work for now would be to complete the task list of &lt;a href="https://github.com/radis/radis/pull/287"&gt;this pr&lt;/a&gt;. And then look at other stuff.&lt;/p&gt;
&lt;div class="footnotes"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="https://www.sciencedirect.com/science/article/pii/S0022407398000788?via%3Dihub"&gt;Rothmann Paper (Eqs.(A7), (A8), (A9)&lt;/a&gt; &lt;a class="footnote-backref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fnref:1"&gt;‚Ü©Ô∏é&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210620_0300_gagan-aryan/</guid><pubDate>Sun, 20 Jun 2021 02:00:06 GMT</pubDate></item><item><title>astropy@GSoC Blog Post #2, Week 1&amp;2</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210619_2154_suyog7130/</link><dc:creator>Suyog Garg</dc:creator><description>&lt;div&gt;&lt;p&gt;Hi,&lt;/p&gt;
&lt;p&gt;How are you?&lt;/p&gt;
&lt;p&gt;So, it's been two weeks of astropy@GSoC work already. Of course I have been damn busy! With the last commit made to the draft PR &lt;a href="https://github.com/astropy/astropy/pull/11835"&gt;https://github.com/astropy/astropy/pull/11835&lt;/a&gt;, a few hours back, I have successfully written a basic CDS writer. And voil√† it works, albeit without the ReadMe at present! üòÅ&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;Note that I am quite unlikely to go into technical details here in these posts. There are two reasons for this. Hhm..., Na I guess there's just one single reason. It would be too repetitive a task to write them. I already write aplenty about those in the GitHub comments and other communications. And of course, the whole codes I am writing during the project are available publicly on GitHub, for the overly curious kind. Moreover, the final report is gonna have more than ample discussion too, because I like to explain myself a lot. üòê What then is the need to write all that here again? So consider these posts my plain uncouth thoughts, which in any case, I suppose, aligns more with the spirit OpenAstronomy asks these for.&lt;/p&gt;
&lt;p&gt;On a second important point, honestly, these Astropy people are really intelligent. It would appear, even more as the project progresses, that they knowingly marked a normal project as &lt;i&gt;Easy&lt;/i&gt;¬†to lure some innocent students! üòÇ&lt;/p&gt;
&lt;p&gt;Anyway, Bye.&lt;/p&gt;
&lt;p&gt;See ya the next time! üôã‚Äç‚ôÇÔ∏è&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;&lt;/div&gt;</description><category>Astropy</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210619_2154_suyog7130/</guid><pubDate>Sat, 19 Jun 2021 20:54:00 GMT</pubDate></item><item><title>A Summer of Coding and Astronomy‚Ää‚Äî‚ÄäGSoC‚Äô21 at OpenAstronomy</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210607_1403_rachittshah/</link><dc:creator>Rachitt Shah</dc:creator><description>&lt;div&gt;&lt;h4&gt;A Summer of Coding and Astronomy‚Ää‚Äî‚ÄäGSoC‚Äô21 at OpenAstronomy&lt;/h4&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/400/0*5kDNG2NTio40EKZh.gif"&gt;&lt;/figure&gt;&lt;h4&gt;‚ÄúIn real open source, you have the right to control your own destiny.‚Äù&lt;/h4&gt;&lt;p&gt;‚Äî Linus¬†Torvalds&lt;/p&gt;
&lt;p&gt;Google Summer of Code is probably the most notable and interesting programs a student can be a part of an undergrad can be a part of. From reading GSoC blogs to write my story, it feels unnatural.&lt;/p&gt;
&lt;p&gt;This post is more of an introduction to GSoC and my project at OpenAstronomy, I‚Äôll be covering my prep and journey later¬†on.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;h4&gt;About my GSoC¬†project!&lt;/h4&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*VCNaN41xLMATNTgC"&gt;&lt;figcaption&gt;The DAVE¬†engine!&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;Generating periodograms for astronomical data is the core task of Stingray. Because periodograms are often noisy, several methods to denoise periodograms exist in the literature, among them the multi-taper periodogram Stingray aims to provide a comprehensive library of reliable, well-tested implementations of common algorithms for time series analysis in Astronomy. DAVE is an elegant GUI to the library, developed during a previous¬†GSoC.&lt;/li&gt;&lt;li&gt;Due to the fast-evolving Python and Javascript landscape, this GUI is not compatible with the current versions of the dependencies. Also, Stingray has now new features that were not implemented in the original¬†GUI.&lt;/li&gt;&lt;li&gt;In this project, I would be refreshing the GUI dependencies, update the package building infrastructure, and add the new functionality introduced in recent versions of Stingray.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;I would be making sure DAVE would be up and running again over the summer, couldn‚Äôt have been¬†happier!&lt;/p&gt;
&lt;h4&gt;About me&lt;/h4&gt;&lt;p&gt;I‚Äôm Rachitt Shah, a second-year undergrad at VIT Pune. I‚Äôm also doing Google Season of Docs with the STE||AR group for their project¬†HPX.&lt;/p&gt;
&lt;p&gt;I‚Äôm a growth associate at gradCapital, a student-centric VC fund that aims to help student startups.&lt;/p&gt;
&lt;p&gt;I love product management, tech(of course!) and venture capital.&lt;br&gt;Here‚Äôs my social profiles link¬†-&lt;/p&gt;
&lt;p&gt;&lt;a href="https://twitter.com/rachittshah"&gt;https://twitter.com/rachittshah&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/rachitt-shah/"&gt;https://www.linkedin.com/in/rachitt-shah/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you have any questions about open-source and dev, don‚Äôt hesitate to reach out. &lt;br&gt;May the source be with¬†you!&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=a7d716df482e" width="1"&gt;&lt;/div&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210607_1403_rachittshah/</guid><pubDate>Mon, 07 Jun 2021 13:03:09 GMT</pubDate></item></channel></rss>