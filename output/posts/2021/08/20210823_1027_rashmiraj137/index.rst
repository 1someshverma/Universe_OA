.. title: Google Summer of Code- Final Evaluation
.. slug:
.. date: 2021-08-23 10:27:57 
.. tags: stingray
.. author: Raj Rashmi
.. link: https://raj-rashmi741.medium.com/google-summer-of-code-final-evaluation-578c0088bcbd?source=rss-8f41b3524ac1------2
.. description:
.. category: gsoc2021


.. raw:: html

    <h3><strong>Topic: Implement JAX based automatic differentiation to Stingray</strong></h3><p>The project involved the study of modern statistical modelling to augment the accuracy, speed, and robustness of the likelihood function, into a software package called Stingray. This report demonstrates the experiment done for a combination of different optimizers to fit the scipy.optimize function. Another emphasis is to investigate the gradient calculation using JAX and compare it with scipy.optimize.</p>
    <h3><strong>Introduction:</strong></h3><p>The proposed milestone was to investigate the room for improvement to enhance the overall performance of modelling to Stingray, using JAX. However, the current stage of the model is still a sandbox model. Stingray is astrophysical spectral timing software, a library in python built to perform time series analysis and related tasks on astronomical light curves. JAX is a python library designed for high-performance numerical computing. Its API for numerical functions is based on NumPy, a collection of functions used in scientific computing. Both Python and NumPy are widely used and familiar, making JAX simple, flexible, and easy to adopt. It can differentiate through a large subset of python’s features, including loops, ifs, recursion, and closures, and it can even take derivatives of derivatives. Such modern differentiation packages deploy a broad range of computational techniques to improve applicability, run time, and memory management.</p>
    <p>JAX utilizes the grad function transformation to convert a function into a function that returns the original function’s gradient, just like Autograd. Beyond that, JAX offers a function transformation jit for just-in-time compilation of existing functions and vmap and pmap for vectorization and parallelization, respectively.</p>
    <!-- TEASER_END -->
    <h3>Experiment:</h3><p>The powerlaw and lorentzian function are the most used to describe periodograms in astronomy. In practice, we use the sum of these components to design a realistic model. For the analysis here we consider a quasi-periodic oscillation and a constant and try to fail the algorithm by, (i) reduce the amplitude, (ii) start the optimization process with parameters very far away from the true parameters, (iii) try different optimizers to experiment on different sensitive aspect of the current likelihood calculation. The current ongoing milestone is to try alternatives of scipy.optimize but this requires series of tests for the same.</p>
    <p>The above tests can be visualized in the notebook added on Github: <a href="https://github.com/rashmiraj137/GSoC-Project">https://github.com/rashmiraj137/GSoC-Project</a></p>
    <h3>Results:</h3><p>During the experiment, it was observed that the algorithm is sensitive to input parameters i.e., it fails for couple of combinations like when the amplitude is set far away from true value, precisely blow absolute value of 1. In general, if we set the parameters very far away from the true value, it fails to approximate the likelihood function. In the notebook (<a href="https://github.com/rashmiraj137/GSoC-Project/blob/main/GSoC_Evaluation%20Notebook.ipynb">link</a>), we demonstrate the room for improvement in the current algorithm by choosing a different set of parameters. The current data fit for the evaluation of likelihood happens using scipy.optimize.minimize function. However, there exists numerous ways to do this. SciPy optimize provides functions for minimizing (or maximizing) objective functions, possibly subject to constraints. It includes solvers for nonlinear problems (with support for both local and global optimization algorithms), linear programming, constrained and nonlinear least-squares, root finding, and curve fitting. The problem with the current minimization algorithm is that it converges at local minimum instead of global, i.e. it is not very robust. Recently, Machine Learning has evident development in such optimization tools. The strategy was to find alternatives that potentially accelerate the code, makes it robust.</p>
    <p>For this experiment case, we choose a couple of optimizers and compare the robustness to Powell (the current optimizer used). So we visualize the fit for couple of optimizers like :</p>
    <ul><li><a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-neldermead.html">minimize(method=’Nelder-Mead’)</a></li><li><a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-powell.html">minimize(method=’Powell’)</a></li><li><a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-cg.html">minimize(method=’CG’)</a></li><li><a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-bfgs.html">minimize(method=’BFGS’)</a></li><li><a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-newtoncg.html">minimize(method=’Newton-CG’)</a></li><li><a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html">minimize(method=’L-BFGS-B’)</a></li><li><a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-tnc.html">minimize(method=’TNC’)</a></li><li><a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-cobyla.html">minimize(method=’COBYLA’)</a></li><li><a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html">minimize(method=’SLSQP’)</a></li><li>minimize(method = “trust-constr”)</li></ul><p>This notebook (<a href="https://github.com/rashmiraj137/GSoC-Project/blob/main/GSoC_Evaluation%20Notebook.ipynb">link</a>) has results using each method and it was observed that Nelder-Mead is more robust as compared to other optimizers. Another optimizer like dogleg, trust-ncg, might be good as well, but the jacobian and hess need to be calculated for them.</p>
    <h3>Future Work:</h3><p>JAX has now its own version of scipy.optimize.minimize but it has couple of bugs and is not as robust as scipy.optimize.minimize. Finding an alternative for scipy.optimize.minimize such that it doesn’t fails even if the start parameters are far away from the true parameters was a goal for this project but unfortunately JAX did not assist that well enough. But there might be a superior algorithm to scipy.optimize.minimize that can be useful.</p>
    <h3>Repositories:</h3><p><a href="https://github.com/StingraySoftware/stingray">https://github.com/StingraySoftware/stingray</a></p>
    <p><a href="https://github.com/StingraySoftware/notebooks">GitHub - StingraySoftware/notebooks: Tutorial notebooks for Stingray</a></p>
    <h3>Blog Post:</h3><p>1. <a href="https://raj-rashmi741.medium.com/jax-based-automatic-differentiation-introduction-of-modern-statistical-modeling-to-stingray-1bc26da7571f">JAX-based automatic differentiation: Introduction of modern statistical modeling to Stingray.</a></p>
    <p>2. Insight of Implementation of JAX to stingray- GSoC coding period!</p>
    <p>3. <a href="https://raj-rashmi741.medium.com/gsoc-update-2d16a70cc267?source=your_stories_page-------------------------------------">GSoC update!</a></p>
    <p><a href="https://raj-rashmi741.medium.com/time-to-review-my-gsoc-project-c34297f2dc81">4. Time to review my GSoC Project.</a></p>
    <h3>Profiles:</h3><p>GitHub: <a href="https://github.com/rashmiraj137">https://github.com/rashmiraj137</a></p>
    <p>LinkedIn: <a href="https://www.linkedin.com/in/rashmi-raj-4b8a2b106/">https://www.linkedin.com/in/rashmi-raj-4b8a2b106/</a></p>
    <p>Medium: <a href="https://raj-rashmi741.medium.com/">https://raj-rashmi741.medium.com/</a></p>
    <img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=578c0088bcbd" width="1" />

