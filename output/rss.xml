<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Universe OpenAstronomy</title><link>http://openastronomy.org/Universe_OA/</link><description>This is an aggregator of openastronomy people</description><atom:link href="http://openastronomy.org/Universe_OA/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Tue, 23 Jun 2020 01:28:33 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>2 Weeks at DiRAC</title><link>http://openastronomy.org/Universe_OA/posts/2020/06/20200620_1723_techguybiswa/</link><dc:creator>Biswarup Banerjee</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;BiWeekly GSoC Updates Blog Post&lt;/strong&gt;&lt;/p&gt;
&lt;figure&gt;&lt;img alt="Data Intensive Research in Astronjomy and Astrophysics" src="https://cdn-images-1.medium.com/max/1024/1*Ek9a1DbEALyFlh8ueU56wQ.jpeg"&gt;&lt;figcaption&gt;Data-Intensive Research in Astronomy and Cosmology&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In all my previous internships/contract-work I have worked with only the web stack and that too on the web front end specifically.&lt;br&gt;
All through the past years, I have been part of creating large scale consumer-facing web apps mostly with Javascript and related frameworks like React, Vue, and Angular.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;But when I started working with DiRAC’s Data Engineering team, I had to work on a totally different tech stack!&lt;/p&gt;
&lt;p&gt;The Data Engineering team worked on Jupyter Lab and Jupyter Notebooks. They also worked mostly with Python/Go-related tech stack and frameworks and on very impressive DevOps stuff on AWS and also on their own EPYC servers.&lt;/p&gt;
&lt;p&gt;Although I had experience in Python mostly from solving LeetCode questions, but not a development experience with Python Frameworks. And I never heard of the Jupyter ecosystem before working at DiRAC. And the biggest DevOps stuff I have had ever done was hosting my projects on simple hosting services like Netlify.&lt;/p&gt;
&lt;blockquote&gt;So all these terms like Jupyter Notebook, Jupyter Lab, EPYC, Pyspark, Tornado, were totally alien to me.&lt;/blockquote&gt;&lt;p&gt;It was a new learning curve for me but definitely very interesting, intriguing, and challenging at the same time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Also and most importantly, my mentors were super supportive and they communicated everything to the minutest details and helped me with links of all the necessary documentation and explained the workflows over video calls.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Well, apart from tech stack, I came to know a lot of people from DiRAC as we interacted over the weekly data engineering meetings. &lt;br&gt;
I had interactions with Colin, Dino, Andy, Chris, and Brigitta who are working on the Data Engineering team.&lt;/p&gt;
&lt;p&gt;Regarding my GSoC project, we are trying to build an MVP and launch a minimal version by the end of next week.&lt;/p&gt;
&lt;img src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=9525ec15bc20" width="1" height="1"&gt;&lt;/div&gt;</description><category>astronomy-commons</category><guid>http://openastronomy.org/Universe_OA/posts/2020/06/20200620_1723_techguybiswa/</guid><pubDate>Sat, 20 Jun 2020 16:23:18 GMT</pubDate></item><item><title>Week 1 &amp; 2: Coding Officially Begins!</title><link>http://openastronomy.org/Universe_OA/posts/2020/06/20200615_1804_siddharthlal25/</link><dc:creator>siddharthlal25</dc:creator><description>&lt;div&gt;&lt;h3 id="hey-sid-did-the-coding-period-officially-begin"&gt;&lt;em&gt;Hey Sid, did the coding period officially begin?&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;The community bonding period ended by the end of last month and the coding period officially began, I started to work on basic structure of the package and setting up the (not so user-friendly, PS: from astronomer’s perspective) interface for the image reduction methods.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;h3 id="hey-what-did-you-build-in-these-two-weeks"&gt;&lt;em&gt;Hey, what did you build in these two weeks?&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;I have set up the basic methods required for processing of astronomical images, let me explain it to you one by one:&lt;/p&gt;

&lt;p&gt;The first method implemented was &lt;code class="language-plaintext highlighter-rouge"&gt;subtract_bias&lt;/code&gt;, this de-biases the image with the help of a bias frame, it has a mutating version as well which de-biases the image in place.&lt;/p&gt;

&lt;p&gt;Next comes &lt;code class="language-plaintext highlighter-rouge"&gt;subtract_overscan&lt;/code&gt;, every CCD plate has some region which is unexposed to light and this is called overscan region. For pre-processing average of this region has to be obtained (there are models as well, that can fit into this region PS: model part has to be implemented later) and then subtracted from the whole image. It also has a mutating variant clubbed together.&lt;/p&gt;

&lt;p&gt;Next in line was &lt;code class="language-plaintext highlighter-rouge"&gt;flat_correct&lt;/code&gt;, this method removes the effect of variations in pixel to pixel sensitivity of detectors and by distortions in the optical path. The interesting point I learned while implementing this is fused broadcasting, believe me Julia keeps on blowing my mind with its speed and succinct syntaxes.&lt;/p&gt;

&lt;p&gt;Next, I implemented some basic functionalities for modifying the image sizes i.e. &lt;code class="language-plaintext highlighter-rouge"&gt;trim&lt;/code&gt; and &lt;code class="language-plaintext highlighter-rouge"&gt;crop&lt;/code&gt;. They are not much different but they are different, let me explain! Trimming is instructing the computer to remove some parts from the image, whereas cropping is instructing the computer to keep a certain part in the image (Yes, that’s the difference!). Sound’s pretty similar, right? The implementations were not that similar, &lt;code class="language-plaintext highlighter-rouge"&gt;crop&lt;/code&gt; was a bit tricky as compared to &lt;code class="language-plaintext highlighter-rouge"&gt;trim&lt;/code&gt; (check out the source code to find the difference). These functions are inherently non-mutating type, but I have also implemented a version of &lt;code class="language-plaintext highlighter-rouge"&gt;crop&lt;/code&gt; as &lt;code class="language-plaintext highlighter-rouge"&gt;cropview&lt;/code&gt;, this returns the &lt;code class="language-plaintext highlighter-rouge"&gt;view&lt;/code&gt; of the passed array. Mutating the &lt;code class="language-plaintext highlighter-rouge"&gt;view&lt;/code&gt; will mutate the initial frame passed, an analogous version for &lt;code class="language-plaintext highlighter-rouge"&gt;trim&lt;/code&gt; here is &lt;code class="language-plaintext highlighter-rouge"&gt;trimview&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Next in line was the function &lt;code class="language-plaintext highlighter-rouge"&gt;combine&lt;/code&gt;, it basically takes a variable number of frames, stacks them together, and then finally combines them using medians (users can also have their custom combining functions).&lt;/p&gt;

&lt;p&gt;Okay, this one is last, &lt;code class="language-plaintext highlighter-rouge"&gt;subtract_dark&lt;/code&gt;, a way to reduce image noise in photographs shot with long exposure times, at high ISO sensor sensitivity, or at high temperatures. It takes advantage of the fact that two components of image noise, dark current and fixed-pattern noise, are the same from shot to shot. This function also has a mutating version clubbed along with it.&lt;/p&gt;

&lt;h3 id="hmmm-interesting-whats-next"&gt;&lt;em&gt;Hmmm, interesting… What’s next?&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Wait, it’s not yet over! I have also implemented these functions to interface directly with &lt;code class="language-plaintext highlighter-rouge"&gt;FITS&lt;/code&gt; files and &lt;code class="language-plaintext highlighter-rouge"&gt;ImageHDU&lt;/code&gt; (an element of the &lt;code class="language-plaintext highlighter-rouge"&gt;FITS&lt;/code&gt; files), putting it simply, a user can load the data (stored in &lt;code class="language-plaintext highlighter-rouge"&gt;FITS&lt;/code&gt; format) directly from the disk and then can play with all these functions!&lt;/p&gt;

&lt;h3 id="okay-cool-so-whats-next-do-you-still-have-something-in-the-pipeline"&gt;&lt;em&gt;Okay, cool! So what’s next? Do you still have something in the pipeline?&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Yes, the combine function still needs to be interfaced with &lt;code class="language-plaintext highlighter-rouge"&gt;FITS&lt;/code&gt; files, and once done, I will go for a release of the package. The next steps would be user-friendly processing pipelines, iterator reductions, and lot’s of documentation to be packed up together with the package.&lt;/p&gt;

&lt;p&gt;Stay tuned to know more!&lt;/p&gt;

&lt;p&gt;-sl&lt;/p&gt;&lt;/div&gt;</description><category>JuliaAstro</category><guid>http://openastronomy.org/Universe_OA/posts/2020/06/20200615_1804_siddharthlal25/</guid><pubDate>Mon, 15 Jun 2020 17:04:56 GMT</pubDate></item><item><title>GSOC 2020: The Coding period commences!</title><link>http://openastronomy.org/Universe_OA/posts/2020/06/20200615_0923_abhijeetmanhas/</link><dc:creator>Abhijeet Manhas</dc:creator><description>&lt;div&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*TYpSW6DKJusjkp7EMXIhyA.png"&gt;&lt;figcaption&gt;Sunrise in Gujarat, near Vadodara city&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;So we got started with the coding period, I had a couple of community meetings with my mentors and few full community meetings where I discussed what I was working upon and what was needed to be done.&lt;/p&gt;
&lt;p&gt;Major work in this fortnight was on refactoring&lt;strong&gt; &lt;/strong&gt;&lt;em&gt;Dataretriever Clients &lt;/em&gt;QueryResponse tables Pull request, &lt;a href="https://github.com/sunpy/sunpy/pull/4213"&gt;PR #&lt;strong&gt;4213&lt;/strong&gt;&lt;/a&gt;. This enabled the simple clients to show more metadata information like SatelliteNumber , Detector, Level, etc. in their response tables. All this information was extracted from the URL corresponding to the desired files using the parser.&lt;/p&gt;
&lt;p&gt;What did it change? Earlier the things looked like this:&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;iframe src="http://openastronomy.org/Universe_OA/posts/2020/06/20200615_0923_abhijeetmanhas/" width="0" height="0" frameborder="0" scrolling="no"&gt;&lt;a href="https://medium.com/media/7f467616b2076a3bc10e61d6de755ee3/href"&gt;https://medium.com/media/7f467616b2076a3bc10e61d6de755ee3/href&lt;/a&gt;&lt;/iframe&gt;&lt;p&gt;And now:&lt;/p&gt;
&lt;iframe src="http://openastronomy.org/Universe_OA/posts/2020/06/20200615_0923_abhijeetmanhas/" width="0" height="0" frameborder="0" scrolling="no"&gt;&lt;a href="https://medium.com/media/c9f17aab04a6eac7d1ddd745b2d2f0bf/href"&gt;https://medium.com/media/c9f17aab04a6eac7d1ddd745b2d2f0bf/href&lt;/a&gt;&lt;/iframe&gt;&lt;p&gt;Those np.nan Wavelength values annoyed me the most and now we not only be having the correct wavelengths, but other details too reflected in the response table. All the columns which were not relevant to the client were removed.&lt;/p&gt;
&lt;p&gt;Was that all about it? No. We need to find better ways of implementing the same feature. Earlier I used a _get_metadata_for_url method to extract all details from the URL, which was separately implemented for all clients. After getting the suggestions from my mentors, I implemented it in a better way; extracting all info in the scraper itself. After completing all tests, we discovered that there can be an even better way of doing this; by removing the client-specific _get_url_for_timerange() method itself! I used the registered attrs for achieving the same. All attrs were iterated to get the list of all possible directories, and then the only thing scraper has to do was pattern matching.&lt;/p&gt;
&lt;p&gt;The idea was to closely link scraper and GenericClient to have minimum client-specific code in their class implementations. I’ll push the changes as I complete all failing tests due to the change and add documentation for the API.&lt;/p&gt;
&lt;p&gt;All the 4 Ground Clients PRs were closed, after a discussion with the SunPy and VSO community. I updated my&lt;a href="https://github.com/sunpy/sunpy/pull/5055"&gt; Gong Synoptic Client Pull Request&lt;/a&gt; and got so far all reviews resolved. This would enable SunPy to access the Magnetogram Synoptic Map archives from NSO-GONG. Originally the issue was opened in &lt;a href="https://github.com/dstansby/pfsspy"&gt;pfsspy&lt;/a&gt;&lt;strong&gt;. &lt;/strong&gt;I also worked on a fix to the wrong goes Satellite Number issue in &lt;a href="https://github.com/sunpy/sunpy/pull/4288"&gt;PR #&lt;strong&gt;4288&lt;/strong&gt;&lt;/a&gt; recently. Using **kwargs in _get_overlap_urls method fixed the bug.&lt;/p&gt;
&lt;p&gt;There were other PRs too made and updated in this period which were merged before SunPy’s 2.0 release. I reduced the time for a goes_suvi client test from 8–10 secs to 1.5–2 secs on my system, in &lt;a href="https://github.com/sunpy/sunpy/pull/4131"&gt;PR #&lt;strong&gt;4099&lt;/strong&gt;&lt;/a&gt;. I had to explore why scraper took so much time for the test. Another one &lt;a href="https://github.com/sunpy/sunpy/pull/4132"&gt;PR #&lt;strong&gt;4132&lt;/strong&gt;&lt;/a&gt; was a way to prevent a future bug in scraper’s filelist method; so now it checks if the &lt;em&gt;&amp;lt;a href&amp;gt; &lt;/em&gt;in any webpage is None or not.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/sunpy/sunpy/pull/4011"&gt;PR #&lt;strong&gt;4011&lt;/strong&gt;&lt;/a&gt; was also updated which will restore the ability to post search filter the responses from VSO. I also went through the JSOC codebase and fido_factory.py to understand the complexities of implementation of&lt;em&gt; Fido post-search filter&lt;/em&gt; in SunPy. It is the next target in my Project. Just as a glimpse, this is how the VSO will look after post search filtering. I have added an extra concatenation routine by overloading + operator.&lt;/p&gt;
&lt;iframe src="http://openastronomy.org/Universe_OA/posts/2020/06/20200615_0923_abhijeetmanhas/" width="0" height="0" frameborder="0" scrolling="no"&gt;&lt;a href="https://medium.com/media/ef282104564e47da6f9bef13c237313e/href"&gt;https://medium.com/media/ef282104564e47da6f9bef13c237313e/href&lt;/a&gt;&lt;/iframe&gt;&lt;p&gt;I’m enjoying my summer now, wherever I face diffculties I talk with my mentors to get it resolved. I faced some issues in connectivity due to the thunderstorms out there in my city, but now everything is back to normal. The weather is pleasant now so I can engage more!&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*g_aP_bLgp1XI6ACBOrN69A.png"&gt;&lt;figcaption&gt;Before Thuderstorms in Vadodara!&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Looking forward to making more PRs in the next fortnight!&lt;/p&gt;
&lt;p&gt;CARPE NOCTEM!&lt;/p&gt;
&lt;img src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=b2eee33f274e" width="1" height="1"&gt;&lt;/div&gt;</description><category>SunPy</category><guid>http://openastronomy.org/Universe_OA/posts/2020/06/20200615_0923_abhijeetmanhas/</guid><pubDate>Mon, 15 Jun 2020 08:23:54 GMT</pubDate></item><item><title>Chapter 1: Apricity</title><link>http://openastronomy.org/Universe_OA/posts/2020/06/20200615_0026_raahul-singh/</link><dc:creator>Raahul Singh</dc:creator><description>&lt;div&gt;&lt;h5&gt;An endeavour to better understand our Sun’s choleric disposition&lt;/h5&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/781/1*tAMsUBRSvq7pUUyMNRAEuQ.jpeg"&gt;&lt;figcaption&gt;&lt;strong&gt;The Bhagirathi Massif&lt;/strong&gt;. The mountain is named after &lt;a href="https://en.wikipedia.org/wiki/Bhagiratha"&gt;Bhagiratha&lt;/a&gt;, the legendary king of the Ikshvaku dynasty who brought the River Ganges, to Earth from the heavens. It symbolizes the flow of divine knowledge, or the knowledge of liberation (Ganga), into human consciousness (earth) by the grace of God (Shiva) and the austere efforts of enlightened masters (Bhagiratha).&lt;/figcaption&gt;&lt;/figure&gt;&lt;blockquote&gt;&lt;strong&gt;ॐ भूर् भुवः स्वः।&lt;/strong&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;strong&gt;तत्सवितुर्वरेण्यं भर्गो॑ देवस्य धीमहि।&lt;/strong&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;strong&gt;धियो यो नः प्रचोदयात् ॥&lt;/strong&gt;&lt;/blockquote&gt;&lt;p&gt;There is some beauty in the fact that the essence of my undertaking needs not more than 24 letters of explanation.&lt;/p&gt;
&lt;p&gt;What I have written above in the &lt;a href="https://en.wikipedia.org/wiki/Devanagari"&gt;Devanagari&lt;/a&gt; script is one of the most important and highly revered Vedic hymns, the Gayatri Mantra. As translated by &lt;a href="https://en.wikipedia.org/wiki/S._Radhakrishnan"&gt;Dr S. Radhakrishnan,&lt;/a&gt; it states,&lt;/p&gt;
&lt;blockquote&gt;“We meditate on the effulgent glory of the divine Light; may he inspire our understanding.’’&lt;/blockquote&gt;&lt;p&gt;The goal of my project is to study solar flares. The effulgent glory, the flares, that the divine Light, our Sun, produces. I shall meditate on them over the summer and better my understanding and appreciation of the mechanisms that govern them.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;I shall also dare to predict them.&lt;/p&gt;
&lt;p&gt;And there you have it,&lt;br&gt;
&lt;strong&gt;Solar Weather Forecasting using linear algebra.&lt;/strong&gt;&lt;/p&gt;
&lt;h4&gt;&lt;em&gt;The Dataset&lt;/em&gt;&lt;/h4&gt;&lt;p&gt;When thinking about flares, you may imagine something like this :&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/800/1*YC4dFQkU03gCMsuZI36RfA.jpeg"&gt;&lt;figcaption&gt;On August 31, 2012, a long prominence/filament of solar material that had been hovering in the Sun’s atmosphere, the corona, erupted out into space at 4:36 p.m. EDT. Seen here from the &lt;a href="https://en.wikipedia.org/wiki/Solar_Dynamics_Observatory"&gt;Solar Dynamics Observatory&lt;/a&gt;, the flare caused auroras to be seen on Earth on September 3.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Flares indeed are mesmerizing. &lt;br&gt;
To analyze the properties of flares, we must understand what flares are.&lt;/p&gt;
&lt;p&gt;A &lt;a href="https://hesperia.gsfc.nasa.gov/sftheory/glossary.htm#FLARE"&gt;solar flare&lt;/a&gt; occurs when magnetic energy that has built up in the &lt;a href="https://hesperia.gsfc.nasa.gov/sftheory/glossary.htm#SOLAR_ATMOSPHERE"&gt;solar atmosphere&lt;/a&gt; is suddenly released. &lt;a href="https://hesperia.gsfc.nasa.gov/sftheory/glossary.htm#ELECTROMAGNETIC_RADIATION"&gt;Radiation&lt;/a&gt; is emitted across virtually the entire &lt;a href="https://hesperia.gsfc.nasa.gov/sftheory/glossary.htm#ELECTROMAGNETIC_SPECTRUM"&gt;electromagnetic spectrum&lt;/a&gt;, from radio waves at the long &lt;a href="https://hesperia.gsfc.nasa.gov/sftheory/glossary.htm#WAVELENGTH"&gt;wavelength&lt;/a&gt; end, through &lt;a href="https://hesperia.gsfc.nasa.gov/sftheory/glossary.htm#OPTICAL"&gt;optical&lt;/a&gt; emission to &lt;a href="https://hesperia.gsfc.nasa.gov/sftheory/glossary.htm#X_RAY"&gt;x-rays&lt;/a&gt; and &lt;a href="https://hesperia.gsfc.nasa.gov/sftheory/glossary.htm#GAMMA_RAY"&gt;gamma rays&lt;/a&gt; at the short wavelength end. The amount of energy released is the equivalent of millions of 100-&lt;a href="https://hesperia.gsfc.nasa.gov/sftheory/glossary.htm#MEGATON"&gt;megaton&lt;/a&gt; hydrogen bombs exploding at the same time!&lt;/p&gt;
&lt;p&gt;These flares emanate from active regions (ARs) in which high magnetic non-potentiality resides in a wide variety of forms.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Active regions&lt;/strong&gt; on the Sun are places where the Sun’s magnetic field is disturbed. These &lt;strong&gt;regions&lt;/strong&gt; frequently spawn various types of &lt;strong&gt;solar&lt;/strong&gt; &lt;strong&gt;activity&lt;/strong&gt;, including explosive “&lt;strong&gt;solar&lt;/strong&gt; &lt;strong&gt;storms&lt;/strong&gt;” such as &lt;strong&gt;solar&lt;/strong&gt; &lt;strong&gt;flares&lt;/strong&gt; and coronal mass ejections (&lt;strong&gt;CME&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;What you see below, is a full disk Line of Sight (LOS) Magnetogram, which is an image of the sun’s magnetic field in the line of sight of the observer.&lt;/p&gt;
&lt;p&gt;The data used in this project comes from &lt;a href="https://www.sunspotter.org/"&gt;Sunspotter,&lt;/a&gt; which is a dataset of such magnetograms with some measured properties for each Active Region.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*oegoLxwR316N4RUuqfKoVw.png"&gt;&lt;figcaption&gt;A full disk Magnetogram&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/360/1*MNeHF04MLtDXL_0U0D15uw.jpeg"&gt;&lt;figcaption&gt;An Active Region&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/360/1*zH_kdPqd0HymM1j94CRjMg.jpeg"&gt;&lt;figcaption&gt;Another Active Region&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;a href="https://www.sunspotter.org/"&gt;Sunspotter&lt;/a&gt; is a citizen science project that asked volunteers to classify solar active regions by their complexity — as it’s believed complexity has a direct relationship with their activity. With more than 25,000 volunteers and millions of classifications produced, we’ve got a &lt;a href="https://zenodo.org/record/1478972#.XI4YPqHgqr8"&gt;very nice dataset&lt;/a&gt;. The images used come from the &lt;a href="http://soi.stanford.edu/science/obs_prog.html"&gt;MDI instrument&lt;/a&gt;, which is onboard of &lt;a href="https://en.wikipedia.org/wiki/Solar_and_Heliospheric_Observatory"&gt;SOHO&lt;/a&gt; — the NASA-ESA mission that’s been observing the sun for more than two decades.&lt;/p&gt;
&lt;p&gt;All of the Active Region observations have a corresponding image, like the ones you see here.&lt;/p&gt;
&lt;p&gt;The dataset is composed of five files:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;strong&gt;lookup_timesfits.csv&lt;/strong&gt;: lists the filenames and the date of the data acquisition.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;lookup_properties.csv&lt;/strong&gt;: lists the properties of the active region observed in each frame to be classified.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;classifications.csv&lt;/strong&gt;: lists each classification made by the volunteers.&lt;/li&gt;&lt;li&gt;&lt;strong&gt;rankings.csv&lt;/strong&gt;: lists the final ranking on complexity.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The score provided on the rankings file follows the &lt;a href="https://en.wikipedia.org/wiki/Elo_rating_system"&gt;Elo rating system&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I have &lt;a href="https://github.com/Raahul-Singh/pythia/pull/26"&gt;recreated the ELO rating algorithm in python&lt;/a&gt;, to reassign complexity score to the Active Regions. This gives us better control over the range of values, which in turn can be tuned to match the sensitivity of the forecasting model.&lt;/p&gt;
&lt;h4&gt;Apollo’s chosen one&lt;/h4&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/345/1*OQvcPJAalRluEumfTTZwcA.jpeg"&gt;&lt;figcaption&gt;&lt;em&gt;Priestess of Delphi&lt;/em&gt; (1891) by &lt;a href="https://en.wikipedia.org/wiki/John_Collier_(Pre-Raphaelite_painter)"&gt;John Collier&lt;/a&gt;, showing the Pythia sitting on a tripod with vapour rising from a crack in the earth beneath her&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Apollo, the Greek god of the Sun, was also the god of prophecies. It is said that when Apollo wished to speak to the mortals, he would speak through his chief priestess, the Oracle of Delphi. She, who alone could understand Apollo’s whims and fancies. The High priestess, &lt;a href="https://en.wikipedia.org/wiki/Pythia"&gt;&lt;strong&gt;&lt;em&gt;Pythia&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;&lt;em&gt;.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In reverence, I have thus decided to name the repository that holds the code for this project, &lt;a href="https://github.com/Raahul-Singh/pythia"&gt;&lt;strong&gt;Pythia&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pythia&lt;/strong&gt;, &lt;strong&gt;for Solar Active Region Data Analysis.&lt;/strong&gt;&lt;br&gt;
Although we have just started, there is a lot you can do with Pythia already.&lt;/p&gt;
&lt;p&gt;To install Pythia, for now, run the following command:&lt;/p&gt;
&lt;iframe src="http://openastronomy.org/Universe_OA/posts/2020/06/20200615_0026_raahul-singh/" width="0" height="0" frameborder="0" scrolling="no"&gt;&lt;a href="https://medium.com/media/43055aa62917cd54a819f498174ce0f5/href"&gt;https://medium.com/media/43055aa62917cd54a819f498174ce0f5/href&lt;/a&gt;&lt;/iframe&gt;&lt;p&gt;Some of the functionalities that Pythia offers, as of writing this post are:&lt;/p&gt;
&lt;p&gt;Using Pythia, you can get the measured properties of any AR in the Sunspotter dataset. This is done using &lt;a href="https://docs.sunpy.org/en/stable/guide/acquiring_data/hek.html"&gt;SunPy’s HEK &lt;/a&gt;module. This is the function description.&lt;/p&gt;
&lt;iframe src="http://openastronomy.org/Universe_OA/posts/2020/06/20200615_0026_raahul-singh/" width="0" height="0" frameborder="0" scrolling="no"&gt;&lt;a href="https://medium.com/media/339c9dc8054c807970ce6a1823d0e284/href"&gt;https://medium.com/media/339c9dc8054c807970ce6a1823d0e284/href&lt;/a&gt;&lt;/iframe&gt;&lt;p&gt;You can also download the full disk MDI magnetograms. This uses &lt;a href="https://docs.sunpy.org/en/stable/guide/acquiring_data/fido.html"&gt;SunPy’s FIDO&lt;/a&gt; module to get the magnetogram as a FITS file. The following function returns an &lt;a href="https://docs.sunpy.org/en/stable/guide/data_types/maps.html"&gt;MDI map&lt;/a&gt; for a given observation date. Should the observation date not be in the Sunspotter dataset CSV files currently loaded in the Sunspotter object, the observation date nearest to the given observation date is used.&lt;/p&gt;
&lt;iframe src="http://openastronomy.org/Universe_OA/posts/2020/06/20200615_0026_raahul-singh/" width="0" height="0" frameborder="0" scrolling="no"&gt;&lt;a href="https://medium.com/media/c19b389e053b71b99ada4301c317e5b1/href"&gt;https://medium.com/media/c19b389e053b71b99ada4301c317e5b1/href&lt;/a&gt;&lt;/iframe&gt;&lt;p&gt;Should you wish for all the maps in a given range, you can use:&lt;/p&gt;
&lt;iframe src="http://openastronomy.org/Universe_OA/posts/2020/06/20200615_0026_raahul-singh/" width="0" height="0" frameborder="0" scrolling="no"&gt;&lt;a href="https://medium.com/media/501b65fb40960a2b1af47fbaa1d36962/href"&gt;https://medium.com/media/501b65fb40960a2b1af47fbaa1d36962/href&lt;/a&gt;&lt;/iframe&gt;&lt;p&gt;Finally, if you wish to plot all the ARs on a full disk magnetogram for which we have data, for any observation date,&lt;/p&gt;
&lt;iframe src="http://openastronomy.org/Universe_OA/posts/2020/06/20200615_0026_raahul-singh/" width="0" height="0" frameborder="0" scrolling="no"&gt;&lt;a href="https://medium.com/media/83603bbd1bd1805d5c0885e08143e66b/href"&gt;https://medium.com/media/83603bbd1bd1805d5c0885e08143e66b/href&lt;/a&gt;&lt;/iframe&gt;&lt;p&gt;Which, when used for observation date &lt;strong&gt;2002–03–18 09:39:01 &lt;/strong&gt;gets you the following plot. I have magnified it to highlight the fact that Active Regions come in all shapes and sizes.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*TyCM1QtLG2yYXXiToq6eZQ.png"&gt;&lt;figcaption&gt;ARs Plotted on an MDI map&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;What I have given is a brief introduction of the project, along with some code examples. Pythia is in active development and there are modules whichI have not mentioned here. I encourage my readers to install Pythia and play around with the code!&lt;/p&gt;
&lt;p&gt;If you find any bugs or would like me to add any features, feel free to &lt;a href="https://github.com/Raahul-Singh/pythia/issues"&gt;open an issue on the main repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My next post will be all about the Exploratory Data Analysis of the Sunspotter data, where we shall delve deep into the Sunspotter dataset.&lt;/p&gt;
&lt;p&gt;Till then,&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Divina Luceit Vos!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;:)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;img src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=aef3bd172dab" width="1" height="1"&gt;&lt;/div&gt;</description><category>SunPy</category><guid>http://openastronomy.org/Universe_OA/posts/2020/06/20200615_0026_raahul-singh/</guid><pubDate>Sun, 14 Jun 2020 23:26:29 GMT</pubDate></item><item><title>GSoC 2020: glue-solar project 1.1</title><link>http://openastronomy.org/Universe_OA/posts/2020/06/20200614_0634_kakirastern/</link><dc:creator>Kris Stern</dc:creator><description>&lt;div&gt;&lt;p&gt;The official coding period of GSoC 2020 has begun on June 2nd (HKT) and the glue-solar work is currently underway according to plan as discussed with mentors &lt;strong&gt;Stuart Mumford&lt;em&gt; &lt;/em&gt;&lt;/strong&gt;and&lt;strong&gt;&lt;em&gt; &lt;/em&gt;Nabil Freij &lt;/strong&gt;during the community bonding period that precedes the coding period. The tasks proposed that we would like to see implemented this summer include but not limited to the following:&lt;/p&gt;
&lt;p&gt;1. Modify the existing glue 1D Profile viewer to provide sliders for extra dimensions (currently collapses)&lt;br&gt;
2. Clean up UX / UI (icon) for the pixel extraction tool (perhaps also upstreaming from glue-solar to glue)&lt;br&gt;
&lt;!-- TEASER_END --&gt;
3. WCS info for derived datasets (i.e. raster + SJI time linking)&lt;br&gt;
4. Loaders for specific instruments: NDData (in glue), NDCube (for NDCube 2.0 with extra coords), SST, IRIS, EIS, DKIST (e.g. with asdf to glue) &lt;br&gt;
5. Add auto-linking to match wavelength and time dimensions in larger cubes (currently already works for Celestial dimensions)&lt;br&gt;
6. Enable image / Movie exports, both with axes and without axes via the matplotlib package&lt;br&gt;
7. Add support for pre-computed statistics in datasets / viewers&lt;/p&gt;
&lt;p&gt;The above, along with the accompanying documentation, are expected to be part of the deliverables of the GSoC project. So far a couple of pull requests (PRs) have been submitted to both the glue and glue-solar repositories (repos), with the most recent one being the one for updating the 1D Profile viewer to plot with wcsaxes instead, which can be accessed at glue-viz/glue's &lt;a href="https://github.com/glue-viz/glue/pull/2156"&gt;PR #2156&lt;/a&gt;. However, the CI problem encountered in this PR seems to be able to be fixed by upstreaming the pixel extraction tool which currently only resides in glue-solar. This will need to be discussed with my mentors as careful considerations such as one pertaining to timing are warranted.&lt;/p&gt;
&lt;p&gt;One of the first glue-solar PRs I have worked on are for adding an “open with SunPy Map” option to the “Import Data” tool. Part of this tool has been merged into glue-solar, while the other part is pending merge in glue. Basically if I load in an AIA map with its associated HMI map, like the sample one from SunPy, while choosing “SunPy Map” as opposed to the FITS format that gets detected by default, as shown in the Fig. 1:&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*f1TZWTVe_Z7YqSwvPEGHXg.png"&gt;&lt;figcaption&gt;Fig. 1. Choosing “SunPy Map” to visualize SunPy maps with Glue.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;So if one plots the SunPy maps individually, one would get the what is shown in Fig. 2. This is basically the same as what one would get with FITS files as well, except for the colormaps.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-xqO_mMCzSNAAZAr6ipeJA.png"&gt;&lt;figcaption&gt;Fig. 2. Plotting “SunPy Map” objects individually&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The magic of our work is that now with the glue-solar plugin, we can overplot AIA map and its associated HMI map, as simply as dragging the HMI data from the Data section onto the AIA map 2D image already opened with the 2D Image viewer, as these are spatially linked by the plugin using some pixel-to-pixel transformation via WCS.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*q0SWRNVOImWlLmpQ2k6_hQ.png"&gt;&lt;figcaption&gt;Fig. 3. Overplotting HMI data with AIA data with colormaps&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;So the subject of attention that has been the focus of my glue-solar work, or my favorite “toy”, over the last few weeks has been firstly a prototype for a “SunPy 1D Profile” tool which plots a 1D spectrum at a spatial position as picked out by the pixel extraction tool currently resides within glue-solar. To upstream the changes I have made to glue, and with the help of my mentors especially Stuart, we have started work on a PR to move such changes to the “1D Profile” in glue proper, using my “SunPy 1D Profile” work as a check and basis. If one loads in a raster cube using some IRIS level 2 data using the IRIS Spectrograph file type enabled by glue-solar, plots a slice of the data cube with the “2D Image” viewer, and then extracts a pixel position in the 2D image in the viewer (with the “HPLN” as the x-axis in lieu of the “wavelength” as the default option), one obtains a 1D profile upon hitting the spectrum icon in the 2D Image viewer panel. The result is as depicted in the collapsing “Fig. 4” image below:&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*WZdvkWlnqECjS66SIM9zbQ.png"&gt;&lt;figcaption&gt;Fig. 4. Plotting a 1D spectrum with wavelength as the x-axis and “Maximum” as the default function option.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;After which if one changes the “function” in the “Plot Options — 1D Profile” from the “maximum” to the newly added “slice” option in my PR, one would obtain a 1D spectrum without any collapsing as shown in the “Fig. 5” image below:&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*8ha0cEHSzxaY1zX-iSgMLw.png"&gt;&lt;figcaption&gt;Fig. 5. Visualizing a 1D spectrum at a pixel coordinate position after switching to the `Slice` function option that is non-collapsing.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This non-collapsing 1D spectrum plotting facility is new to glue with the glue-solar plugin, and is currently still under heavy development.&lt;/p&gt;
&lt;p&gt;Overall it has been a great experience this second time around participating in GSoC at this early phase. The experience is especially positive given that I am engaged in some work that I could understand and really like. Previously I have been exposed to data cubes in my PhD studies in astrophysics, with the knowledge gained now comes in handy for my glue and glue-solar work. I look forward to contributing as much as possible to the project in the months to come during GSoC, and possibly beyond.&lt;/p&gt;
&lt;img src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=c2151e535e0c" width="1" height="1"&gt;&lt;/div&gt;</description><category>SunPy</category><guid>http://openastronomy.org/Universe_OA/posts/2020/06/20200614_0634_kakirastern/</guid><pubDate>Sun, 14 Jun 2020 05:34:50 GMT</pubDate></item><item><title>Google Summer of Code - Blog #1!</title><link>http://openastronomy.org/Universe_OA/posts/2020/06/20200613_2135_pkj-m/</link><dc:creator>pkj-m</dc:creator><description>&lt;div&gt;&lt;p&gt;Hey there, welcome to the second blog of the series, and the first one to document the coding period. The Community bonding period which I described in my previous blog ended on 31st May and paved the way for the official coding period of the Google Summer of Code. These past two weeks were my first where I spent most of my time working on the actual code that will be a part of my project. My primary objective over these two weeks was to study the proof of work code that implements the spectral matrix algorithm to compute the spectra and execute it on a GPU. This was followed by a period of studying the different mechanisms with which RADIS calculates the spectras, and to understand the differences between each of them. This was important as implementing GPU compatible methods for all these distinct pipelines is my final objective and it is essential for me to understand the differences between these methods at the very onset of my project. Finally, the remaining time was spent on back and forth discussions with my mentors on various languages and libraries that could have been possible choices for undertaking this project. Once we had made our decision, I spent the time going through the library’s documentation, source code and tutorials to familiarize myself with these tools.&lt;/p&gt;

&lt;p&gt;The first major objective for these two weeks focussed on studying and executing the proof of work code. This was a single CUDA C file which demonstrated the idea of using a spectral matrix to compute a spectra while making use of a GPU could offer performance boosts of multiple orders over the naive methods. I initially planned on executing the code and running it on my personal computer, but the idea was quickly dismissed because of reasons I already discussed in my previous blog. As a result, I ended up using Google Colab for this experimentation which came with its own fair share of discomforts. The first, and most significant of which, was the lack of persistent storage on Colab and thus being forced to resort to Google Drive for saving our database instead. This was costly in terms of both, the time it took to store the data on the cloud and also on the overall performance of the code as the time taken to load the data to memory increased significantly compared to a single CPU-GPU system like my personal laptop. This however, was not detrimental to the fundamental objective as the benchmarking could be done for each part of the code separately, and thus it did not influence or affect the execution of the device code or its perfomance in any way. Another task which popped up when using Colab to run CUDA was to setup the system so it could run native CUDA C files along with the Python code as well. This fortunately was not very difficult to solve and a couple of google searches gave us the list of all the necessary packages we needed to compile and execute C files on Colab. Once that was set up, the only thing that was left for me to do was transfer the data from my laptop to Google Drive. This once again posed a problem that I had not anticipated. Uploading 8GB of data takes &lt;em&gt;much&lt;/em&gt; longer than downloading the same amount of data! As soon as that realization hit me, I decided to adopt another approach. I copied the code that I used to download the data from the FTP server to my local storage and ran it on Google Colab! This allowed me to once again redownload the entire data (which in the raw format was ~ 30GB) directly on my Drive instead. The process was much faster than I had anticipated and I soon had the raw data on my Drive. After running another couple of scripts to format and repartition the data into separate numpy arrays, I was ready to go. Execution of the code went smoothly except for a few hiccups surrounding the matplotlibcpp library that was being used to plot the output spectra. I wasn’t able to solve this problem immediately like the others and talked to my mentors about it. They advised me to not worry too much about it right now as it really wasn’t the critical part of the project. The major part, the kernel that was supposed to run on the GPU ran as expected and the results we obtained by timing the kernel performance were very positive! Now that I had successfully executed the code, what followed was a series of different runs of the same code, only this time with a different aim to test how far we could take this GPU compatible code. To give some numbers here, the original proof of work code that crunched the 8GB processed database computed a total of 240 million lines in less than a second! To be more specific, it took 120 ms on average to achieve that number. To put that into perspective, a naive implementation of the same code, that does not make use of the optimizations we did here, would take 10,000x longer to produce the same results! That in itself makes the naive approach an impractical solution to the problem. Compared to the current RADIS implementation, the performance gain was still significant with upto 50x gain in terms of time spent for computing the spectra. In order to see how far we could take this code, we also tried it with it a bunch of different ranges from the same dataset. While the original code was tested on a range that spanned from 1750 to 2400 cm-1 wavenumber, we took it as far as 1250-3050 cm-1. Surprisingly, the code scaled pretty well with the increase in the number of lines being computed, going from the original 120 ms taken to compute 240M lines to ~ 220 ms to compute 330M lines. Testing such a wide range and getting such positive results was sufficient proof for us to pack up the analysis part and move on to the actual implementation.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;p&gt;In order to integrate the GPU compatible spectral matrix method with the RADIS code base, the first thing that needed to be worked on was the language itself. The proof of work had been written completely in CUDA C, while RADIS is pure-Python. In order to bridge this gap, we had multiple options. The first and most obvious was to simply rewrite the entire code using Python with the help of some CUDA library. This, however, meant a lot of work in re-implementing the multiple methods, and more importantly, did not allow us to reuse the code that already existed. Therefore, in order to maximize our efficiency and also get the best performance possible, we decided to use a new language, or more specifically – a language extension for Python, known as Cython. The idea behind Cython is to use Python with a static compiler, which allowed Python programs to be precompiled into binaries, which could then be imported to other Python programs and achieve performance on par with native C code, because that is the intermediary code Cython converts the Python code into! Thus, by extension, any code that was already written in C was directly compatible with Cython. The main task now was to get the C code we had with us to talk to Cython with as few modifications as possible. This infact, is something that is still ongoing and would be finished as a part of my first evaluation. The last few days of this period have mostly been spent on learning Cython and its nuances. While the idea of Cython is to provide a smooth experience for Python users to gain C-level performance, ironically I had the opposite experience with it. I found Cython quite confusing at the beginning, and while most resources and tutorials focussed on making Python code achieve C-level performance, I was genuinely surprised by the lack of documentation/tutorials explaining how to export C code that already exists to Python. The few examples that were mentioned on the website were very generic and did not help much in terms of my requirements, where I needed to use things like references of vectors, etc. However, with more research and googling, I was able to find a compromise solution that worked well and thus allowed me to execute the method in Cython with minimal modifications to the original code.&lt;/p&gt;

&lt;p&gt;Overall, I think its been a good two weeks with a lot of progress made on the knowledge front. Apart from the objectives mentioned above, I also went through the draft of the paper my mentors have been working on which goes into the mathematics of the method and explains how it works. While I wasn’t able to comprehend everything properly, it did give me a good high-level idea of what exactly we’re trying to accomplish with our kernels. With this, I think I’d like to conclude this blog. Over the next two weeks, the end of which will also mark the completion of my first evaluation, I’ll continue to work on Cython-izing our host code, and start looking into CuPy as an alternative to CUDA C for our project! More about that in the next blog! Thanks!&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2020/06/20200613_2135_pkj-m/</guid><pubDate>Sat, 13 Jun 2020 20:35:56 GMT</pubDate></item><item><title>GSoC 2020: Blog 1 - Beginning of Coding Period</title><link>http://openastronomy.org/Universe_OA/posts/2020/06/20200613_1820_jes24/</link><dc:creator>Jyotirmaya Shivottam</dc:creator><description>&lt;div&gt;&lt;p&gt;So the community bonding period of GSoC has ended and the coding period is officially underway. In my last blogpost, I had outlined the basic principles of General Relativity that go into my project. I had also mentioned, that the next blog will have details about the coding process. However, things had to be slowed down considerably, due to the announcement of closure of my academic session and some logistical issues. This has also affected my blog schedule, as I could not work on a blog, that was supposed to be up 2 weeks ago. However, I am pleased to inform that all the issues are sorted out now, leaving the rest of the summer free for me to delve into the project. Whew. I am also exceedingly grateful to my mentors, who have been understanding throughout. As for details on the code implementation for my project, I have decided to break it up across the blogs, as "Progress Reports" (bland, I know), in order to provide a better understanding of both what I am working on and my approach to it. So, read on.&lt;/p&gt;

&lt;h3&gt;
&lt;!-- TEASER_END --&gt;
&lt;a href="http://openastronomy.org/Universe_OA/posts/2020/06/20200613_1820_jes24/#progress-so-far" class="anchor"&gt;
&lt;/a&gt;
Progress so far...
&lt;/h3&gt;

&lt;p&gt;Over the last few discussions with my GSoC mentors, we have discovered some logical bottlenecks in some of the EinsteinPy modules, especially the way the &lt;code&gt;metric&lt;/code&gt; &amp;amp; &lt;code&gt;utils&lt;/code&gt; modules work at the moment. Currently, the &lt;code&gt;utils&lt;/code&gt; module stores most of the necessary functions required to form a &lt;code&gt;metric&lt;/code&gt; class, while the &lt;code&gt;metric&lt;/code&gt; module lacks support for user-defined metrics. It also handles the calculation of particle trajectory, which logically belongs inside a &lt;code&gt;geodesic&lt;/code&gt; module. Since my project is on Null Geodesics, these issues are major obstacles, that must be overcome, before the work on adding support for Null Geodesic calculation begins.&lt;/p&gt;

&lt;p&gt;As such, I am refactoring these modules, so that we have logical cohesion across EinsteinPy, whilst also adding some new features, like a brand new &lt;code&gt;metric&lt;/code&gt; class, that supports defining arbitrary metrics and also adding first order linear perturbations to the metric, written in Kerr-Schild form. I have also grouped together the utility functions present in &lt;code&gt;utils&lt;/code&gt; in &lt;code&gt;metric&lt;/code&gt; itself. These changes can be followed at the PR link, &lt;a href="https://github.com/einsteinpy/einsteinpy/pull/512"&gt;here&lt;/a&gt;. At the moment, I am reusing some of the old tests and writing some new ones for the new features. I hope to see this PR clear all tests and be merged soon.&lt;/p&gt;

&lt;h3&gt;
&lt;a href="http://openastronomy.org/Universe_OA/posts/2020/06/20200613_1820_jes24/#until-next-time" class="anchor"&gt;
&lt;/a&gt;
Until next time...
&lt;/h3&gt;

&lt;p&gt;After &lt;code&gt;metric&lt;/code&gt;, comes the &lt;code&gt;coordinates&lt;/code&gt; module, which will see some changes and code rearrangements too, albeit not as much as &lt;code&gt;metric&lt;/code&gt; and &lt;code&gt;utils&lt;/code&gt;. The basic work on this has already started and after the first PR is merged, I will open another PR with these changes.&lt;/p&gt;

&lt;p&gt;We have lost around 2 weeks, due to the aforementioned issues on my side. Therefore, it is contingent on me to accelerate the pace of development now. Over the next few weeks, we should see some cool new feature additions to EinsteinPy. I will be detailing them here, as we proceed with the project.&lt;/p&gt;&lt;/div&gt;</description><category>EinsteinPy</category><guid>http://openastronomy.org/Universe_OA/posts/2020/06/20200613_1820_jes24/</guid><pubDate>Sat, 13 Jun 2020 17:20:19 GMT</pubDate></item><item><title>Week 1 &amp; 2: Tip-Off</title><link>http://openastronomy.org/Universe_OA/posts/2020/06/20200613_0950_sahilyadav27/</link><dc:creator>Sahil Yadav</dc:creator><description>&lt;div&gt;&lt;p&gt;The first week started off with having a video call with the mentors and trying to pave the path ahead for the next 3 months. There were 2 options: Either convert ROOT files to the hdf5 format from CTA or establish a new DL1 reader for ROOT. I decided to run some tests and get an idea about the speed and memory requirements for both the methods. We were more inclined to create a new class for the ROOT files so that we don’t have to save a separate hdf5 file for each ROOT file when using CTLearn. The reading times for both the file types were also similar, so we decided to implement a new child class for ROOT files.&lt;/p&gt;
&lt;p&gt;In order to move ahead with this plan, I first wrote down the entire DL1DataWriter code, highlighting the hdf5 dependent parts. This way, I was able to get a better understanding of the code and its intricacies. After talking with Tjark some more, we decided to implement 2 child classes for hdf5 and ROOT which inherited the parent Writer class.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/251/1*JDweXFtW-o-ZqcIbIfzkoQ.png"&gt;&lt;/figure&gt;&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/251/1*BfDNzmeG5NYEQn8xTOGZrg.png"&gt;&lt;figcaption&gt;MC simulated images from MAGIC Cam 1 &amp;amp; 2, an event which only triggered one telescope and not the other.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;After another video call, Ari suggested that I convert the ROOT files into an hdf5 file with the CTA ML Data format to understand the differences between the formats of MAGIC and CTA. Although there is already a library ctapipe_io_magic to produce the MAGICEventSource and use it for DL1DataReader and produce an hdf5 file, there were a lot of issues with trying to use it.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;So we decided to implement our own code to convert the file. This way, any ROOT file can be converted to hdf5 and used by CTLearn before the actual project is complete and in place. During week 2, I worked on writing code for the same. It is mostly complete and a few things need to be ironed out which will be done in the next call on 15th.&lt;/p&gt;
&lt;p&gt;Once this is done, I’ll start to work on creating separate child classes for ROOT and hdf5 inheriting from DL1DataWriter as discussed above.&lt;/p&gt;
&lt;img src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=1eb6744a41ea" width="1" height="1"&gt;&lt;/div&gt;</description><category>CTLearn</category><guid>http://openastronomy.org/Universe_OA/posts/2020/06/20200613_0950_sahilyadav27/</guid><pubDate>Sat, 13 Jun 2020 08:50:12 GMT</pubDate></item><item><title>Community Bonding Period-GSoC20</title><link>http://openastronomy.org/Universe_OA/posts/2020/05/20200530_1804_siddharthlal25/</link><dc:creator>siddharthlal25</dc:creator><description>&lt;div&gt;&lt;p&gt;It was around 11:30 in the night on 4th of May, the Google servers glitched for a second or so but then I could see my project on the Org’s list.&lt;/p&gt;

&lt;p&gt;Finally, I got selected for GSoC 2020 after around two months of involvement with the JuliaAstro community. Before my selection, I had primarily contributed to &lt;a href="https://github.com/JuliaAstro/Photometry.jl"&gt;Photometry.jl&lt;/a&gt; and &lt;a href="https://github.com/JuliaAstro/DustExtinction.jl"&gt;DustExtinction.jl&lt;/a&gt;, both were a part of the JuliaAstro community!&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;p&gt;The next step was the community bonding phase, in this phase, I got my development setup more robust (I updated my OS, updated Atom, and also Juno). I also sharpened my Julia skills and learned the art of succinct documentation in this period. I went through the codebase of the repositories available in this community to get a flavor of Julia and it’s usage for making things work!&lt;/p&gt;

&lt;p&gt;Since I was under lockdown back in my hometown, I got quite bored and decided to start contributing towards the project. I set up the basic framework of the package and added a few basic skeletal functions after discussions about it with the mentors.&lt;/p&gt;

&lt;p&gt;At last, I would like to thank Miles, Mosè, and Yash for their guidance and prompt responses to all my dumb queries, it wouldn’t have been possible for me to start contributing without their guidance and help. A big shout-out to you guys! Thank you so much!&lt;/p&gt;

&lt;p&gt;Oh! I forget to mention my project, this summers I will be developing a package for reduction of astronomical images in Julia.&lt;/p&gt;

&lt;p&gt;Stay tuned for my upcoming blogs to know more about the project!&lt;/p&gt;

&lt;p&gt;-sl&lt;/p&gt;&lt;/div&gt;</description><category>JuliaAstro</category><guid>http://openastronomy.org/Universe_OA/posts/2020/05/20200530_1804_siddharthlal25/</guid><pubDate>Sat, 30 May 2020 17:04:56 GMT</pubDate></item><item><title>Google Summer of Code - Blog #0!</title><link>http://openastronomy.org/Universe_OA/posts/2020/05/20200530_1804_pkj-m/</link><dc:creator>pkj-m</dc:creator><description>&lt;div&gt;&lt;p&gt;Hey there. Welcome to the first of what is going to be a series of blog posts chronicling my journey as I participate in the Google Summer of Code this year with RADIS (registered as a sub-org under OpenAstronomy). This particular blog post, as the title suggests, is meant to give a quick introduction to GSoC as well as my organization and the project.&lt;/p&gt;

&lt;h4 id="what-is-google-summer-of-code"&gt;What is Google Summer of Code?&lt;/h4&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;Borrowing straight from it’s landing page, Google Summer of Code is a global program focused on bringing more student developers into open source software development. Students work with an open source organization on a 3 month programming project during their break from school.
In other words, GSoC (as it is more commonly referred to) is a program sponsored by Google which aims to connect university students around the world with open source organizations in order to promote the open-source culture. The students get an opportunity to peek into the world of open source development, learn new skills and also get compensated for the work, quite generously. In turn, the organizations benefit from a few extra pairs of helping hands, using them for a wide array of issues, from refactoring code, to fixing existing bugs, and ofcourse, to add new features to the existing code base. It’s a great program and any college student interested in software development should definitely check it out. The website contains a lot more information &lt;a href="https://summerofcode.withgoogle.com/"&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now that’s clear, let me talk a little about the specific organization that I will be working with.&lt;/p&gt;

&lt;h4 id="radis--openastronomy"&gt;RADIS ∈ OpenAstronomy&lt;/h4&gt;

&lt;p&gt;While the most commmon way for organizations to participate in the program and request slots for GSoC is to register directly, many organizations, for various reasons, often come together under an umbrella organization and register for GSoC as one single unit. This bundling of organizations can happen on various basis, but the most common reason is that these organizations often work towards the same goal, or operate in the same domain. In my case, this is true as well. I applied for and got selected for GSoC with the OpenAstronomy organization, which as the name suggests is an umbrella organization meant to act as a central hub for all the great number of ‘sub-organizations’ that operate within it. You can read more about OpenAstronomy and what it does &lt;a href="https://openastronomy.org/"&gt;here&lt;/a&gt;. One such sub-organization, with which I shall be working, happens to be RADIS.&lt;/p&gt;

&lt;p&gt;RADIS is a radiation software, a fast line-by-line code for synthesizing and fitting infrared absorption and emission spectra such as encountered in laboratory plasmas or exoplanet atmospheres.&lt;/p&gt;

&lt;p&gt;While RADIS was written with performance in mind, and performance it delivers, it is still limited by factors such as the speed offered by Python, and using a single CPU to carry out all its calculations. While the problem with a programming language can be solved (not so) easily by switching to other languages, such as C++, we’re still limited to using a single processing unit to carry out all the computations. This is the exact problem that my project tries to solve. The title of my project, “Accelerate Synthetic Spectra Calculations using CUDA”, is all about removing the restriction of a single CPU and instead allow RADIS to perform much faster by utilising the power of a GPU instead. I will not be going into the detail of the project here/yet, but the general idea is to switch certain parts of the pipeline in RADIS’ execution which slow it down to the GPU, which as people familiar with GPUs would know, is adept at executing a large volume of simple tasks.&lt;/p&gt;

&lt;p&gt;The technicalities of the project, including how exactly the problem can be shifted to the GPU, how its being implemented on the GPU, or its integration with the RADIS will be discussed in the future blog posts. For now, I will like to keep this blog limited to the things that have already occured, as a part of the ‘Community Bonding Period’:&lt;/p&gt;

&lt;h4 id="the-community-bonding-period"&gt;The Community Bonding Period&lt;/h4&gt;

&lt;p&gt;The Community Bonding Period is an almost 30-days long period meant to serve as a warm-up or a buffer before the actual coding period begins. It can be used for a wide variety of purposes, such as getting a better understanding of the codebase, figuring out the intricacies of your project et al. As such, the greater part of my community bonding period went into understanding the exact details of what I will be doing over the coming months and how I will be doing it. While the pre-selection period did include a fair amount of contributions being made by me towards the organization, it was mainly an attempt to understand the general architecture and codebase of RADIS more thoroughly, and did not involve anything specific to the project I had applied for. Once the selected projects were made public on 4th May 2020, that is when the community bonding period officially started and I started to focus exclusively on my project as well. The seed idea that eventually led to my project started when my mentors decided to play around with the code, and instead of using pure Python for the processing, decided to precompile some of the slower parts of the code into a DLL and imported it to Python instead. The results of this experiment were incredible, and paved the way for my mentor, Dirk van den Bekerom to write the first proof-of-work code demonstrating the use of GPUs to calculate the spectras that were previously being done entirely on the CPU. Benchmarking showed performance boosts of upto 10,000x compared to the naive implementation of spectra calculations on Python and upto 50x from the current implementation of RADIS.&lt;/p&gt;

&lt;p&gt;After discussing with my mentors, the project timeline was decided and the work was done accordingly. Keeping in mind my objectives for the coming month, as a part of the first evaluation, I decided to spend a lot of time on understanding the existing code base of RADIS &lt;em&gt;which focuses on the spectra calculations&lt;/em&gt;. Note that this is completely different from the code that I worked on earlier, which focussed on issues completely different from this, and mostly revolved around the post-processing of spectra instead of calculating it. This included a light reading of the original RADIS paper which talked about the general idea and logic behind RADIS. In addition to that, I also spent time on setting up the right environment for the development work that was to come.
Since the project is about GPUs and CUDA, I had to ensure that CUDA was properly installed and running on my system. While this might seem like a trivial task, it can easily get very messy when working on a linux distribution. Fortunately, I already had a working installation of CUDA on my system so I didn’t have to spend much time on it except for testing and tuning it. Another major issue that this project entailed was the handling of vast amounts of data.&lt;/p&gt;

&lt;p&gt;To keep it simple, in order to calculate the spectra, RADIS requires some data. This data includes information on various parameters such as positions, intensities, air- and self-broadened half-widths, et cetera for different molecules. For my project, for the time being, I was using the CDSD-4000 database, which is a high-temperature databank for CO2 molecule. The major issue this databank presented was the vast size of it. While the complete databank would have been incredibly huge (but fortunately not needed) the portion of the databank that we did focus on was not a small package either, occupying 30GB space unprocessed. Further processing of this data reduced it down to 8GB. While that might seem like a manageable size, the issue was that in order to compute the spectra efficiently and reduce the latency in loading the data, all of it had to be stored on the device RAM. This requirement was simply not possible for me to satisfy with just my personal computer which has a NVidia GTX 1650 with only 4GB of VRAM. Thus, I was left with two options. To either trim the database further and then work on it, or find another machine with specifications high enough to crunch the numbers without trimming it down. After discussing with my mentors and weighing the pros and cons, I decided to try out both.
We used Google Colab with its free GPU access to process the entire 8GB data in one go. The major problem we faced with this method was loading the data onto the colab server. Since even the processed files were 8GB, and Colab did not offer persistent storage, we would have to upload 8GB of data every time we wanted to test the code out, which ofcourse would not have been practical. This was solved by using Google Drive, which can be mounted in colab and work as a persistent storage setup. So far, I have thoroughly enjoyed the convenience and power offered by Colab, and that too for no charge, and hope it continues to perform so wonderfully. In addition, I also trimmed the original database down to smaller sizes and tried to process them on my personal machine, which it did without any hassles.&lt;/p&gt;

&lt;p&gt;Another interesting question that we faced was the tools to use. While it might seem like a no-brainer to use C with CUDA, it unfortunately was not an option as RADIS was written in Python. Therefore, we had to spend a fair amount of time trying to figure out additions to Python in the form of libraries which allow CUDA access. A few of the many different options that are available for such purposes include using Cython, PyCUDA, Cupy, Numba, PyOpenCL and many more. The decision to pick one over the other is a very subjective one, and the answer mostly depends on the kind of application you’re trying to produce. For our particular project, the only requirement was to have access to constant memory on the device which can be achieved using Cython or PyCUDA. While I personally enjoyed PyCUDA due to its extensive documentation and support from NVidia, my mentor seems to prefer Cython so the final decision is still not here!&lt;/p&gt;

&lt;p&gt;Apart from all this, I also spent a good amount of time studying the proof-of-work code that already exists. That included the differences from pure CPU code, the division of work between host and the device, and way the the actual calculations are being done in order to compute the spectra. Finally, I also spent a fair amount of time on revising my CUDA concepts in order to ensure there were no knowledge gaps.&lt;/p&gt;

&lt;p&gt;That pretty much sums up my community bonding period! Over the coming 4 weeks, my objectives include reproducing the proof of work, and figure out the implementation details with my mentors. That will be followed by implementing one of the broadening steps in Python and integrating it with the RADIS.&lt;/p&gt;

&lt;p&gt;I am quite excited about the upcoming months and my journey with RADIS. I believe it will be a great learning experience and I would like to thank Google, OpenAstronomy, RADIS, but most importantly, my incredibly helpful and fun mentors Erwan Pannier, Dirk van den Bekerom and Minesi N for giving me this wonderful opportunity!&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2020/05/20200530_1804_pkj-m/</guid><pubDate>Sat, 30 May 2020 17:04:56 GMT</pubDate></item></channel></rss>