<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Universe OpenAstronomy (Posts about radis)</title><link>http://openastronomy.org/Universe_OA/</link><description></description><atom:link href="http://openastronomy.org/Universe_OA/categories/radis.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Sat, 24 Jul 2021 04:53:03 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Chapter 3: Midnight Sun</title><link>http://openastronomy.org/Universe_OA/posts/2021/07/20210719_1645_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;div&gt;&lt;p&gt;Phase 1 is over :) ! We are half way through the journey. Great learning experience so far. Let’s find out what I accomplished during the previous 2 weeks (since I believe you have been following me from the beginning ;)&lt;/p&gt;
&lt;p&gt;Getting straight to the point, most of the time was spent on fixing bugs of the Profiler class and other Pull requests regarding documentation and gallery example. A new gallery example was added to demonstrate the working of &lt;code class="language-text"&gt;SpecDatabase&lt;/code&gt; and &lt;code class="language-text"&gt;init_database&lt;/code&gt; to help user to store all Spectrums in the form of a &lt;code class="language-text"&gt;.spec&lt;/code&gt; file and all input parameters in a &lt;code class="language-text"&gt;csv&lt;/code&gt; file under a folder. The same folder can be used to retrieve all Spectrums thus saving a lot of time and also no need to recompute all spectrums, so quite a handy feature. Radis has &lt;code class="language-text"&gt;plot_cond&lt;/code&gt; function to plot a 2D heat map based on the parameters in csv file for all spectrums. Creates some good looking and informative plots :) &lt;br&gt;-&amp;gt; &lt;a href="https://radis.readthedocs.io/en/latest/auto_examples/plot_SpecDatabase.html#sphx-glr-auto-examples-plot-specdatabase-py"&gt;Gallery Example&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Back to the analysis part; for LDM we expected:&lt;br&gt;&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;time(LDM_fft) ~ c2*N_lines + c3*(N_G*N_L + 1)*N_v*log(N_v) (where N_v =  Spectral Points)
time(LDM_voigt) ~ c2*N_lines + c3'*(N_G*N_L + 1)*N_truncation*log(N_truncation) (where N_truncation = broadening width / wstep)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For Legacy method I was able to prove that Calculation Time is independent of Spectral Range if we keep the N&lt;em&gt;lines and wstep constant but same is not for LDM voigt.&lt;br&gt;
A straight up comparison between Legacy and LDM voigt for NO  keeping N&lt;/em&gt;lines and wstep constant and varying the Spectral range:
&lt;a href="https://public.tableau.com/app/profile/anand.kumar4841/viz/LDMvsLegacyforSpectralRangeN_linesconstantandVoigtbroadening/Sheet1"&gt;Link&lt;/a&gt;&lt;br&gt;
Here also for None optimization we are getting constant time for different spectral range but a linear dependency for LDM Voigt which will fail the assumption of&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;t_LDM_voigt ~ c2*N_lines + c3'*(N_G*N_L + 1)*N_truncation  *log(N_truncation  )
but rather t_LDM_voigt ~ c2*N_lines + c3*(N_G*N_L + 1)*N_v*log(N_v)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3&gt;A New Discovery&lt;/h3&gt;
&lt;p&gt;On generating spectrum for millions of lines, one unique observation was seen. The bottleneck step was no longer taking the most time. Max time was spent upon an unknown process. Upon deep analysis it was found a part of code was using &lt;code class="language-text"&gt;sys.getsizeof()&lt;/code&gt; to get the size of dataframe, and when the dataframe consisited of &lt;code class="language-text"&gt;object&lt;/code&gt; type columns with millions of lines, most of the time was spent on this step only.&lt;/p&gt;
&lt;p&gt;&lt;span class="gatsby-resp-image-wrapper" style="display: block; margin-left: auto; margin-right: auto;"&gt;
&lt;a class="gatsby-resp-image-link" href="https://anandkumar-blog.netlify.app/static/95eda74e349d883f4a1fcc85291a91cc/6af66/ldm.png" rel="noopener" style="display: block;" target="_blank"&gt;
&lt;span class="gatsby-resp-image-background-image" style="display: block;"&gt;&lt;/span&gt;
&lt;img alt="complexity.jpg" class="gatsby-resp-image-image" src="https://anandkumar-blog.netlify.app/static/95eda74e349d883f4a1fcc85291a91cc/f058b/ldm.png" style="width: 100%; height: 100%; margin: 0; vertical-align: middle;" title="complexity.jpg"&gt;
&lt;/a&gt;
&lt;/span&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We replaced it with &lt;code class="language-text"&gt;memory_usage(deep=False)&lt;/code&gt; with a different threshold which made computation almost &lt;strong&gt;2x&lt;/strong&gt; faster.&lt;/p&gt;
&lt;p&gt;&lt;span class="gatsby-resp-image-wrapper" style="display: block; margin-left: auto; margin-right: auto;"&gt;
&lt;a class="gatsby-resp-image-link" href="https://anandkumar-blog.netlify.app/static/28b1ad4d276fa9921520808bc6360002/87488/ba.png" rel="noopener" style="display: block;" target="_blank"&gt;
&lt;span class="gatsby-resp-image-background-image" style="display: block;"&gt;&lt;/span&gt;
&lt;img alt="complexity.jpg" class="gatsby-resp-image-image" src="https://anandkumar-blog.netlify.app/static/28b1ad4d276fa9921520808bc6360002/f058b/ba.png" style="width: 100%; height: 100%; margin: 0; vertical-align: middle;" title="complexity.jpg"&gt;
&lt;/a&gt;
&lt;/span&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;So phase 1 is over,and phase 2 is going to begin which will mainly focus on optimizing the the existing LDM method with appropriate truncation and other possible areas!&lt;/p&gt;
&lt;p&gt;See you on the other side of the sea ;)&lt;/p&gt;
&lt;p&gt;&lt;span class="gatsby-resp-image-wrapper" style="display: block; margin-left: auto; margin-right: auto;"&gt;
&lt;a class="gatsby-resp-image-link" href="https://anandkumar-blog.netlify.app/static/6c695ad1951b1c737cc12c701ffce0e4/2551b/other.jpg" rel="noopener" style="display: block;" target="_blank"&gt;
&lt;span class="gatsby-resp-image-background-image" style="display: block;"&gt;&lt;/span&gt;
&lt;img alt="complexity.jpg" class="gatsby-resp-image-image" src="https://anandkumar-blog.netlify.app/static/6c695ad1951b1c737cc12c701ffce0e4/828fb/other.jpg" style="width: 100%; height: 100%; margin: 0; vertical-align: middle;" title="complexity.jpg"&gt;
&lt;/a&gt;
&lt;/span&gt;&lt;br&gt;&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/07/20210719_1645_anandxkumar/</guid><pubDate>Mon, 19 Jul 2021 15:45:32 GMT</pubDate></item><item><title>GSoC - 2</title><link>http://openastronomy.org/Universe_OA/posts/2021/07/20210719_0300_gagan-aryan/</link><dc:creator>Gagan Aryan</dc:creator><description>&lt;div&gt;&lt;p&gt;Welcome back !! So we are done with our first phase of the project and are shifting into the second one. I will be keeping this blog short since most of the details of the refactor have already been written in my previous post.&lt;/p&gt;
&lt;p&gt;By the time I was writing my previous post I had a pretty decent idea of how I would be doing each of the refactors. We had already decided that we may not have to implement all of them because Vaex might render a few of those changes redundant.&lt;/p&gt;
&lt;p&gt;I started out by writing a proof-of-concept to remove the column where partition function was added. Only the case of equilibrium molecules was handled here. The idea was to make use of pandas’ dictionary efficiently and remove the column. With the proof-of-concept we could conclude that not only did this approach reduce memory, but it also reduced CPU pressure by around 2x. For the lines of &lt;code&gt;HITEMP-CH4&lt;/code&gt; molecules for the waverange 2000-3000 previously the dataframe occupied 1.2 GB but with this method we could compress that to around 100 MB. &lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;Apart from this I wrote down another notebook that demostrated that we can radically improve memory usage by crunching the datatypes of the columns of &lt;code&gt;HITRAN/HITEMP&lt;/code&gt; molecules. The notebook just contains elementary operations to arrive at the right datatype for each of the column. We haven’t implemented this into the codebase yet because we still haven’t figured out what we will be doing with the missing lines. A problem I had already mentioned in my first post. &lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;I was somehow able to sneak my way into successfully completing GSoC phase one with feedback that has pumped me to do even better. I am looking forward to the second phase and hope to deliver.&lt;/p&gt;
&lt;div class="footnotes"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="https://github.com/radis/radis-benchmark/blob/master/manual_benchmarks/test_Qgas.ipynb"&gt;Proof-of-Concept for Qgas&lt;/a&gt; &lt;a class="footnote-backref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fnref:1"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="https://github.com/radis/radis-benchmark/pull/11"&gt;Proof-of-Concept for Datatype Crunching - WIP&lt;/a&gt; &lt;a class="footnote-backref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fnref:2"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/07/20210719_0300_gagan-aryan/</guid><pubDate>Mon, 19 Jul 2021 02:00:06 GMT</pubDate></item><item><title>Chapter 2: Survey Corps</title><link>http://openastronomy.org/Universe_OA/posts/2021/07/20210705_2340_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;div&gt;&lt;p&gt;So its been around 4 weeks into the coding period, a lot of insights and progress so far!&lt;/p&gt;
&lt;h3&gt;Profiler Class&lt;/h3&gt;
&lt;p&gt;The good news is that the Profiler class has been successfully implemented in the develop branch and will be available to users by version &lt;code class="language-text"&gt;0.9.30&lt;/code&gt; .&lt;br&gt;
&lt;!-- TEASER_END --&gt;
Link : &lt;a href="https://github.com/radis/radis/pull/286"&gt;Profiler PR&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Below is a simple example how all steps are printed based on the verbose level:&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;wmin = 2000
wmax = 3300
wstep = 0.01
T = 3000.0 #K
p = 0.1 #bar
broadening_max_width=10

sf = SpectrumFactory(wavenum_min=wmin, wavenum_max=wmax,
pressure=p,
wstep=wstep,
broadening_max_width=broadening_max_width,
molecule="CO",
cutoff=0, # 1e-27,
verbose=3,
)
sf.load_databank('HITEMP-CO')
s = sf.eq_spectrum(T)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Output:&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;... Scaling equilibrium linestrength
... 0.01s - Scaled equilibrium linestrength
... 0.00s - Calculated lineshift
... 0.00s - Calculate broadening HWHM
... Calculating line broadening (60869 lines: expect ~ 6.09s on 1 CPU)
...... 0.16s - Precomputed DLM lineshapes (30)
...... 0.00s - Initialized vectors
...... 0.00s - Get closest matching line &amp;amp; fraction
...... 0.02s - Distribute lines over DLM
...... 1.95s - Convolve and sum on spectral range
... 2.14s - Calculated line broadening
... 0.01s - Calculated other spectral quantities
... 2.21s - Spectrum calculated (before object generation)
... 0.01s - Generated Spectrum object
2.22s - Spectrum calculated&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Also we can access these steps and the time taken by them using &lt;code class="language-text"&gt;Spectrum.get_conditions()['profiler']&lt;/code&gt;. Also there is a parameter &lt;code class="language-text"&gt;SpectrumFactory.profiler.relative_time_percentage&lt;/code&gt; that stores the percentage of time taken by each steps at a particular verbose level, helpful seeing the most expensive steps in Spectrum calculation.&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Legacy Method Complexity&lt;/h3&gt;
&lt;p&gt;Several Spectrums were benchmarked against various parameters to see it’s correlation and derive its complexity. We used Profiler class with &lt;a href="https://radis.readthedocs.io/en/latest/source/radis.lbl.loader.html#radis.lbl.loader.DatabankLoader.init_database"&gt;init_database()&lt;/a&gt; which stores all parameters of Spectrum along the Profiler in a &lt;code class="language-text"&gt;csv&lt;/code&gt; generated file; all spectrum info got added into the csv file  which could be used to do create visualizations to analyze the data. We used &lt;code class="language-text"&gt;Xexplorer&lt;/code&gt; library and &lt;code class="language-text"&gt;Tableau&lt;/code&gt;(a visual analytics platform) to create visualizations. A &lt;a href="https://github.com/anandxkumar/Benchmark_Visualization_GSoC_2021"&gt;github repository&lt;/a&gt; was created to store the Visualization along the CSV data file of each benchmark.&lt;/p&gt;
&lt;p&gt;Following are the inference of the benchmarks for Legacy Method:&lt;/p&gt;
&lt;b&gt;
•  Calculation Time ∝ Number of lines&lt;br&gt;
•  Calculation Time ∝ Broadening max width&lt;br&gt;
•  Calculation Time ∝ 1/wstep&lt;br&gt;
•  Calculation Time not dependent on Spectral Range&lt;br&gt;
&lt;/b&gt;&lt;br&gt;
&lt;p&gt;So complexity of Legacy method can be derived as: &lt;br&gt;
&lt;strong&gt;&lt;code class="language-text"&gt;complexity = constant * Number of lines * Broadening Max Width / Wstep&lt;/code&gt;&lt;/strong&gt; &lt;br&gt;&lt;/p&gt;
&lt;h3&gt;LDM Method Complexity&lt;/h3&gt;
&lt;p&gt;Similar technique was used to benchmark LDM method. Now LDM uses 2 types of broadening method that are &lt;code class="language-text"&gt;voigt&lt;/code&gt; and &lt;code class="language-text"&gt;fft&lt;/code&gt;. &lt;code class="language-text"&gt;voigt&lt;/code&gt; uses truncation for calculating spectrum  in wavenmber space where as &lt;code class="language-text"&gt;fft&lt;/code&gt; calculates spectrum on entire spectral range in fourier space. So benchmarks were done on both methods to compare their performance against various parameters.&lt;/p&gt;
&lt;p&gt;Spectrum were benchmarked against parameters like Spectral Range, Wstep, Spectral Points, Number of Lines and Broadening Max Width. Following are the inferences.&lt;/p&gt;
&lt;p&gt;For &lt;code class="language-text"&gt;fft&lt;/code&gt;:&lt;br&gt;
&lt;b&gt;
• Calculation Time ∝ Spectral Points&lt;br&gt;
• Calculation Time ∝ Number of Lines&lt;br&gt;
&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;For &lt;code class="language-text"&gt;voigt&lt;/code&gt;:&lt;br&gt;
&lt;b&gt;
• Calculation Time ∝ Spectral Points&lt;br&gt;
• Calculation Time ∝ Number of Lines&lt;br&gt;
• Calculation Time ∝ Broadening Max Width&lt;br&gt;
&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;For LDM we are expecting the following complexity:&lt;br&gt;
&lt;strong&gt;&lt;code class="language-text"&gt;t_LDM_fft ~ c2*N_lines + c3*(N_G*N_L + 1)*N_v*log(N_v)&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;
&lt;strong&gt;&lt;code class="language-text"&gt;t_LDM_voigt ~ c2*N_lines + c3'*(N_G*N_L + 1)*N_truncation*log(N_truncation)&lt;/code&gt;&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt; So the goal for the next 2 weeks will be to get the complexity of both &lt;code class="language-text"&gt;voigt&lt;/code&gt; and &lt;code class="language-text"&gt;fft&lt;/code&gt; method and see places for improving both methods and quite possibily create a &lt;code class="language-text"&gt;Hybrid&lt;/code&gt; method taking the best of both worlds. &lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/07/20210705_2340_anandxkumar/</guid><pubDate>Mon, 05 Jul 2021 22:40:32 GMT</pubDate></item><item><title>Chapter 1: First Flight</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_2240_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;div&gt;&lt;p&gt;Hey! Missed me? I’m back with another blog, the first related to the Coding Period. Got some progress and interesting observation to share!&lt;/p&gt;
&lt;h3&gt;Ready -&amp;gt; Set -&amp;gt; Code -&amp;gt; Analyze&lt;/h3&gt;
&lt;p&gt;The first thing I did in the coding period, was analyse the problem and get a feasible approach to resolve it.&lt;br&gt;&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;&lt;strong&gt;Problem:&lt;/strong&gt; Find the complexity of the Legacy and LDM method.&lt;br&gt;
&lt;strong&gt;Solution:&lt;/strong&gt; Run some benchmarks and find the bottleneck step.&lt;br&gt;&lt;/p&gt;
&lt;p&gt;First I chose the &lt;strong&gt;Legacy&lt;/strong&gt; method because if its simpler architecture. I ran some benchmarks varying the &lt;code class="language-text"&gt;spectral range&lt;/code&gt; of &lt;code class="language-text"&gt;OH&lt;/code&gt; and &lt;code class="language-text"&gt;CO2&lt;/code&gt; molecule to get similar number of lines. I kept parameters like &lt;code class="language-text"&gt;pressure&lt;/code&gt;, &lt;code class="language-text"&gt;temperature&lt;/code&gt;, &lt;code class="language-text"&gt;broadening_max_width&lt;/code&gt;, &lt;code class="language-text"&gt;wstep&lt;/code&gt;, etc constant to see the dependence of Legacy method on &lt;strong&gt;Spectral range&lt;/strong&gt;. &lt;br&gt;&lt;/p&gt;
&lt;p&gt;In order to get similar number of lines, I created a function which will take the &lt;strong&gt;Spectrum Factory&lt;/strong&gt; &lt;code class="language-text"&gt;dataframe&lt;/code&gt; and select the target number of lines. But the issue with Pandas dataframe is that when modify the dataframe there are chances that the metadata will get lost and we will no longer be able to do Spectrum calculation. To avoid this we have to drop the right number of lines with &lt;code class="language-text"&gt;inplace=True&lt;/code&gt;. So we will need to fix the number of lines and then we can proceed ahead with the benchmarking. Every parameter is the same except the Spectral Range.  Full code &lt;a href="https://gist.github.com/anandxkumar/cbe12f47170e1d71a82f4b246bd01dcc"&gt;here&lt;/a&gt;.&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Earlier we assumed that the complexity of Legacy method is: &lt;br&gt;
&lt;strong&gt;&lt;code class="language-text"&gt;Voigt Broadening = Broadening_max_width * spectral_range/math.pow(wstep,2) * N&lt;/code&gt;&lt;/strong&gt; &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Thus I was expecting to have different calculation time for both benchmarks. But to my surprise the computational times were almost equivalent! I re-ran each benchmarks &lt;strong&gt;100 times&lt;/strong&gt; just to be sure and more precise about it. Following were the observations:&lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of lines - &lt;b&gt;{‘OH’: 28143, ‘CO’: 26778}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;Total Calculation time(Avg) -  &lt;b&gt;{‘OH’: 4.4087, ‘CO’: 3.8404000000000003}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;Total Voigt_Broadening TIME(Avg) - &lt;b&gt;{‘OH’: 3.1428814244270327, ‘CO’: 3.081623389720917}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;spectral_range - &lt;b&gt;{‘OH’: 38010, ‘CO’: 8010}&lt;/b&gt;&lt;/li&gt;
&lt;li&gt;Legacy_Scale - &lt;b&gt;{‘OH’: 4x10^14, ‘CO’: 8x10^13}&lt;/b&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are some inference we can make from the above observation:&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A)&lt;/strong&gt; The bottleneck step(Voigt Broadening) loosely depends on &lt;code class="language-text"&gt;Spectral Range&lt;/code&gt;.&lt;br&gt;
&lt;strong&gt;B)&lt;/strong&gt; The complexity of Voigt Broadening needs to be modified because there is a difference of order of &lt;strong&gt;~10&lt;/strong&gt; in the Legacy Scaled value of OH and CO2.&lt;br&gt;&lt;/p&gt;
&lt;p align="center"&gt;
&lt;span class="gatsby-resp-image-wrapper" style="display: block; margin-left: auto; margin-right: auto;"&gt;
&lt;a class="gatsby-resp-image-link" href="https://anandkumar-blog.netlify.app/static/d9b32b4e96e6cd9a91016a49ad940239/0b533/Blog2.png" rel="noopener" style="display: block;" target="_blank"&gt;
&lt;span class="gatsby-resp-image-background-image" style="padding-bottom: 100%; display: block;"&gt;&lt;/span&gt;
&lt;img alt="Blog2" class="gatsby-resp-image-image" src="https://anandkumar-blog.netlify.app/static/d9b32b4e96e6cd9a91016a49ad940239/0b533/Blog2.png" style="width: 100%; height: 100%; margin: 0; vertical-align: middle;" title="Blog2"&gt;
&lt;/a&gt;
&lt;/span&gt;&lt;br&gt;
&lt;b&gt;Credits - Me :p&lt;/b&gt;&lt;br&gt;
&lt;/p&gt;
&lt;p&gt;So in order to do some analysis, we first need data of different steps in the broadening phase and conditions of various Spectrum which brings me to the &lt;strong&gt;Code&lt;/strong&gt; part in &lt;strong&gt;Coding Period.&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;h3&gt;Profiler Class&lt;/h3&gt;
&lt;p&gt;The aim of this class is to replace all the print statements by a common &lt;code class="language-text"&gt;start&lt;/code&gt;, &lt;code class="language-text"&gt;stop&lt;/code&gt;, &lt;code class="language-text"&gt;_print&lt;/code&gt; method. Earlier each step computational time was done using &lt;code class="language-text"&gt;time()&lt;/code&gt; library. Now the whole codebase is being refactored with the Profiler class that will do all the work based on the &lt;code class="language-text"&gt;verbose&lt;/code&gt; level. In addition to this the biggest benefit is that each step will be stored in a dictionary with its computational time that will help me gather data to find which step is in actual bottleneck and further which part of the function is the most expensive time wise. A simple example is below:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Before:&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;if __debug__:
t0 = time()
..........
..........
if __debug__:
t1 = time()
.........
.........
if __debug__:
if self.verbose &amp;gt;= 3:
printg("... Initialized vectors in {0:.1f}s".format(t1 - t0))&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;After:&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;self.profiler.start(
key="init_vectors", verbose=3, details="Initialized vectors"
)
.........
.........
self.profiler.stop("init_vectors")&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;So using a common key we can make it happen. This will be stored in the conditons of &lt;code class="language-text"&gt;Spectrum&lt;/code&gt; object in the &lt;code class="language-text"&gt;'profiler'&lt;/code&gt; key. All these Spectrums and their conditions can be exported using a &lt;a href="https://radis.readthedocs.io/en/latest/spectrum/spectrum.html#spectrum-database"&gt;SpecDatabase&lt;/a&gt;. This will create a csv file comprising of all the parameters of all Spectrums which will be useful in getting some insights.
-&amp;gt; &lt;a href="https://github.com/radis/radis/pull/286"&gt;PR LINK&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Digging in whiting_jit&lt;/h3&gt;
&lt;p&gt;Based on several benchmarks, it is estimated that around &lt;strong&gt;70-80%&lt;/strong&gt; time is spent on calculating the broadening. The broadening part has the following hierarchy:&lt;br&gt;
&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;_calc_broadening()
-&amp;gt; _calc_lineshape()
-&amp;gt; _voigt_broadening()
-&amp;gt; _voigt_lineshape()
-&amp;gt; whiting_jit()&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;On close inspection we observed that &lt;strong&gt;80-90%&lt;/strong&gt; time is spent on &lt;code class="language-text"&gt;whiting_jit&lt;/code&gt; process. Going further down in &lt;code class="language-text"&gt;whiting_jit&lt;/code&gt;, &lt;strong&gt;60-80%&lt;/strong&gt; time is spent on &lt;strong&gt;lineshape calculation.&lt;/strong&gt; Below is the formula:&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;lineshape = (
(1 - wl_wv) * exp(-2.772 * w_wv_2)
+ wl_wv * 1 / (1 + 4 * w_wv_2)
# ... 2nd order correction
+ 0.016 * (1 - wl_wv) * wl_wv * (exp(-0.4 * w_wv_225) - 10 / (10 + w_wv_225))
)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The whole process can be divided into 4 parts:&lt;br&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;    part_1 =   (1 - wl_wv) * exp(-2.772 * w_wv_2)

part_2 =    wl_wv * 1 / (1 + 4 * w_wv_2)

# ... 2nd order correction
part_3 =  0.016 * (1 - wl_wv) * wl_wv * exp(-0.4 * w_wv_225)

part_4 =  - 10 / (10 + w_wv_225)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The complexity of each part comes out: &lt;br&gt;
&lt;b&gt;&lt;/b&gt;&lt;/p&gt;
&lt;div class="gatsby-highlight"&gt;&lt;pre class="language-text"&gt;&lt;code class="language-text"&gt;    o1 = broadening__max_width * n_lines / wstep

O(part_1) = n_lines * o1
O(part_2) = n_lines * 4 * o1
O(part_3) = (n_lines)**2 * o1
O(part_4) = o1 &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Running several benchmark showed us that &lt;strong&gt;part_3&lt;/strong&gt; takes the most time out of all steps. So clearly we can see that the complexity of Legacy method is not dependent on
Spectral Range but rather &lt;code class="language-text"&gt;Number of Calculated Lines&lt;/code&gt;,&lt;code class="language-text"&gt;broadening__max_width&lt;/code&gt; and &lt;code class="language-text"&gt;wstep&lt;/code&gt;. It may seem that the complexity of Legacy method is:&lt;br&gt;&lt;/p&gt;
&lt;p align="center"&gt;&lt;b&gt; n_lines^2 * broadening__max_width * n_lines / wstep&lt;/b&gt;&lt;/p&gt; &lt;br&gt;
&lt;p&gt;But inorder to prove this we need more benchmarks and evidence to verify this and it may involve normalization of all steps in lineshape calculation!&lt;br&gt; &lt;/p&gt;
&lt;p&gt;So the goal for the next 2 weeks is clear:&lt;br&gt;
&lt;b&gt;i)&lt;/b&gt; Refactor the entire codebase with Profiler.&lt;br&gt;
&lt;b&gt;ii)&lt;/b&gt; Find the complexity of &lt;strong&gt;Legacy Method&lt;/strong&gt; with the help of more benchmark and analysis.&lt;br&gt;
&lt;b&gt;iii)&lt;/b&gt; Do the same for &lt;strong&gt;LDM Method&lt;/strong&gt;!&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Ok I guess time’s up! See you after 2 weeks :)&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210621_2240_anandxkumar/</guid><pubDate>Mon, 21 Jun 2021 21:40:32 GMT</pubDate></item><item><title>GSoC - 1</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210620_0300_gagan-aryan/</link><dc:creator>Gagan Aryan</dc:creator><description>&lt;div&gt;&lt;p&gt;This is the first blog that documents the coding period of my GSoC21 journey. I learnt a few interesting things in these two weeks, as I expected I would. So, let’s dive in and see if you knew few of these stuff I learnt.&lt;/p&gt;
&lt;h3 id="starting-off-"&gt;Starting off !!!&lt;/h3&gt;
&lt;p&gt;I started off by getting a brief idea of the scope of the changes that could be done to the dataframe. This was the task I had decided on for the first week. Whenever we are involved in a project that runs for a period of anywhere between 2-4 months it is important to have a timeline or a roadmap of sorts to be able to look back to. This doesn’t really have to be something rigid. We can chose to deviate from it and infact deviations are bound to happen due to multiple reasons. It can happen because of an unexpected bug in between, or because you came across some alternative that you did not consider at the start or simply because it is one of those projects that gives better insights as you dwell into it.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;Every good GSoC proposal consists of a tentative timeline that depicts the work we plan on doing as the weeks progress. Here is the timeline I had submitted in my proposal.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Timeline1" src="https://gagan-aryan.netlify.app/images/gsoc-1/timeline1.png"&gt;&lt;br&gt;
&lt;img alt="Timeline2" src="https://gagan-aryan.netlify.app/images/gsoc-1/timeline2.png"&gt;&lt;/p&gt;
&lt;p&gt;So as per this I was supposed to finish off the refactors to the dataframe and also finish setting up the benchamarks. But I was unable to complete these. I had underestimated the work it would take to complete them. Nonetheless, I also did have some time to look up at the things I am supposed to do in the second half of the coding period.&lt;/p&gt;
&lt;h3 id="memory-and-time-performance-benchmarks---tic-tok"&gt;Memory and Time performance benchmarks - Tic-Tok&lt;/h3&gt;
&lt;p&gt;Before making any changes to the codebase Erwan suggested me to have the benchmarks setup. So what do I mean by this? To make sure that the changes I am making to the code are indeed reducing the memory consumption of the computations we use a few tools that help us track the memory consumption for various calculations as a function of git commits. There are multiple tools that help us do this. Radis already used a tool developed by &lt;a href="https://github.com/airspeed-velocity/asv"&gt;airspeed velocity&lt;/a&gt; to track the memory computions. I ran into a lot of troubles in setting these up and a lost a lot of valuable time in the process ultimately Erwan fixed it and I was able to run the benchmarks on my machine.&lt;/p&gt;
&lt;p&gt;The benchmarks still seem to take a lot of time to run though and for them to be feasible to be used a tool through which I can check the performance regularly there are a few things I need to learn. I hope to pick these up in the next few days.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Performance" src="https://gagan-aryan.netlify.app/images/gsoc-1/brace-yourselves.png"&gt;&lt;/p&gt;
&lt;p&gt;We are also trying to look at a few other alternatives that can be used instead of asv. I will update the you guys regarding this in the next blog post.&lt;/p&gt;
&lt;h3 id="oh-pandas-here-i-deal-with-you-"&gt;Oh Pandas here I deal with you !&lt;/h3&gt;
&lt;h4 id="lets-ditch-a-few-columns"&gt;Let’s ditch a few columns&lt;/h4&gt;
&lt;p&gt;We can reduce the memory usage of pandas by using one really simple trick - avoid giving loading the columns that are not required for computation. Below I demostrate how just dropping a few columns can provide significant improvement in the memory consumption. I am using &lt;code&gt;HITEMP-CH4&lt;/code&gt; database for demonstration.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;div class="chroma"&gt;
&lt;table class="lntable"&gt;&lt;tr&gt;&lt;td class="lntd"&gt;
&lt;pre class="chroma"&gt;&lt;code&gt;&lt;span class="lnt"&gt; 1
&lt;/span&gt;&lt;span class="lnt"&gt; 2
&lt;/span&gt;&lt;span class="lnt"&gt; 3
&lt;/span&gt;&lt;span class="lnt"&gt; 4
&lt;/span&gt;&lt;span class="lnt"&gt; 5
&lt;/span&gt;&lt;span class="lnt"&gt; 6
&lt;/span&gt;&lt;span class="lnt"&gt; 7
&lt;/span&gt;&lt;span class="lnt"&gt; 8
&lt;/span&gt;&lt;span class="lnt"&gt; 9
&lt;/span&gt;&lt;span class="lnt"&gt;10
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class="lntd"&gt;
&lt;pre class="chroma"&gt;&lt;code class="language-python"&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;radis.io.hitran&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;hit2df&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hit2df&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"06_HITEMP2020_2000.0-2500.0.par"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;memory_usage&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"deep"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="n"&gt;memory&lt;/span&gt; &lt;span class="n"&gt;usage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;30.5&lt;/span&gt; &lt;span class="n"&gt;MB&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s2"&gt;"id"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"iso"&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;verbose&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;memory_usage&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"deep"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="n"&gt;memory&lt;/span&gt; &lt;span class="n"&gt;usage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;25.4&lt;/span&gt; &lt;span class="n"&gt;MB&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;The peak memory usage before dropping the columns was 30.5 MB and once I remove a few columns the peak memory usage becomes 25.4 MB. I have already implemented the dropping of id column and handled the case of single isotope as well by dropping the column and istead just storing the information of the isotope as a meta attribute. We have also finalised on the discarding of the other columns by considering the physics of these quantities. Let’s check out a few of them. Since I haven’t already implemented the optimisations that follow I will save the implementation details for the next blog.&lt;/p&gt;
&lt;h4 id="einsteins-coeffecients-and-linestrengths"&gt;Einstein’s Coeffecients and Linestrengths&lt;/h4&gt;
&lt;p&gt;There are four parameters of interest to describe the intensity of a line : Linestrength $(int)$, Einstein emission coefficient $(A)$ and Einstein absorption coefificent $(B_{lu})$, Einstein induced emission coefficient $(B_{ul})$. All of them are somehow linked to the Squared Transition Dipole Moment $(R)$. &lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;$$ B_{lu}=10^{-36}\cdot\frac{8{\pi}^3}{3h^2} R_s^2 \cdot 10^{-7} $$&lt;br&gt;
$$ B_{ul}=10^{-36}\cdot\frac{8{\pi}^3}{3h^2} \frac{gl}{gu} R_s^2 \cdot 10^{-7} $$&lt;br&gt;
$$ A_{ul}=10^{-36}\cdot\frac{\frac{64{\pi}^4}{3h} {\nu}^3 gl}{gu} R_s^2 $$&lt;/p&gt;
&lt;p&gt;So now the idea would be to drop the $int$ column and use $A_{ul}$ to calculate the value of $int$ from it. The reason to drop $int$ and not $A_{ul}$ some databases like &lt;code&gt;ExoMol&lt;/code&gt; databases only provide the value of $A_{ul}$.&lt;/p&gt;
&lt;h4 id="concat-better"&gt;Concat better&lt;/h4&gt;
&lt;p&gt;For anyone who wants concate multiple datafiles pandas tends to become useless as the memory scales up. I started out experimenting concat operations inorder to cluster the isotopes of each type, run computations on them and later concat them. But I later learnt that since this data is already in the form of a single dataframe, indexing is a better parameter to track the memory consumption. Nonetheless there are a few other places in Radis where we process multiple files and concat them, hence this experiment would help us decide how we can chose to replace the current approach with a better one. I tried out three methods. I was using some random dummy datafiles of around 780 MBs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Normal &lt;code&gt;pandas.concat&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Concat with a doubly ended queue&lt;/li&gt;
&lt;li&gt;Concat with parquet&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here are the results of each of these methods -&lt;/p&gt;
&lt;div class="tab" id="cbbfde2b70afc7e2"&gt;
&lt;div class="tab__links"&gt;
&lt;button class="tab__link"&gt;pandas.concat&lt;/button&gt;
&lt;button class="tab__link"&gt;deque&lt;/button&gt;
&lt;button class="tab__link"&gt;parquet&lt;/button&gt;
&lt;/div&gt;
&lt;div class="tab__content" id="c6b1e010ed52c9f8"&gt;
&lt;h4 id="pandasconcat"&gt;pandas.concat&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;CPU Time - 0:02:43.797588
Peak Memory Usage - 4.1050 GB
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="tab__content" id="a412292f0cf68165"&gt;
&lt;h4 id="pandasconcat-with-a-doubly-queue"&gt;pandas.concat with a doubly queue&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;CPU Time - 0:02:34.484612
Peak Memory Usage - 3.7725 GB
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="tab__content" id="c655ff276a8106d7"&gt;
&lt;h4 id="concat-with-parquet"&gt;Concat with parquet&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;CPU Time - 0:01:37.984875
Peak Memory Usage - 1.6829 GB
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Looking at the results, parquet seems like a really good option to me. But we will run for a few more examples and later check which one suits the best.&lt;/p&gt;
&lt;h3 id="the-next-two-weeks"&gt;The next two weeks&lt;/h3&gt;
&lt;p&gt;The project is making progress in all fronts. I feel I need to reorganize my thoughts a bit. My main work for now would be to complete the task list of &lt;a href="https://github.com/radis/radis/pull/287"&gt;this pr&lt;/a&gt;. And then look at other stuff.&lt;/p&gt;
&lt;div class="footnotes"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="https://www.sciencedirect.com/science/article/pii/S0022407398000788?via%3Dihub"&gt;Rothmann Paper (Eqs.(A7), (A8), (A9)&lt;/a&gt; &lt;a class="footnote-backref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fnref:1"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210620_0300_gagan-aryan/</guid><pubDate>Sun, 20 Jun 2021 02:00:06 GMT</pubDate></item><item><title>Chapter 0: Prologue</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210606_2240_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;div&gt;&lt;p&gt;Hi There and Namaste! This is going to be the second blog and first blog related to GSoC where I will be sharing my experience Community Bonding Period Experience with &lt;b&gt;Radis&lt;/b&gt;. Before moving ahead lets learn about GSoC and my perspective about it.&lt;/p&gt;
&lt;h3&gt;Google Summer of Code&lt;/h3&gt;
&lt;p&gt;GSoC or the way I like to say it &lt;strong&gt;(Great Summer Opportunity to Code ;)&lt;/strong&gt; is a program conducted and funded by Google to promote college students around the world to engage with Open Source Community and contribute to the organization for a tenure of 3 months. In the process, code is created and released for the world to see and use. But the main aim of GSoC is to promote students to stick to the organizations and help to grow the Open Source Community. This is a great initiative by Google that brings thousands of students every year and help them get an opportunity to peek into the world of open source development, learn new skills and also get compensated for the work, quite generously.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;I remember during second year of my college, it was around end of March and my roommate was applying for GSoC and I was like what is this program? There I got to know about it but since the deadline was near I was afraid of doing all the stuffs in a week of time, so I didn’t apply for it. Fast forwarding to next year, I was prepared enough this time and I feel priviledged to be a part of GSoC as part of OpenAstronomy. &lt;/p&gt;
&lt;h3&gt;My GSoC Project&lt;/h3&gt;
&lt;p&gt;I’m part of &lt;b&gt;&lt;a href="https://github.com/radis/radis"&gt;Radis&lt;/a&gt;&lt;/b&gt; organization which is a sub-org of &lt;a href="https://github.com/OpenAstronomy"&gt;OpenAstronomy&lt;/a&gt;. Radis is a fast line-by-line code used to synthesize high-resolution infrared molecular
spectra and a post-processing library to analyze spectral lines. It can synthesize absorption
and emission spectrum for multiple molecular species at both equilibrium and
non-equilibrium conditions.&lt;br&gt;
Radis computes every spectral line (absorption/emission) from the molecule considering
the effect of parameters like Temperature, Pressure. Due to these parameters, we don’t get
a discrete line but rather a shape with a width. This is called line broadening and for any spectral synthesis code, this is the bottleneck step. Ok let us C what my GSoC project is all about! &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Radis has 2 methods to calculate the lineshape of lines.&lt;br&gt;
● Legacy Method&lt;br&gt;
● DLM Method&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The goal of this project is to derive an equation comprising all parameters that affect the
performance for calculating Voigt broadening by running several benchmarks for different
parameters involved in the calculation of lineshapes to check their significance in
computation time. Then we need to find the critical value for the derived equation (&lt;code class="language-text"&gt;Rc&lt;/code&gt;)
which will tell us which optimization technique to select based on the computed &lt;code class="language-text"&gt;R&lt;/code&gt; value in
&lt;b&gt;calc_spectrum()&lt;/b&gt;. An &lt;code class="language-text"&gt;optimization = "auto"&lt;/code&gt; will be added that will choose the best method based on the parameters provided.&lt;/p&gt;
&lt;h3&gt;Community Bonding Period&lt;/h3&gt;
&lt;p&gt;The first phase of GSoC is the &lt;b&gt;Community Bonding Period&lt;/b&gt; which is a 3 weeks long period. Its main aim is allow the student to get familiar with the community and the codebase. It serves as a warm-up period before the coding period. The first thing I did was that I went though the original Radis &lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S0022407318305867?via%3Dihub"&gt;paper&lt;/a&gt; and also the DLM implementation &lt;a href="https://ui.adsabs.harvard.edu/abs/2021JQSRT.26107476V/abstract"&gt;paper&lt;/a&gt; because our project objective is based on these 2 implementations. It helped me understand the main purpose of RADIS, its architecture and the science behind different steps of both equilibrium and non-equilibrium spectrum, though I have to accept these papers are way too technical for me :p (Complex Spectroscopy related formulas).&lt;br&gt; I believed inorder to get myself ready for the coding period, I shall focus on solving some related issues to make me more familiar with the codebase.&lt;br&gt;&lt;/p&gt;
&lt;p&gt;In order to compute any spectrum we need to determine several parameters like: minimum-maximum wavenumber, molecule, Temperature of gas, mole fraction, wstep, etc.&lt;br&gt;
&lt;code class="language-text"&gt;wstep&lt;/code&gt; determines the wavenumber grid’s resolution. Smaller the value, higher the resolution and vice-versa. By default radis uses &lt;code class="language-text"&gt;wstep=0.01&lt;/code&gt;. You can manually set the wstep value in &lt;b&gt;calc_spectrum()&lt;/b&gt; and &lt;strong&gt;SpectrumFactory&lt;/strong&gt;. To get more accurate result you can further reduce the value, and to increase the performance you can increase the value.&lt;/p&gt;
&lt;p&gt;Based on wstep, it will determine the number of gridpoints per linewidth. To make sure that there are enough gridpoints, Radis will raise an &lt;strong&gt;Accuracy Warning&lt;/strong&gt;. If number of gridpoints are less than &lt;code class="language-text"&gt;GRIDPOINTS_PER_LINEWIDTH_WARN_THRESHOLD&lt;/code&gt; and raises an &lt;strong&gt;Accuracy Error&lt;/strong&gt; if number of gridpoints are less than &lt;code class="language-text"&gt;GRIDPOINTS_PER_LINEWIDTH_ERROR_THRESHOLD&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So inorder to select the optimum value of &lt;code class="language-text"&gt;wstep&lt;/code&gt; I had to refactor the codebase such that we could compute the minimum FWHM (&lt;code class="language-text"&gt;min_width&lt;/code&gt;) value after calculating the HWHM of all lines and and set &lt;code class="language-text"&gt;wstep = min_width / GRIDPOINTS_PER_LINEWIDTH_WARN_THRESHOLD&lt;/code&gt;. All &lt;code class="language-text"&gt;wstep&lt;/code&gt; dependent parameters had to be refactored to make sure they are not being called before the calculating &lt;code class="language-text"&gt;min_width&lt;/code&gt;. At the end this feature was successfully merged in the develop branch of Radis and now users can use &lt;code class="language-text"&gt;wstep = "auto"&lt;/code&gt; to automatically get the optimal value of &lt;code class="language-text"&gt;wstep&lt;/code&gt;. This feature will be available from version &lt;b&gt;0.9.30&lt;/b&gt;. Here is the &lt;a href="https://github.com/radis/radis/pull/271"&gt;link&lt;/a&gt; of the merged PR.&lt;/p&gt;
&lt;p&gt;In short, the Community Bonding Period has been great and I have learned alot about Radis during this time. In the next 2 weeks I will be focussing on building a benchmarking framework and run various benchmarks for both Legacy and DLM method and determine the most influential paramters for performance.&lt;/p&gt;
&lt;p&gt;I’m very excited for the upcoming months. I know that this summer is going to be a life long experience and I’m really looking forward to do amazing things for the community and want to thank Google, OpenAstronomy, Radis and my mentors &lt;a href="https://github.com/erwanp"&gt;Erwan Pannier&lt;/a&gt;, &lt;a href="https://github.com/dcmvdbekerom"&gt;Dirk van den Bekerom&lt;/a&gt; and &lt;a href="https://github.com/pkj-m"&gt;Pankaj Mishra&lt;/a&gt; for this opportunity.
I’m ready for this amazing adventure.&lt;/p&gt;
&lt;p align="center"&gt;
&lt;b&gt;LETS DO THIS&lt;/b&gt;&lt;br&gt;
&lt;img src="https://anandkumar-blog.netlify.app/2b4e6a4b663f4bc49d559484b8dd37b1/Start.gif"&gt;&lt;br&gt;
ps: Am a huge Spiderman Fan :p
&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210606_2240_anandxkumar/</guid><pubDate>Sun, 06 Jun 2021 21:40:32 GMT</pubDate></item><item><title>GSoC - 0</title><link>http://openastronomy.org/Universe_OA/posts/2021/06/20210606_0300_gagan-aryan/</link><dc:creator>Gagan Aryan</dc:creator><description>&lt;div&gt;&lt;p&gt;I will be documenting my journey in the GSoC program under Radis (OpenAstronomy). This blog is the first in the series of those blogs and will contain a quick overview of what Google Summer of Code is, an intro to the organization I will be working with and the project I will be involved in, and what I did in the 20-day community bonding period.&lt;/p&gt;
&lt;h3 id="what-is-gsoc"&gt;What is GSoC?&lt;/h3&gt;
&lt;p&gt;I remember attending one of Programming Club IIT Kanpur’s lectures in my freshman year of college, and my senior just asked the students if they knew what GSoC was. I had no idea. But I glanced over to see if my peers knew something and saw a few of them nodding enthusiastically and a few others muttering among themselves. The senior didn’t explain what GSoC was, but he did ask us to check it out ourselves. I did. I wouldn’t save I understood the entire program back then since I didn’t even know what open source was.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;Fast forward around 9-10 months, I started contributing to open source. I really felt it helped me skill up as a developer, which motivated me to participate in GSoC.&lt;/p&gt;
&lt;p&gt;Google Summer of Code or GSoC is a program sponsored by Google that aims to connect university students worldwide with open source organizations to promote the open-source culture. Students work with an open-source organization on a 10-week programming project during their break from school and get an opportunity to contribute to high-quality code, learn new skills, and also get compensated for the work. In turn, the organizations benefit from a few extra pairs of helping hands. Any college student interested in software development should definitely check out this program.&lt;/p&gt;
&lt;h3 id="radis-and-my-project"&gt;Radis and my project&lt;/h3&gt;
&lt;p&gt;Radis&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; is a fast line-by-line code for synthesizing and fitting infrared absorption and emmision spectra such as encountered in laboratory plasmas or exoplanet atmospheres.&lt;/p&gt;
&lt;p&gt;Radis aims to provide a wide array of features and remain user-friendly at the same so. It currently supports spectral calculations on databases like HTIRAN and high-temperature databases like HITEMP, CDSD-4000 with a future plan on extending the support to ExoMol. It comes with just a one-line install and post-processing tools for analysis of the spectra. Users can also combine ranges to create a mixture of gases or calculate radiative transfer along the line-of-sight.&lt;/p&gt;
&lt;p&gt;RADIS uses Pandas dataframe for handling all the databases currently. Quoting the words of Wes (the core dev of Pandas), “pandas rule of thumb: have 5 to 10 times as much RAM as the size of the dataset” &lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;. Which makes it impossible to read, say, a database of size 5GB on a machine with a RAM of 16GB.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Pandas Meme" src="https://gagan-aryan.netlify.app/images/gsoc-0/pandas_meme.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;The goal of this project would be first to reduce the memory usage of the current calculations. Then, we replace pandas with libraries that are better suited for handling larger-than-memory databases, which would make it possible to compute spectral databases of up to billions of lines (of the scale of hundreds of GB or terabytes). I will say the core technical details of the project for the upcoming blogs.&lt;/p&gt;
&lt;h3 id="community-bonding-period"&gt;Community Bonding Period&lt;/h3&gt;
&lt;p&gt;The Community Bonding Period is an almost 20-days long period meant to serve as a warm-up or a buffer before the actual coding period begins. It can be used for a wide variety of purposes, such as getting a better understanding of the codebase and figuring out its intricacies. I started out by quickly going over Spectro-102&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt; again since I had left out a few parts the last time I did. I then studied the RADIS &lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; paper. Though I cannot really say the entire document, I did get a top-level idea of how it works and how it is different from other software.&lt;/p&gt;
&lt;h4 id="my-failed-attempts-in-wrapping-up-the-previous-work"&gt;My failed attempts in wrapping up the previous work&lt;/h4&gt;
&lt;p&gt;After my GSoC application, I started working on a feature request that asked a specific function in the code to return the wavelength and the intensity grid in sorted ascending order. I just assumed that all I need to do was sort the grids, and I did this and created a PR. I later learned that Radis, like any good codebase, has many tests that make sure things don’t break when a new change is made. Apparently, returning the wavelengths and intensity grid in the sorted order broke the physics when combining spectra.&lt;/p&gt;
&lt;p&gt;Before this PR, I was unaware of &lt;code&gt;pytests&lt;/code&gt;. I went through the documentation&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt;, ran the tests on my machine, and checked out each of the failing tests. This helped me understand different parts of the code, especially the &lt;code&gt;spectrum&lt;/code&gt; and &lt;code&gt;los&lt;/code&gt; modules of the repository. The tests passed of the &lt;code&gt;spectrum&lt;/code&gt; module passed after a few modifications. But, after I updated Erwan regarding my progress, I realized that I need to now design new tests since we cannot pinpoint where we are having problems in the codebase with the existing ones. Besides, I learned about the different types of tests (non-regression, validation, and verification) that exist in RADIS to ensure things don’t break after a brief chat with Erwan.&lt;/p&gt;
&lt;p&gt;We have decided how we will tackle this issue, but since I am required to start on my project from tomorrow, I will be getting back to this PR later and hope to find time for the same during the coding period.&lt;/p&gt;
&lt;h4 id="discovery-during--hitemp-co2h2o-download-automation"&gt;Discovery during HITEMP (CO2/H2O) download automation&lt;/h4&gt;
&lt;p&gt;In the first phase of my project, I am required to use a few hacks&lt;sup id="fnref:5"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:5"&gt;5&lt;/a&gt;&lt;/sup&gt; in the pandas and boost their memory performance. This includes dropping a few columns and changing the datatypes of a few others. Coincidentally Dirk encountered an issue while working on automating the download of CO2/H2O for HITEMP. So, CO2/H2O spectral databases contain multiple zip files, and automatic download of this was not supported in RADIS. Due to NaN values and the &lt;code&gt;np.uint&lt;/code&gt; not supporting them, the datatypes of a few columns conflicted when databases were added on top of one another. Currently, this is being handled by returning the parameters in the form of a memory inefficient &lt;code&gt;np.float64&lt;/code&gt;. I will have to bring them down to more suitable datatypes (&lt;code&gt;np.uint&lt;/code&gt;) most probably. This will probably be the first thing I will do as part of the project.&lt;/p&gt;
&lt;h3 id="the-next-two-weeks"&gt;The next two weeks&lt;/h3&gt;
&lt;p&gt;In the next two weeks I will be involved in figuring out and implement all the database pre-processing that can be done to boost pandas’ performance&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt;. I will also setup memory performance benchmarks to track these changes. I am super excited to see how this project goes. I would like to thank Google, OpenAstronomy, RADIS and my mentors &lt;a href="https://github.com/erwanp"&gt;Erwan Pannier&lt;/a&gt;, &lt;a href="https://github.com/dcmvdbekerom"&gt;Dirk van den Bekerom&lt;/a&gt; and &lt;a href="https://github.com/pkj-m"&gt;Pankaj Mishra&lt;/a&gt;. I hope to learn a lot of stuff along the way and hopefully I will deliver. So,&lt;/p&gt;
&lt;p&gt;&lt;img alt="Let the Games Begin" src="https://media1.giphy.com/media/xT0xevozBTg7ChpL44/source.gif"&gt;&lt;/p&gt;
&lt;div class="footnotes"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;&lt;a href="https://gagan-aryan.netlify.app/tags/gsoc21/doi.org/10.1016/j.jqsrt.2018.09.027"&gt;RADIS: A nonequilibrium line-by-line radiative code for CO2 and HITRAN-like database species, E. Pannier &amp;amp; C. O. Laux&lt;/a&gt; &lt;a class="footnote-backref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fnref:1"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;&lt;a href="https://wesmckinney.com/blog/apache-arrow-pandas-internals"&gt;Apache Arrow and the “10 Things I Hate About pandas”, Wes Mckinney&lt;/a&gt; &lt;a class="footnote-backref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fnref:2"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;&lt;a href="https://mybinder.org/v2/gh/radis/spectro101/HEAD?filepath=102_lab_spectroscopy.ipynb"&gt;Spectro102&lt;/a&gt; &lt;a class="footnote-backref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fnref:3"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;&lt;a href="https://radis.github.io/"&gt;Radis Documentation&lt;/a&gt; &lt;a class="footnote-backref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fnref:4"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:5"&gt;
&lt;p&gt;&lt;a href="https://pythonspeed.com/articles/pandas-load-less-data/"&gt;Pythonspeed article - Pandas Load less data&lt;/a&gt; &lt;a class="footnote-backref" href="https://gagan-aryan.netlify.app/tags/gsoc21//index.xml#fnref:5"&gt;↩︎&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/06/20210606_0300_gagan-aryan/</guid><pubDate>Sun, 06 Jun 2021 02:00:06 GMT</pubDate></item><item><title>print(" Hello World!!! ")</title><link>http://openastronomy.org/Universe_OA/posts/2021/05/20210518_2240_anandxkumar/</link><dc:creator>anandxkumar</dc:creator><description>&lt;div&gt;&lt;p&gt;Hey everyone &lt;b&gt;Anand Kumar&lt;/b&gt; this side. This is going to be a series of blogs where I will cover my Summer Journey with &lt;b&gt;Radis&lt;/b&gt; organization as a part of Google Summer of Code. Welcome to my first blog where I will be introducing myself coz that is kind of necessary :p. I’m a Junior from National
Institute of Technology, Hamirpur, India currently pursuing my BTech in Computer Science and Engineering.&lt;br&gt;&lt;/p&gt;
&lt;p&gt;I am a geek. I love life, computers and everything in between!&lt;br&gt;
&lt;!-- TEASER_END --&gt;
I have been coding since my school days and soon realized that man this thing is so cool!
I am an A.I. enthusiast and have made various projects related to Data Analysis, Machine Learning, Deep Learning
and Web Development. Also I have completed a Data Analytics Internship at Pikkal &amp;amp; Co, Singapore and a
Deep Learning Internship at Mavoix Solutions Pvt Ltd, Bangalore.&lt;br&gt;
Currently I’m a Student Developer at OpenAstronomy organization as a part of Google Summer of Code 2021. &lt;br&gt;&lt;/p&gt;
&lt;p&gt;Wanna know a less known fact, I’m a huge hardcore video gamer! If I’m not coding, I will probably be killing some time
on my laptop or my Playstation console ;)&lt;/p&gt;
&lt;p&gt;Wanna know more about me and my work? Below are some links, do check out;)&lt;br&gt;
&lt;a href="https://www.linkedin.com/in/anand-kumar-83896717a/"&gt;LinkedIn&lt;/a&gt; | &lt;a href="https://github.com/anandxkumar"&gt;github&lt;/a&gt;
| &lt;a href="https://anandkumar.netlify.app/"&gt;Portfolio&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Also one huge shout out to the guys at &lt;b&gt;GatsbyJS&lt;/b&gt; for providing such an amazing blogging template(keep it simple and clean, they say!).
The biggest advantage of this template is that every blog is written in &lt;b&gt;Markdown&lt;/b&gt;. So its gives alot of flexibility and functionality
to the user to edit their texts. Plus their templates codebase is easy to understand so anyone can just clone and get started!&lt;/p&gt;
&lt;p&gt;Anyways I guess this should wrap up this blog. See you in the next one where I will be starting my GSoC journey and discuss my project ;)&lt;br&gt;
Till then take care and ba-bye :)&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2021/05/20210518_2240_anandxkumar/</guid><pubDate>Tue, 18 May 2021 21:40:32 GMT</pubDate></item><item><title>Google Summer of Code - The End!</title><link>http://openastronomy.org/Universe_OA/posts/2020/08/20200827_0614_pkj-m/</link><dc:creator>pkj-m</dc:creator><description>&lt;div&gt;&lt;p&gt;Hello everyone! It has been a while since my last blog, and for a good reason. The past few weeks have been quite productive, and I thought it might be a good idea to present one final report of the work that I did over this past month instead of breaking it into subparts. With this blog, I will also be marking the end of my journey through the Google Summer of Code program. This blog will talk about some of the changes that the work I did as a part of GSoC brought to RADIS, and how you, the user can and will benefit from it.&lt;/p&gt;

&lt;p&gt;In my last blog, I briefly mentioned what I was planning to do with the GPU code and how to integrate it with RADIS. The current RADIS code performs calculation of spectra at thermal equilibrium (and even in non-equilibrium conditions) in primarily two ways:&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;ol&gt;
&lt;li&gt;by defining a &lt;code class="language-plaintext highlighter-rouge"&gt;SpectrumFactory&lt;/code&gt; object, and then calling the method &lt;code class="language-plaintext highlighter-rouge"&gt;sf.eq_spectrum(Tgas=T)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;by passing the necessary parameters in the method &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt;, which returns a Spectrum object directly&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In our attempt to add support for GPU accelerated spectrum calculation, we wanted to keep the interface as similar to the original one as possible. Thus, the new method which we introduced to calculate the spectrum using GPU was naturally called &lt;code class="language-plaintext highlighter-rouge"&gt;eq_spectrum_gpu&lt;/code&gt;. The &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt; method, which is actually a wrapper that makes of the &lt;code class="language-plaintext highlighter-rouge"&gt;eq_spectrum&lt;/code&gt; method underneath, was also modified and a new parameter called &lt;code class="language-plaintext highlighter-rouge"&gt;mode&lt;/code&gt; was added. Depending on what the value of &lt;code class="language-plaintext highlighter-rouge"&gt;mode&lt;/code&gt; is, the calculation of spectrum could be performed either on the GPU or on the CPU.&lt;/p&gt;

&lt;p&gt;Now coming to the implementation of &lt;code class="language-plaintext highlighter-rouge"&gt;eq_spectrum_gpu&lt;/code&gt;, I tried to keep the structure of the code as similar to the current CPU implementation as possible. What it meant was that the preprocessing done was in a way quite similar to the CPU version of the method. The difference actually came in during the broadening step. Initially our implementation was different from the CPU version when it came to loading the data, primarily because the data being loaded in the GPU method was in the ‘npy’ format. This made it necessary to implement another method for loading this data, as the data loader in RADIS did not support npy files. While implementing this was not difficult, it was not seen as a very good design decision, as this type of loading and handling of data was very isolated and not compatible with the rest of RADIS’ features. Therefore, ultimately it was to keep this mpy2df method as an additional, helper method, and instead of using it as the primary source of data, we use the dataframe which RADIS already generates instead. This allowed us to keep things compatible with the current implementation to a great extent, and the only downside, if it can be considered that, was the need to now compute the parameters before the spectrum is calculated, which in case of npy files were already present for us. This however, was virtually a non-problem since this step was not even remotely close to the bottleneck, and the flexiblity it provided in terms of loading and preprocessing data outweighed this extra computation easily. At this point we had the data loaded in memory, either through the legacy data loader using the dataframe, or by passing the location of the npy files in the system and loading them directly. After this, we had to pass this data to the GPU module. The GPU module, titled py_cuFFS, is actually a Cython file with some CuPy, which serves as the complete host+device code for the computation of the spectra. Using Cython over Python allows us to compile the module prior to using it, which gives an added performance boost. The compilation however, is a machine-specific process and cannot have a single-file-handles-all kind of implementation. Thus, instead of sharing the binary file with the users, we instead share the source code. Whenever the user calls the GPU accelerated methods on their system for the first time, RADIS automatically compiles the source code into the binary, which then gets compiled according to the system environment of the user. Now, the compiled binary is imported by RADIS, and the input parameters such as the temperature, pressure and the partition function are passed on to the GPU.&lt;/p&gt;

&lt;p&gt;The GPU module returns the spectrum to RADIS, which then computes other quantities, such as the absorbance and transmittance from this. From this stage onwards the code for &lt;code class="language-plaintext highlighter-rouge"&gt;eq_spectrum&lt;/code&gt; and &lt;code class="language-plaintext highlighter-rouge"&gt;eq_spectrum_gpu&lt;/code&gt; is identical. Both the methods update the metainformation such as the calculation time, number of lines calculated, etc. In order to read more about the GPU module and how to use it, I highly recommend the users to go through the &lt;a href="https://radis.readthedocs.io/en/latest/lbl/gpu.html"&gt;documentation&lt;/a&gt; which has not just examples but also a guide on how to setup your system in order to make use of these GPU accelerated methods. If you’d just like to observe for now, I would recommend going through this &lt;a href="https://github.com/radis/radis-benchmark/blob/master/TEST1.ipynb"&gt;notebook&lt;/a&gt; on radis-benchmark which gives an example of how to use these methods to calculate the spectrum on the GPU, and an impressive speed test between the GPU and the CPU methods when calculating spectra with 5M lines.&lt;/p&gt;

&lt;p&gt;This wraps up my journey with RADIS a Google Summer of Code participant. It has been an excellent experience with a great deal of learning involved. I had prior experience of working with CUDA for deep learning pipelines but this was a completely new domain. In addition to the programming itself, I got exposed to the world of spectroscopy which was also very interesting. While my contribution to RADIS under the GSoC aegis comes to an end with this blog, I am still really excited to be a part of RADIS as it grows further. My GSoC project started what would hopefully end with a completely GPU-accelerated RADIS, but there is still plenty of work before we can say that. My GSoC project implemented the thermal equilibrium variant of the spectrum calculation method, but we still need to work on non-equilibrium methods. In addition, we also need to modify the GPU code itself to allow support for weighted air- and self-collision factors, among other things. There’s a lot to do, but I think we’ve had a good start. Most importantly, I am happy with the way I am ending this project. The code is ready and we have proper guide, documentation and examples in place so any new user can easily try this feature out themselves! I am really excited about the feedback and what users have to say about this GPU implementation. Finally, none of this would have been possible without the constant support and assistance from my amazing mentors, Erwan ( &lt;a href="https://github.com/erwanp"&gt;@erwanp&lt;/a&gt; ) and Dirk ( &lt;a href="https://github.com/dcmvdbekerom/"&gt;@dcmvdbekerom&lt;/a&gt; ). It would be hard to understate the contribution they’ve made to my project, helping constantly not just with the code but also in designing and planning the next steps. My lack of knowledge in the domain of spectroscopy was a huge pain at times which led to extremely long periods of slow progress due to painful debugging, but they always took out time from their packed schedules to help me out. Once again, thank you! It has been a wonderful experience, working with RADIS and I am really excited to see what how RADIS grows in the future!&lt;/p&gt;

&lt;p&gt;P.S. for anyone who’d like to go through the code of the project, you can find the pull request here: https://github.com/radis/radis/pull/117. And more importantly, if you’d like to know more, want to contribute, or just talk to the team, feel free to join us at our slack &lt;a href="https://radis.github.io/slack-invite/"&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2020/08/20200827_0614_pkj-m/</guid><pubDate>Thu, 27 Aug 2020 05:14:12 GMT</pubDate></item><item><title>Google Summer of Code - Blog #4!</title><link>http://openastronomy.org/Universe_OA/posts/2020/07/20200728_1823_pkj-m/</link><dc:creator>pkj-m</dc:creator><description>&lt;div&gt;&lt;p&gt;Hello! Welcome to the latest blog post in my Google Summer of Code series. With this blog, we’ll marking the end of the second phase of evaluations! Additionally, this blog is going to be a little different from all the previous ones as we finally start to work and discuss about Python and the actual RADIS code base.&lt;/p&gt;

&lt;p&gt;Honestly, I am actually really glad that we’re finally at this stage. The monotonity of Cython and working on the same, huge Cython+CuPy file with more than a thousand lines of code was getting very frustrating :P&lt;/p&gt;
&lt;!-- TEASER_END --&gt;

&lt;p&gt;In this blog, I shall be discussing more about the way we will integrate the GPU code with RADIS. But before we do that, let us first understand how RADIS handles this part right now, performing computations purely on the CPU. In order to compute the spectra of a molecule, we make use of module defined in RADIS known as &lt;code class="language-plaintext highlighter-rouge"&gt;Line-by-Line&lt;/code&gt; Module or LBL for short. This module contains numerous methods which are used for calculations of spectras. One of the most standard methods for computing the spectras is known as &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt;, which takes in inputs such as the molecule, isotopes, temperature, pressure, waverange over which the spectra needs to be calculated (and many other depending on the user’s requirements) and returns the result in the form of &lt;code class="language-plaintext highlighter-rouge"&gt;Spectrum&lt;/code&gt; object which neatly packs the information in single object which can then be processed as needed. This is what the user sees when they use RADIS. However, for us to develop and contribute, we also need to understand what goes on under the hood when a method such as &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt; is called.&lt;/p&gt;

&lt;p&gt;The first thing that occurs when &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt; is called is the validation and conversion of physical quantities (such as temprature and pressure) into default units that are assumed in the rest of the code. This is followed by the instantiation of an object from the &lt;code class="language-plaintext highlighter-rouge"&gt;SpectrumFactory&lt;/code&gt; class, which will contain all the information about the spectrum to be computed. This is all standard, something that will happen each time &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt; gets called. However, from here on, things start to get interesting as we look into the databank. Let us first understand what the purpose of a databank is: in order to compute a spectra for a specfic molecule, we need information about it. This could include data like line positions, line intensities, pressure-broadening parameters, etc. We don’t have to understand what each of these quantities mean or represent (to be honest, I don’t either) but the main idea we need to internalize is that we can’t compute a spectra without any information. So while it might be fair to say that a spectra can literally be generated from thin air, metaphorically that does not hold true.
This data, used to compute the spectra, can often be huge and therefore we try to avoid moving it around as much as possible. RADIS has implemented cache features in order to minimize the computations to be done on the raw databank. The important idea we need to focus on, is that once a databank has been loaded, we don’t have to load it once more if we ever call &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt; again for the same data (with maybe different waverange, etc.) This is done using extra memory: the original data that is loaded in the memory is saved as a dataframe, and every call made to &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt; first creates a copy of this data and works on it instead. This preserves the original dataset and allows us to use it again in case it is required, as the cost of the extra memory needed to save the copy. This was also the first problem I had to tackle: the GPU code we were going to use made use of &lt;code class="language-plaintext highlighter-rouge"&gt;npy&lt;/code&gt; formatted arrays to load the data, whereas RADIS was designed to load &lt;code class="language-plaintext highlighter-rouge"&gt;h5&lt;/code&gt; or &lt;code class="language-plaintext highlighter-rouge"&gt;par&lt;/code&gt; files. This meant I had to modify the databank loader RADIS used to support npy files. This was not a very significant problem however, as we could simply load the data separately instead of using the databank. The only issue with this approach would have been the repeated loading of data everytime the function call was made, but since we were using a small dataset, it wasn’t a significant issue and we decided to push it back and first focus on getting the GPU code to work with independent data being loaded.&lt;/p&gt;

&lt;p&gt;Once we had the data, the next step was to perform the computations. Now I won’t go into the details in this part since the mathematics behind these computations was something which: 1. I didn’t understand completely, and 2. I didn’t need to understand completely. Given my project already had a proof-of-concept code with the mathematics inplace, and my job was to make that code compatible with RADIS, I didn’t need to understand the nuances of it. I did go through the paper draft to understand the general idea behind the parallel approach we used, but I didn’t even try with the serial code since I had absolutely nothing to do with it. This mathematics was actually abstracted and encapsulated in another method &lt;code class="language-plaintext highlighter-rouge"&gt;eq_spectrum&lt;/code&gt;. Once this method finished execution, we would have the spectrum ready with all the information ready for the user. Within this method, there was one specific method that was of particular interest to us: &lt;code class="language-plaintext highlighter-rouge"&gt;_calc_broadening&lt;/code&gt;, responsible for the broadening step in the spectra calculation pipeline. This method was the bottleneck in the entire process, and the GPU code was primarily helping us speed up this particular portion of the process. My job here was to create another analogous method &lt;code class="language-plaintext highlighter-rouge"&gt;eq_spectrum_gpu&lt;/code&gt; which would do the exact same job as &lt;code class="language-plaintext highlighter-rouge"&gt;eq_spectrum&lt;/code&gt;, except instead of calling the sequence of serial methods to compute the spectra, I’d simply call the GPU code which was ready for us, and obtain the result. Some further processing of this result (which is computationally very inexpensive compared the rest of process), we have our spectra ready. It is a lot simpler and significantly easier than the CPU equivalent since we off-load the entire process to the Cython-compiled binaries and don’t have to worry about anything else. One problem I had with this (which is still not resolved as of writing this) is how do we fill in the missing information gaps in the Spectrum object, which in case of the CPU code were filled sequentially as the pipeline proceeded. Since our GPU code would only return the final result, I am not sure if we would be able to recover the preceding information bits which we didn’t obtain. Hopefully I’ll have the answer to it tomororrow when Erwan replies. That would pretty much finish my job here.&lt;/p&gt;

&lt;p&gt;However, another important bit of information that we’re skimming over is how we call the compiled binary file. This part is in fact what I will be working on over the coming week. The idea is that although right now I am using a simple binary file located in the same file to run the code, it is possible (and highly likely) that the same binary file will not work properly (or at all) in another system it is not meant for. As a consequence, the entire method might crash. In order to resolve this, we will be looking into placing the source code instead of the binary file in the RADIS codebase, and everytime a user needs to use the GPU version of &lt;code class="language-plaintext highlighter-rouge"&gt;calc_spectrum&lt;/code&gt;, the code will first, automatically, compile the Cython file on their system, obtain the binary file meant for their system, and use that instead. The details regarding the implementation of this bit are still not clear as it is something I will be working on next week!
The next blog will cover the implementation details about this runtime-compilation setup along with a discussion on what is next! Thank you!&lt;/p&gt;&lt;/div&gt;</description><category>radis</category><guid>http://openastronomy.org/Universe_OA/posts/2020/07/20200728_1823_pkj-m/</guid><pubDate>Tue, 28 Jul 2020 17:23:12 GMT</pubDate></item></channel></rss>